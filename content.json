{"pages":[{"title":"关于我","text":"浙江大学光电科学与工程学院2020级本科生 正准备入计算成像+AI的坑 联系方式见网页左下","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"hexo配置问题","text":"记录hexo配置中遇到的各种问题，随缘更新中…… OS：Windows 10 主页类型：gitpage 官方文档：文档 | Hexo 第一个网页 部署到github 配置主题 安装 配置 正文部分 写入正文 编辑文章的分类、标签 插入图片 插入公式 插入TOC目录 插入代码 About页 Tag页 开启评论区 首页显示文章摘要 第一个网页# 此部分根据官方文档操作即可完成，故不赘述 安装：文档 | Hexo 建站：建站 | Hexo 部署到github# 此部分根据官方文档操作即可完成，故不赘述 在 GitHub Pages 上部署 Hexo | Hexo 一键部署 | Hexo 配置主题# 这里面有非常多主题，随便选一个自己喜欢的用就好了。 我选择的主题是Anatolo，这个主题的作者锦心是一位可爱的OIer妹妹（她还不到大一就能自己写theme，而我配置她的主题都磕磕绊绊quq），她的主页：Lhc_fl Home (lhcfl.github.io) 安装# 参考Anatolo (lhcfl.github.io) 配置# 复制_config.example.yml为_config.yml，修改hexo根目录下的 _config.yml ： theme: Anatolo 上面是锦心写的配置方法，但是当时我花了很久时间才理解，具体来说应该是把themes/Anatolo下的_config.example.yml文件复制到themes/Anatolo（一开始我以为是复制到根目录……），然后修改hexo项目根目录下的 _config.yml ： theme: Anatolo。 正文部分# 写入正文# 新建文章 ：用hexo new &lt;title&gt; ，可以在_post目录下新建&lt;title&gt;.md文件，之后便可以按照markdown语法开始写作。 编辑文章的分类、标签# 参照如下写法即可 1234567title: hexo配置问题date: 2023-01-12 02:45:58tags: - 环境配置 - hexocategories: - 环境配置 插入图片# 这篇文章十分详细地讲了如何方便地结合Typora插入图片：hexo博客如何插入图片 - 知乎 (zhihu.com) 插入公式# 找不到参考的文章了quq 告诫我配置完环境一定要马上整理 警钟长鸣 插入TOC目录# 直接用Typora中的TOC是不能插入目录的！ 正确插入方法如下： 安装hexo-toc插件：npm install hexo-toc --save 参照文档内的方法编辑_config.yml 在想插入目录的地方写入 1&lt;!-- toc --&gt; 插入代码# 无需插件，正常写入即可。 TODO：学习利用插件对代码块进行优化的方法。 About页# 用hexo new page \"about\"新建“关于”页面，然后正常写入内容即可 Tag页# 用hexo new page \"tag\"新建标签统计页面即可，无需写入任何内容 开启评论区# 我使用的是gitalk，配置参照如下两篇博客 Hexo-Anatolo主题添加gitalk评论系统 - 简书 (jianshu.com) 微调Hexo主题Anatolo接入gitalk · Csome 但是需要提醒一点： _config.yml下的repo属性只需要填写仓库名字即可，不要填写仓库链接！ 首页显示文章摘要# 配置步骤： Anatolo/layout/page.pug文件中block content中调用+make_post函数时的true改成false Anatolo/_config.yml文件中useSummary的值设置为true md文件中的front-matter里面加上excerpt: xxx，xxx为摘要内容 尝试对上面的步骤做一个解释（因为我不懂前端，这个步骤也没有文档说明，解释的可能不对） Anatolo/layout/page.pug 的content这个block可能是产生网页内容的部分，我注意到里面调用了一个mixins中定义的make_post函数，page参数代表的是md文件中的front-matter和正文组成的struct。 123456block content .autopagerize_page_element: .content: .post-page include mixins +make_post(page, false) if page.comments include partial/comments 然后找到Anatolo/layout/mixins.pug中的make_post函数的，这边关注它的前半部分就可以了。我们呢发现如果is_detail的值是true，那么首页的摘要处只会显示文章内容和copyright两部分，所以要修改make_post中的输入参数（步骤1）。 然后我们发现无论config文件中的useSummary值如何，这时程序会去找item.excerpt，所以我们front-matter中要写入excerpt属性，内容为摘要（步骤3）。而如果useSummary的值是true，那么如果我们在front-matter中写入了excerpt属性，那么就会把excerpt的内容作为摘要，如果没有写入，则会把文章的前160词截取作为摘要。 123456789101112131415161718192021222324mixin make_post(item, is_detail) .post.animated.fadeInDown .post-title h3 if is_detail a= item.title else a(href= url_for(item.path))= item.title if is_detail .post-content p!= item.content if theme.copyright.show .tip!= (item.copyright || theme.copyright.default) + \"&lt;br&gt;\" + __(\"author\") + \": \" + (item.author || theme.author) else .post-content if theme.useSummary - var summary = item.excerpt || item.content p!= truncate(strip_html(summary), {length: 160}) else .card if item.excerpt p!= item.excerpt else p!= item.summary || item.content","link":"/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"},{"title":"李宏毅机器学习-Why Deep Learning?","text":"引言——鱼与熊掌可以兼得吗 为什么需要隐藏层 为什么需要“deep” 实验结果 形象的理解 举例说明 李宏毅老师的这节课没有讲到什么“干货”，但是讲了一个挺有意思的问题，也是解决了一个困扰了我很久的疑问。 引言——鱼与熊掌可以兼得吗# 在机器学习中，我们常常构建一个模型，并希望通过训练让模型找到合适的参数，即从集合H\\mathcal{H}H中找出适合的模型hhh，理论上当参数量足够大的时候，理想的loss可以无限低，但是由于over fitting或其它各种各样的原因，实际的loss无法达到理想。 如下图所示，当模型参数量大，模型可选择面比较大时，理想loss比较小，但是“理想”和“现实”差距大；模型参数少，模型可选择面比较小时，模型的理想loss比较大，但是和“现实”差距小。 而深度学习，是一个能用比较少的参数（这里的少是相对达到同样效果的非深度学习模型而言的），却能达到比较低的理想loss的方法，同时由于参数较少，“理想”和“现实”差距较小。所以说深度学习是“鱼与熊掌可以兼得”的机器学习。 为什么需要隐藏层# 训练模型，其实就是在寻找一个以人力不可能写出的复杂非线性函数。而对于任何一个非线性函数，其实都可以通过合适的采样，然后把采样点连起来形成折线去近似，随着折线数的增加，近似会越来越精确。 而对于折线，可以用一系列的阶梯函数（如下图上的aia_iai​上面的蓝线）叠加生成，这些阶梯函数可以通过线性函数经过sigmoid直接得到或者由两个线性函数经过ReLU后再叠加得到。因此在理论上，只需要一个隐藏层，就可以构造出任意的非线性函数。 为什么需要“deep”# 那么曾经有人就提出了一个问题，既然一个隐藏层就可以表达出任意非线性函数，为什么要Deep Learning而不是“Fat Learning”呢？ 概括成一句话就是：Deep Learning可以从简单的非线性函数构成复杂的非线性函数，相比简单的叠加方式产生非线性函数，Deep Learning能高效地利用比较少的参数产生复杂的非线性函数 实验结果# Conversational Speech Transcription Using Context-Dependent Deep Neural Networks 里面对比了Deep Learning架构和“Fat Learning”架构对于相同任务的效果，对于Deep Learning而言，5层、每层2000个神经元的模型和1层、3772个神经元的模型参数数是接近的，但是5层网络的架构效果更好。 形象的理解# 假设今天我们要实现四个二进制数中1的个数的统计，4个二进制数有16种可能，那么如果电路只有两层，就需要16个门电路，但是如果用下图中的多层同或门电路，就可以只用三个门形成的电路，相对而言就比较高效。 举例说明# 下面举一个构成2k2^k2k条线段组成的折线的例子来说明Deep Learning的优势 xxx到a1a_1a1​，构成了2条线段构成的V形折线，而a2=f(a1)a_2=f(a_1)a2​=f(a1​)、a3=g(a2)a_3=g(a_2)a3​=g(a2​)又是两个相同的V形函数，最终a3=h(x)a_3=h(x)a3​=h(x)就是一个232^323条线段组成的折线，共使用3层网络，6个神经元。 对于2k2^k2k条线段组成的折线，如果用一层隐藏层（激活函数同样使用ReLU）构成，那么需要2k2^k2k个神经元，但是如果用多层具有两个神经元的网络构成，只需要kkk层网络，共2k2k2k个神经元就足够了。 这个例子表示在构成复杂但是有一定规律的函数上，Deep Learning 相比“Fat Learning”是具有优势的。对于图像、语音、文字等任务，由于图像、语音、文字有一些内在特性、规律，需要的函数也可能会有一定规律（比如图像分类模型需要模型能提取特征），所以用Deep Learning方法训练的模型会有比较好的效果。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/"},{"title":"李宏毅机器学习-hw3-CNN-总结","text":"任务介绍 训练结果 训练方法 Data augmentation 模型设计 Loss function的选择与调参 Learning rate调整方案 Test time augmentation 进一步提升的可能 数据增强可能可以尝试mix up 模型设计可能可以继续尝试 Learning rate调整方案有待提升 Cross Validation + Ensemble 其它值得记录的东西 autodl tensorboard使用 layer的梯度变化 任务介绍# 这个作业要求训练一个能对11种食物进行分类的CNN。使用的数据集为food-11，数据集划分如下： 训练集：9866张带有label的图像 验证集：3430张带有label的图像 测试集：3347张不带有label的图像 完成训练后，将用测试集进行测试，输出含有3347张图片的预测label的csv文件，上传至kaggle后系统根据预测准确率自动评分。 Baseline如下： Simple : 0.50099 Medium : 0.73207 Strong : 0.81872 Boss : 0.88446 训练结果# Times private score public score Improvement（相比前一步） 1 0.57063 0.55278 直接运行初始代码 2 0.71830 0.75298 数据增强、调整模型、调整loss function 3 0.75757 0.78884 调整数据增强、手搓ResNet、loss function调参 4 0.77592 0.80378 减少ResNet层数、调整learning rate 5 0.77251 0.80677 减少ResNet层数 6 0.82415 0.85358 增加ResNet层数 7 0.84208 0.87450 使用tta 训练方法# Data augmentation# 在观察了部分数据之后，发现合理的图片增强方式包括但不限于 缩放裁剪 随机翻转 随机旋转 仿射变换 随机灰度化 最终代码如下所示： 123456789101112train_tfm = transforms.Compose([ # Resize the image into a fixed shape (height = width = 128) transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)), transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转 transforms.RandomVerticalFlip(p=0.5), # 随机上下翻转 transforms.RandomRotation(degrees=(0, 180)), # 图像随机旋转 transforms.RandomAffine(30), transforms.RandomGrayscale(0.2), # 随机灰度化 # You may add some transforms here. # ToTensor() should be the last one of the transforms. transforms.ToTensor(),]) 模型设计# 模型使用的是ResNet，ResNet的搭建见：记手搓ResNet的经历 · 核子的Blog (hezj-opt.github.io) 在调整参数方面，最终调整预卷积层的通道数为32，全连接层的dropout为0.4，残差块采用两个3x3卷积层和shortcut path构成的残差块，最终使用通道数为64、128、256、512的残差块数量分别为1、2、1、1 1model = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 2, 1, 1], 11).to(device) 在第4次训练中，使用过层数较少的网络（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 2, 1], 11).to(device)），但是效果不佳，当时以为是over fitting，所以在第5次训练中把网络改得更简单了（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 1, 1], 11).to(device)），修改后发现效果不仅没有显著提升，而且训练时长增加了非常多（第4次训练大致12小时，第5次训练花了将近16小时）。所以发现是模型欠拟合，决定增加模型复杂度，才在第6次训练后通过strong baseline。 鉴于几次调参后的效果，推断三次训练在下图中位置红色线框覆盖的区间内。 Loss function的选择与调参# 查阅资料发现，使用Focal loss会比Cross entropy有更快的收敛速度（参数γ导致），而且更适合各个标签的数据集不均的情况（参数α导致）。 下图显示了由于Focal loss的γ参数，Focal loss会收敛得更快 Focal loss中的α参数可以平衡训练集中各类图片数量的差异，比如在本次任务中，我先统计训练集中了11类食物的数量，然后对样本少的赋比较高的α值，样本多的食物赋比较低的α值。 123count = count / np.max(count) # count 为11类食物样本数的统计结果alpha = 1 / count# alpha = [1, 2.317, 0.663, 1.008, 1.172, 0.750, 2.259, 3.55, 1.163, 0.663, 1.402] Learning rate调整方案# 优化器选择Adam，初始学习率为0.0004，设置decay=1e-5 在学习率调整上，通过余弦退火调整学习率，退火周期为16，即每过16轮，学习率突然增大，然后再慢慢减小。如下图除了蓝线、红线以外的曲线所示。 这么做好处有二，一是可以加快收敛速度，二则是可以帮助“翻越” loss surface上的一些小山峰，从而更好地跳出“局部最小”（其实往往不是局部最小，但是有“跳出”的作用）找到全局最优解。 Test time augmentation# 训练时，我们对训练数据进行了增强，但是测试时用的就是正常的图片，但是我们想利用模型对增强的图片的识别效果辅助判断。所以可以让测试图片产生若干张增强的图片，依次求未增强图片和增强图片的预测向量，然后把向量相加后再求出最大值所在位置，就是最终预测结果。模型对同一个物体的原图和多张增强图像进行预测，一定程度上可以校正只对原图进行预测时的错误。 比如在本次hw中，我对每张测试图片产生五张增强图片，对六张图片进行预测，得到 preds列表，内含未增强图像的预测结果（preds[0]）和五张增强图像的预测结果。最终的预测结果为 12preds = 0.5* preds[0] + 0.1 * preds[1] + 0.1 * preds[2] + 0.1 * preds[3] + 0.1 * preds[4] + 0.1 * preds[5]prediction = np.argmax(preds, axis=1) 进一步提升的可能# 数据增强可能可以尝试mix up# mix up数据增强是一种进阶的数据增强，它可以使得模型在判断时不会那么绝对，可以减小过拟合。如果要实现mix up，要改写Dataset类，并且还需要写适用于mix up的loss function。 模型设计可能可以继续尝试# 过了strong baseline以后我没有继续调整模型，表格中最后一次产生的结果实际上用的是第6次训练得到的模型，只是在测试时采取tta。所以可能可以继续尝试加深模型，直至出现过拟合，再适当减小模型。 Learning rate调整方案有待提升# 我的实现中退火周期时固定的，但是看了原始论文后，发现设置合适的初始退火周期，然后在每个退火周期后，将退火周期增大一倍可能是个不错的策略。 Cross Validation + Ensemble# 简单地来说，Cross Validation + Ensemble是先把训练集和验证集重新进行划分，进行k种划分，训练出k个模型，最终用k个模型对图片进行多次预测。效仿tta的方式融合产生预测结果，此时不同的模型之间可以互相弥补缺陷，所以可以得到比较好的预测结果。 但是这玩意真的耗时间，第6次训练花费时间为8-9h，如果训练四个模型得35h左右…… 其它值得记录的东西# autodl tensorboard使用# 本次作业中我学习了autodl自带的tensorboard，发现操作十分简单。 开机后打开AutoPanel 用pip下载tensorboard 1$ pip install tensorboard 新建一个writer 12from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter('/root/tf-logs') 把想要看的东西放进tensorboard 123456789101112131415# loss 曲线writer.add_scalars(main_tag='Loss', tag_scalar_dict={'train': train_loss, 'valid': valid_loss}, global_step=epoch + 1)# Accuarcy曲线writer.add_scalars(main_tag='Accuracy', tag_scalar_dict={'train': float(train_acc), 'valid': float(valid_acc)}, global_step=epoch + 1)# 各层的参数的梯度绝对值曲线for name, parms in model.named_parameters(): writer.add_scalar(f\"Grad/{name}\", torch.norm(parms.grad), epoch + 1) layer的梯度变化# 下面两张图时预卷积层的weight的梯度模长变化和某个残差块中的一个卷积层的weight的梯度模长变化（经过平滑后的结果）。所以说可以发现哪怕训练到几乎停滞，其实并不是陷入局部最小值，甚至不在鞍点。所以之前推测学习率调整方案还可以进一步优化。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/"},{"title":"李宏毅机器学习-lect4 Self attention","text":"Self attention 解决什么问题 FC(Fully Connected)有什么不足 Self attention 的架构 Self attention 的基本计算过程 求attention score 求解Self attention层输出的向量 矩阵视角下的计算过程 Self attention的优化 Multi-head Self attention Positional Encoding Self attention 和其他网络对比 CNN RNN GNN Self attention 解决什么问题# 用一句话概括就是：Self attention用来解决当输入为向量的序列时的问题（像音频、文本都是经典的输入为向量序列的数据） Self attention的输出一般有如下三种类型 N个vector产生N个label ​ 例如输入一个句子，输出每个词的词性 N个vector产生1个label ​ 例如输入一个句子，判断这句话蕴含的情绪为positive or negative N个vector产生N’（N≠N’）个label ​ 例如机器翻译，输入的句子和输出的句子词数很可能不一样 FC(Fully Connected)有什么不足# FC也可以用来解决输出为向量序列的问题，如天气预测等等，但是它相比self attention有一些不足之处。 FC如果要充分考虑“上下文”——一个向量和它相邻的很多个向量，甚至可能整个序列一起考虑，就需要把考虑的向量串联起来，通过fully connect产生新的向量，那么参数的矩阵可能会非常大（这个在后面会解释），这可能导致很大的运算量和overfitting。 Self attention 的架构# 首先我们来看一个self attention层要做什么。 概况一下就是，一个self attention层要先算出每个向量和其他向量的关联性，这个关联性用attention score α\\alphaα 表示。然后再根据加权算出输出向量序列，值得一提的是，计算输出向量序列中的每个向量是并行的。 然后我们再看整个网络架构，经过self attention层后，产生了输出向量序列，这时候可以先对每一个向量进行fully connect，产生输入到下一个self attention层中的输入向量，然后就是重复，直至产生最终输出。 Self attention 的基本计算过程# 求attention score# 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求a1\\mathbf{a^1}a1与自身及其他向量的attention score α1,i′\\alpha'_{1,i}α1,i′​为例，来说明求解流程 对a1\\mathbf{a^1}a1求q1\\mathbf{q^1}q1，q1=Wq⋅a1\\mathbf{q^1}=\\mathbf{W}^q\\cdot\\mathbf{a^1}q1=Wq⋅a1，其中Wq\\mathbf{W}^qWq为参数矩阵，是要学习的参数 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…，ki=Wk⋅ai\\mathbf{k^i}=\\mathbf{W}^k\\cdot\\mathbf{a^i}ki=Wk⋅ai，其中Wk\\mathbf{W}^kWk为参数矩阵，是要学习的参数 求q1\\mathbf{q^1}q1和k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…的关联性，常见方法是向量点乘，如α1,2=(q1)T⋅k2\\alpha_{1,2}=(\\mathbf{q^1})^\\mathrm{T}\\cdot\\mathbf{k^2}α1,2​=(q1)T⋅k2 对上一步求得的α1,1,α1,2,…\\alpha_{1,1},\\alpha_{1,2},\\dotsα1,1​,α1,2​,…通过激活函数，常用softmax，最后得到attention score α1,1′,α1,2′,…\\alpha'_{1,1},\\alpha'_{1,2},\\dotsα1,1′​,α1,2′​,… 可以参照下图直观地理解上述步骤 求解Self attention层输出的向量# 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求输出向量序列中的b1\\mathbf{b^1}b1为例，来说明求解流程 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求v1,v2,…\\mathbf{v^1},\\mathbf{v^2},\\dotsv1,v2,…，vi=Wv⋅ai\\mathbf{v^i}=\\mathbf{W}^v\\cdot\\mathbf{a^i}vi=Wv⋅ai，其中Wv\\mathbf{W}^vWv为参数矩阵，是要学习的参数 利用求得的attention score对vi\\mathbf{v^i}vi进行加权：b1=∑α1,i′v1\\mathbf{b^1}=\\sum \\alpha'_{1,i}\\mathbf{v^1}b1=∑α1,i′​v1，从而得到b1\\mathbf{b^1}b1 可以参照下图直观地理解上述步骤 矩阵视角下的计算过程# 下面三张图非常直观地展示了 产生qi,ki,vi\\mathbf{q^i},\\mathbf{k^i},\\mathbf{v^i}qi,ki,vi 计算attention score 计算输出向量bi\\mathbf{b^i}bi 上面提到的矩阵视角下的计算过程可以归结为下图，我们也可以发现，对于一层self attention层而言，需要学习的参数只有Wq,Wk,Wv\\mathbf{W}^q,\\mathbf{W}^k,\\mathbf{W}^vWq,Wk,Wv 前面说到FC可能在考虑整个向量序列的情况下有大量的参数，比如说输入的向量序列为10000个100x1的向量，如果在考虑整个向量序列的情况下要产生10000个10x1的向量，用FC需要的参数数量级为(10000x100)x(10000x10)，而如果用self attention，那么Wq\\mathbf{W^q}Wq的元素个数数量级为100x100。 Self attention的优化# Multi-head Self attention# 考虑到在不同的视角下，一个向量和其他向量会有不同的关联性，所以我们可以用不同系数矩阵，最终产生多组向量序列输出。 我们也可以对多组向量序列输出进行拼接、变换，得到一组输出向量序列 Positional Encoding# 在之前的产生输出向量序列的操作中，我们可能忽略了输入向量序列在坐标或者时间尺度上的相关性，而这有时是很重要的，比如在音频识别中，识别一个音符考虑的就是在一小段时间内的信号，那么这一小段时间内向量的相关性就极强。 所以我们可以通过在输入向量序列上加一个用来代表位置的向量，来为输入向量增加坐标/时间信息。这个代表位置的向量可以是直接计算得到的，也可以是通过神经网络学习得到的。 Self attention 和其他网络对比# CNN# 其实图像也可以看作是向量的序列——比如一张100x100x3的图片，可以看作是100x100个向量（一个像素的RGB三通道值作为向量）。 通过设置如果attention score合适，是不是也可以表现出CNN的效果？而且self attention可以打破CNN中由于卷积核带来的像素只与周边像素作用的限制，让一个像素可以和离它较远的像素作用。 所以，self attention是更灵活的CNN，而CNN是简化的self attention 在二者训练效果对比中也可以发现，对于小的数据集，自由度比较小的CNN表现得更好，对于大数据集而言，self attention表现的效果更好。 RNN# 相比RNN而言，self attention由于并行运算可以获得更快的运算速度，同时相比RNN不易考虑到距离一个向量在坐标尺度下比较远的向量，self attention可以通过attention score充分考虑到一个向量在坐标尺度下距离比较远的向量。 GNN# self attention可以看作是一种特殊的GNN，如果两个点直接由连线，那么attention可以设为1，否则直接设为0。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/"},{"title":"李宏毅机器学习-lect6 GAN","text":"GAN的基本思想 生成器 “对抗”的含义 训练过程概述 GAN的理论 目标函数 JS divergence的缺陷 Wasserstein distance 评价生成器 评价指标 图像质量 图像多样性 评价函数 Inception score FID Conditional GAN（条件式生成） Conditional-GAN+监督学习 Cycle-GAN 为什么需要Cycle-GAN Cycle-GAN的结构 Cycle-GAN的其他应用 GAN的基本思想# 生成器# GAN的中文名叫做对抗生成网络，生成网络的一大特点是让AI具有创造力，所以和CNN、Transformer的一大不同点就是对于一个输入而言，由于生成器同时接受一个随机变量，所以输出并不是固定的。如下图所示，生成器接受x和随机变量z作为输入，其中随机变量z需要来自一个简单到我们可以写出其表达式的随机分布（这样我们才能从中采样），然后输出y，y是一个复杂的随机分布。 那么为什么输出需要是随机分布，而不是像CNN一样固定的值呢？最重要的原因是，在很多问题中答案不一定是唯一的，如果使用监督学习，可能有两大坏处： 给出的结果可能是模糊、不合实际的 比如在某个小游戏的视频预测任务中，AI需要根据之前的帧产生接下来的动画，如果使用监督学习，会发现在路口处小人可能会分裂成两个，原因是训练资料内在入口处小人可能向左转或向右转，这两种情况本来都是合理的，但是如果使用监督学习，AI在训练时不能明确哪种是正确的答案，那么为了使得loss最小，AI就会给出模棱两可的答案——分裂，而这可能是不合实际的。 AI缺乏“创造力” AI在某些特定的人物需要一定的“创造力”，比如让AI绘画，要求画出有红眼睛的人物，答案显然不是唯一的；又如让AI回答“你知道辉夜是谁吗？”，也有不止一种回答。 “对抗”的含义# 为什么起名叫对抗生成网络，原因来自生成器的训练是伴随着它的“对手”——鉴别器的训练进行的。在生成器的训练中，我们会同时训练一个鉴别器，用来判定生成器的表现。 以生成二次元人物头像为例：生成器要生成二次元人物头像，而鉴别器要鉴别生成的图像像不像人工绘制的二次元人物头像，此时为了“骗过”鉴别器，生成器要让自己生成的图像更像真人绘制的二次元人物头像，而鉴别器为了更好地把生成器生成的二次元头像检查出来，也要提升自己的鉴别水平，最终二者相互促进，使得生成器生成非常逼真的二次元人物头像。 训练过程概述# GAN的训练过程，大致可以概述为如下步骤： 固定生成器参数，训练鉴别器 因为一开始未经训练的鉴别器是毫无鉴别功能的，所以我们应当先训练鉴别器。训练时同时给鉴别器喂入真实样本（标记为1）和生成器随机生成样本（标记为0）。 固定鉴别器参数，训练生成器 在鉴别器有一定鉴别功能、能给生成器的表现打一个相对合理的分数后，就可以开始训练生成器了。此时将生成器和鉴别器拼接成一个大网络，固定鉴别器参数，更新生成器参数，使得最终输出分数能提高。 重复前两个步骤 通过重复，最终可以获得良好的生成器。下图概述了上面两个步骤 GAN的理论# 目标函数# 让我们先考虑生成器的优化目标，以图像生成任务为例，对于生成器而言，目标就是让产生的图片的分布和真实图片的分布是类似的。数学表达式可以写成 G∗=arg⁡min⁡GDiv(PG,Pdata) G^{*}=\\arg \\underset{G}{\\min} Div(P_G, P_{data}) G∗=argGmin​Div(PG​,Pdata​)其中Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)表示产生图片的分布和真实图片分布的距离。 然后再考虑鉴别器的优化目标，对于鉴别器而言，目标是能鉴别出生成的图片。那么在鉴别器优化后，由于鉴别器能力的提升，算出的Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)会变大。我们就可以把目标写成 D∗=arg⁡max⁡DV(D,G) D^{*}=\\arg \\underset{D}{\\max} V(D, G) D∗=argDmax​V(D,G)其中V(D,G)V(D, G)V(D,G)的值和https://arxiv.org/abs/1406.2661里提到的JS divergence在数学形式上是相近的，可以写成 V(G,D)=Ey∼Pdata [log⁡D(y)]+Ez∼Pz(z)[log⁡(1−D(G(z)))] V(G, D)=\\mathbb{E}_{y \\sim P_{\\text {data }}}[\\log D(y)]+\\mathbb{E}_{z \\sim P_z{(z)}}[\\log (1-D(G(z)))] V(G,D)=Ey∼Pdata ​​[logD(y)]+Ez∼Pz​(z)​[log(1−D(G(z)))]上式中第一项代表鉴别器对真实图片的输出，第二项代表鉴别器对生成图片的输出，V(G,D)V(G,D)V(G,D)的形式是很像Cross Entropy的。在优化时，第一项只与鉴别器相关，第二项与鉴别器和生成器都相关。那么最终的优化目标表达式可以写作 min⁡Gmax⁡DEx∼pdata (x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))] \\min _{G} \\max _{D} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] Gmin​Dmax​Ex∼pdata ​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))]这是一个min-max模型，代表着鉴别器和生成器训练时的“对抗”。 JS divergence的缺陷# 刚刚我们提到了用JS divergence来衡量生成图片和真实图片之间的差异，但是JS divergence存在比较严重的问题。 生成图片分布PGP_{G}PG​和真实图片分布经PGP_{G}PG​常重叠率是很低以至于可以忽略的，而且就算真的有一部分重叠了，也可能因为采样不足而在采样分布上没有重叠。 而在PGP_{G}PG​和PGP_{G}PG​重叠率为0时，JS divergence的值是恒定的（为log2），此时就无法衡量PGP_{G}PG​和PGP_{G}PG​的真实差距 Wasserstein distance# 在https://arxiv.org/abs/1701.07875中，作者提出了WGAN所用到的Wasserstein distance。Wasserstein distance衡量的是将PGP_{G}PG​“移动”到PGP_{G}PG​的最小平均距离。表达式可以写成 min⁡Gmax⁡D∈1−LipschitzEx∼pdata (x)[D(x)]−Ez∼pz(z)[D(G(z))] \\min _{G} \\max _{D\\in{1-Lipschitz}} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[ D(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[D(G(\\boldsymbol{z}))] Gmin​D∈1−Lipschitzmax​Ex∼pdata ​(x)​[D(x)]−Ez∼pz​(z)​[D(G(z))]此时鉴别器有约束条件，这个条件要求鉴别器需要足够平滑，如果没有这个条件，鉴别器的训练会不收敛。 WGAN依然有缺点，有几种提升方案 参数限制（Force the parameters w between c and -c） WGAN-GP（gradient penalty）https://arxiv.org/abs/1704.00028 SNGAN（Spectral Normalization → Keep gradient norm smaller than 1 everywhere）https://arxiv.org/abs/1802.05957 评价生成器# 评价指标# 图像质量# 为了评估生成的图像与真实图像是否相似，可以用一个CNN来对生成的图片进行分类，如果分类结果是明确的，即有一类的值很高，那么说明生成的图像质量很好。 图像多样性# 除了评估图像质量以外，还需要评估图像多样性，因为哪怕生成的图像质量很好，如果遇到了mode collapse，那么可能生成的图像都十分相似，这显然不是我们想要的。 除了mode collapse以外，还可能遇到mode dropping的问题，就是生成的图片看上去多样性还行，但是实际上只是真实分布中的一部分而已。 评价函数# Inception score# Inception score借助CNN，将所有生成图片经过CNN后的输出取平均，如果各类的值比较平均，那么说明多样性是足够的，此时Inception score较高。同时Inception score也会考虑单张图片经过CNN后的分布，如果分布集中，说明图像质量高，此时Inception score较高。 总的来说，Inception score综合考虑图像质量和图像多样性，图像质量和多样性越高，则Inception score越高。 FID# FID衡量的是生成图片分布和真实图片分布的距离。同样借助CNN，将生成图片和真实图片经过CNN，不同的是此时收集的是softmax前的输出，最后得到两个分布，将两个分布视为高斯分布，然后计算两个分布间的Frechet distance。显然FID越小代表两个分布约接近，说明生成器性能更佳。 Conditional GAN（条件式生成）# 很多时候，我们希望GAN能生成符合一定条件的结果，比如AI绘画里我们会根据自己的喜好提出一些要求。所以就需要Conditional GAN。 训练Conditional GAN的时候，以生成图片任务为例：对于生成器而言，我们以条件作为x，伴随着随机变量z输入；对于鉴别器而言，应当同时输入图片和条件，以及一个标量（这个标量仅当图片为真实图片，且与要求对应时才是1，否则都是0），鉴别器此时输出的标量一方面衡量图片是否是真实图片，另一方面还要衡量图片和条件是否对应。 Conditional-GAN+监督学习# 但是有时AI会“过于具有想象力”，以至于超出条件的约束，比如在通过建筑结构图生成建筑照片的时候，只用GAN的结果相比结构图多了一个类似烟囱的结构。为了克服这个问题，有时会结合GAN和监督学习，将AI的“想象力”控制在合理的范围内。 比如在[1611.07004] Image-to-Image Translation with Conditional Adversarial Networks (arxiv.org)内，在目标函数中包含了L1范数LL1(G)=Ex,y,z[∥y−G(x,z)∥1]\\mathcal{L}_{L 1}(G)=\\mathbb{E}_{x, y, z}\\left[\\|y-G(x, z)\\|_{1}\\right]LL1​(G)=Ex,y,z​[∥y−G(x,z)∥1​]，最终优化目标为 G∗=arg⁡min⁡Gmax⁡DLcGAN(G,D)+λLL1(G) G^{*}=\\arg \\underset{G}{\\min} \\underset{D}{\\max} \\mathcal{L}_{c G A N}(G, D)+\\lambda \\mathcal{L}_{L 1}(G) G∗=argGmin​Dmax​LcGAN​(G,D)+λLL1​(G) Cycle-GAN# 在很多时候，我们会遇到没有成对的输出和输出的数据集，比如想要做真人头像和二次元人物头像的风格转换，但是很多真人头像都没有对应的二次元人物头像。 为什么需要Cycle-GAN# 以图片风格转换为例，如果使用一般的GAN，那么可能导致生成的图片风格正确，但是和输入的图像没什么关系，比如把李宏毅老师的头像输入，可能产生的是辉夜的头像。那么如果用Conditional-GAN做风格转换，往往又需要成对的训练资料。 Cycle-GAN的结构# 以风格转换为例，为了保证风格转换后的图片和原始图片相关，会再加入一个用于将风格转换后的图片转换回原始风格的生成器，那么为了要将风格转换后的图片能转换回原始风格，就会要求生成的图片和原始图片是相关的。相应的，我们需要一个鉴别器来判断转换回原始图片的效果。所以如下图所示Cycle-GAN包含了将两种风格的图片互相转换的生成器和对应的鉴别器。 Cycle-GAN的其他应用#","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/"},{"title":"李宏毅机器学习-lect3 CNN","text":"CNN 的想法 卷积层（Convolutional layers） 卷积核 步长 （Stride） 填充 （Padding） 参数共享（Parameter Sharing） 卷积层的运行过程 卷积层的两种理解方式 池化层（Pooling layers） Flatten layers CNN 的缺点 CNN 的想法# 对于图片分类的任务，我们可以采用让特征识别（这与人类分类物体的方法是类似的）的方法，让每一个神经元只与部分区域（Receptive field）关联，而不需要每一层都full connect。 所以，CNN是为了影像的特性而生的，把CNN用于影像领域外的任务要仔细思考是否出现影像类似的特性。 卷积层（Convolutional layers）# 卷积核# 卷积核大小表示局部特征区域（即Receptive field）的大小 卷积核一般大小取3x3，同时包含RGB三个通道 Q：卷积核大小只有3x3，如果图片尺寸比较大，3x3会不会无法识别特征？ 这个问题将在下面回答 步长 （Stride）# 每次卷积核移动的长度，一般设为1或2，因为我们希望Receptive field之间是有重叠的，因为如果Receptive field之间完全没有重叠，那么如果pattern出现在两个Receptive field的交界上，特征就难以被识别。 填充 （Padding）# 移动卷积核时，如果步长大于1，则移动时卷积核可能会超出图片范围，则需要在边上填充一些值，常见的方法有补0法、取平均法等。 参数共享（Parameter Sharing）# 对于一个特征，可能出现在图片不同的位置，而对于一个区域，有一组神经元负责，每个神经元负责识别不同的特征，所以此时可以让负责不同区域，但功能相同的神经元享有相同的参数，从而减少参数数量。 对于每一个区域，有一组神经元负责，每个神经元有一组参数，这一组组参数叫做filter，所有区域共享一组filter 卷积层的运行过程# 之前提到，图片中每一个小区域有一组神经元负责，每一个神经元的参数叫做filter，所有小区域共享一组filter，那么卷积层可以看作每一个filter对图像分别作用，得到一组图像，所有的filter对图像作用后，得到了新的图像，图像的channel数则为filter的数量。这样的一张图片叫做特征图像。 所以一张图像经过卷积层后，会得到一张特征图像。之前有提到，卷积核大小只有3x3时会不会无法识别较大特征，这是不会的，因为在下一个卷积层中对特征图像做卷积时，若卷积核为3x3，步长为1时，则相当于对5x5大小区域卷积。 卷积层的两种理解方式# 池化层（Pooling layers）# 对于一张较大的图片而言，采样时少采样一些点并不会影响图像是什么 池化层并没有参数，其操作时固定的，相当于一个算符 常用的池化方法有Max Pooling，过程如下图所示，一般而言，池化时分组大小为2x2 一般而言，池化常常在卷积层后使用，如一个或两个卷积层后跟一个池化层，用于缩小图片，从而减小运算量。但是这对于网络的效果而言可能是由损害的，因为如果特征特别细小，则池化可能会漏过特征。 Flatten layers# 图像经过一系列卷积、池化后，得到小的特征图像，此时这个特征图像代表图片中较大的、全局的特征，此时就可以把图像展平，然后通过全连接层，经过softmax归一化后，得出分类的结果。 其实到这里，和回归问题里用到的神经网络是类似的，只不过回归问题的特征是显式的，所以一开始就可能是全连接层，而影像类任务中，一开始需要先提取特征，最后再让特征经过全连接层计算。 CNN 的缺点# CNN很难处理图片的缩放、旋转，所以我们需要数据增强（data augmentation）","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/"},{"title":"李宏毅机器学习-lect5 Transformer","text":"Transformer是什么 Transformer的应用 Transformer的架构 Encoder Decoder Autoregressive Transformer Masked Self-attention Cross attention Non autoregressive Transformer(NAT) Transformer的训练 loss的来源 训练的一些tips Teacher Forcing Copy Mechanism Guided Attention Beam Search Exposure bias Transformer是什么# Transformer可以理解为是Sequence-to-sequence（简称Seq2seq）的模型，它接受向量序列作为输入，输出向量序列。 Transformer的应用# Transformer的应用除了上图提到的语音识别、机器翻译、语音翻译以外，还有 聊天机器人 句子词性分析 多标签分类 物体检测 Transformer的架构# Transformer由编码器（Encoder）和解码器（Decoder）组成，前向传播的过程是Encoder将输入向量序列编码产生新的向量序列，然后Decoder将编码的向量结合begin向量（标记着位置，是一个one hot向量）产生第一个输出向量，然后把产生的第一个向量再输入Decoder，产生第二个向量，直到产生end向量为止，代表着输出完成。 如下图所示，是\"Attention is all you need\"文中提出的Transformer的内部结构，左半边是Encoder，右半边是Decoder。接下来具体解释Encoder和Decoder的组成。 Encoder# Encoder由一系列block组成，每一个block里面都包含了self-attention层和FC层。 实际上，Transformer里融合了ResNet的思想，再self-attention产生了输出向量序列后，还会加上输入的向量，然后在一起进行layer normalization，就是对每一个向量，减掉其平均值后再除以标准差。进行了layer normalization后的向量才会被输入FC。 如下图所示，FC的部分也有residual的部分，经过了FC、layer normalization后，才得到一个block的输出。 此时我们回顾Encoder的架构，首先对于Encoder中的一个block，要做的事情有 Positional Encoding（位置编码，lect4的笔记里有提到） 结合ResNet性质的Self-attention+layer normalization 结合ResNet性质的FC+layer normalization Decoder# Autoregressive Transformer# Autoregressive transformer的前向传播过程大致是结合Encoder的输出和Begin向量得到第一个输出向量，以语言识别为例，然后取输出向量中对应概率最大的字对应的one hot向量作为第一位输出的结果，然后再用第一位输出的结果输入Decoder，产生下一个输出，直至产生End向量，才代表输出结束。 下图是Decoder的内部架构，Positional encoding和FC两个部分和Encoder是差不多的，所以下面重点分析Masked Attention部分和Masked Attention后的Attention block两个部分。 Masked Self-attention# Masked Self-attention和一般的Self-attention不同之处在于：产生第一个输出向量时，只能考虑第一个输入向量，产生第二个输入向量时，只能考虑第一、第二个输入向量，以此类推。下面两张图很好地解释了这个机制 Cross attention# 这部分是Decoder内部架构图中第二个attention的模块，下图很好地说明了cross attention地机制。BEGIN向量和“机”向量进行Masked Self-attention，产生q向量，然后再同Encoder产生的输出得到attention score，再加权，通过FC layer得到第二个输出向量。 最重要的一点就是这个过程中的q向量不是来自于Encoder的输出，而是来自Mask Self-attention的输出。 Non autoregressive Transformer(NAT)# 和autoregressive transformer（AT）不同，NAT一次性产生所有的输出向量，然后截取END向量之前的向量作为最后的输出序列。 相比AT，它的优势在于的产生速度快，并且可以控制输出长度，但是它的效果往往不如AT。 Transformer的训练# loss的来源# loss的来源可以是每个输出的向量与正确向量的交叉熵，在训练时我们希望交叉熵最小化。 训练的一些tips# Teacher Forcing# 为了防止训练时出现因第一个向量输出错误导致接下来的训练错误这样“一步错，步步错”的情况，所以我们在训练时把正确答案输入给decoder。 Copy Mechanism# 有时适当地从输入中进行一些copy，可以更好地完成任务。比如在聊天机器人训练中，可以对人名进行copy，因为一个人名在训练资料中出现的次数可能很少，网络无法通过学习习得输出正确人名的能力，所以对人名进行直接copy。 Guided Attention# Guided Attention，可以理解为用先验知识来限制self attention层的注意力机制，比如在语音识别中，一个音节的识别只与对应时刻的向量及其周边向量有关。所以应该把attention限制在这个时刻附近较小的范围内。 Beam Search# 可以把找输出向量序列看作是树形搜索，所以就把问题看作是设计合适的搜索算法，使得全局的loss最小。 Exposure bias# 训练时我们把正确答案直接喂给decoder，但是如果在测试的时候产生了错误的向量，可能就会产生mismatch，从而影响后面的输出。所以我们可以在训练时就适当地喂入错误的向量，使得模型的适应能力更强。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/"},{"title":"李宏毅机器学习-lect7 Self-Supervised Learning","text":"自监督学习 BERT 什么是BERT BERT的训练方法 Masking Input Next Sentence Prediction 怎么使用BERT 文章情感判断 句子词性标记 逻辑判断 摘录型问答 BERT的评估 为什么BERT有效 解释与佐证 对之前解释的质疑 Multi-lingual BERT GPT GPT的训练 GPT的使用 自监督学习在语音和图像领域上的应用 概述 Generative Approaches 语音 视觉 Predictive Approaches Contrastive Learning SimCLR Bootstrapping Approaches Simply Extra Regularization VICReg 自监督学习# 在监督学习中，我们用成对的训练资料进行训练（比如中译英任务中中文文章与其对应的英文翻译），而在无监督学习当中，是没有成对的训练资料的（比如不带英文翻译的中文文章）。 自监督学习是无监督学习的一种，他将训练资料x的分为x’和x’‘，用x’进行训练，然后用x’'进行验证。 自监督学习现在被广泛用于大模型的预训练，比如BERT和GPT。 BERT# 什么是BERT# BERT是芝麻街里的人物一个巨大的预训练语言表征模型，它的全称是Bidirectional Encoder Representations from Transformers。它强调了不再像以往（Transformer）一样采用传统的单向（每一个token的注意力只能在之前的token上）语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，BERT自监督训练时每一个token的注意力可以放到全局。 BERT在预训练后，只需要接上一个额外的输出层，给予一点带标注的资料微调，就可以在翻译、问答等下游语言任务中有优秀的表现。 从BERT开始，一大批巨大模型开始出现，如GPT、T5等。 BERT的训练方法# Masking Input# 用一句话概括Masking Input就是让BERT学会做“填空题”，这里的填空题是指在把输入的一些token换成特定的token或者随机的token，从而使完整的句子中出现空位，BERT需要学会预测这些被替换的token是什么，训练时目标是最小化输出token和正确token的交叉熵。 Next Sentence Prediction# 这个方法是输入两个句子，判断sentence 1与sentence 2在一篇文章中的前后关系。 怎么使用BERT# 经过预训练后的BERT学会了做“填空题”、“排序题”，此时的BERT像干细胞，没有具体的功能，但有分化为执行具体任务的能力。 要让BERT能做具体的任务，我们需要在BERT后加入一个输出层（BERT其实和Transformer的Encoder类似，所以后面需要输出层），然后再给予一点具体任务的成对资料进行训练（这个过程称为Fine-tune，即微调），就可以让BERT做具体的任务。微调前，输出层的参数是随机的，而BERT的参数是经过预训练的。 下面举四个例子具体说明如何使用BERT 文章情感判断# 预训练的BERT接受句子和CLS token作为输入，在预训练的BERT后接一个参数随机初始化的线性输出层，线性输出层将BERT输出的其中一个向量作为输入，然后用（文章，情感）这样成对的资料微调模型，就可以得到良好的效果。 句子词性标记# 和刚刚不同的是，此时需要输出的长度和文章长度是一致的，需要把句子中每个词在BERT中的输出都接入输出层，同样给予一点成对资料训练即可。 逻辑判断# 逻辑判断是指输入两个句子，判断两个句子是否有逻辑关系。此时只需要在两个句子之间加入一个SEP token即可。 摘录型问答# 摘录型问答是指让模型从文章中摘录一部分作为答案（这种问题保证答案在文中可以摘录），输出层需要输出两个token，代表答案在文章中的始末位置。 BERT接受输入的方式和逻辑判断中相似，只是两个句子换成了问题+文章，此时需要两个向量（长度和BERT输出的向量是相同的，参数是随机的），向量和文章经过BERT后产生的各个向量作内积，产生每个位置作为答案开始或结束时的概率，然后分别选择概率最高的位置作为答案的开始/结束。 BERT的评估# BERT常用GLUE评估，GLUE里面有九项语言相关任务，通过BERT在九项任务上的性能综合评估BERT。 https://arxiv.org/abs/1905.00537中总结：随着时间推进，BERT的表现越来越好，甚至在多项任务上都能超过人的表现（水平直线）。 为什么BERT有效# 解释与佐证# 最常见的一种解释是BERT通过做“填空题”或其他预训练任务，真的“学会了”语言，能根据上下文将文字映射到语义空间上，相近意思的词在语义空间上的映射会比较接近。比如同样是“苹”这个字，吃苹果中的“苹”和“果”、“草”的映射比较接近，苹果手机中的“苹”可能和“电”的映射比较接近。所以在BERT对每个字做了有效的映射后，输出层可以很容易地输出正确结果。 取BERT对不同语境下的“苹”的映射，求各个映射之间Cosine Similarity，发现相近语义的“苹”字之间Cosine Similarity比较高，不用语义的“苹”字之间Cosine Similarity比较低。这个实验在一定程度上能说明BERT为什么有效。 对之前解释的质疑# https://arxiv.org/abs/2103.07162中指出，用语言资料训练的BERT用在蛋白质、基因、音乐领域的任务也会有比不用预训练的方法表现更好，所以BERT有效的原因真的是因为BERT“学会了”语言吗？BERT有效的原因目前还在探索中。 Multi-lingual BERT# 之前提到的BERT都是单一语言的BERT，但是实际上，预训练BERT的时候可以用多语言的资料进行训练。比如下图指的就是让BERT做不同语言的“填空题”。 https://arxiv.org/abs/1909.09587中提出在zero-shot（提问时没有给出任何参考案例）阅读理解任务中，多Multilingual-BERT在只经过英文问答题的微调后，做中文的阅读题时竟然还能有不低的正确率。 一种解释是对于Multilingual-BERT，其实不同的语言的相同语义的词在语义空间上的映射是很接近的，但是训练这样的Multilingual-BERT是需要大量的资料与运算资源，才能让BERT学到不同语言之间的联系。 但是，如果语言之间真的没有差距，那么比如做英文问答时不会冒出别的语言吗？而这实际上是不会的，因为其实不同语言之间还是有一定的距离的。所以李宏毅课题组进行了一项实验（https://arxiv.org/abs/2010.10041），把中文词汇映射的平均值减去英文词汇映射的平均值，得到两种语言之间的差距。然后让Multilingual-BERT读入英文句子，加上英文到中文之间的差距，竟然能输出中文的句子了，而且部分中英词汇在语义上是相同的。所以一定程度上说明了Multilingual-BERT中蕴藏了语言之间的联系。 GPT# GPT的训练和使用其实和BERT是很类似的。 GPT的训练# 与BERT相似的是，GPT同样使用自监督学习进行预训练，但是预训练的任务是预测句子中的下一个token。 GPT的使用# 和BERT相似的是，GPT在迁移到下游任务时也需要一点成对的训练资料，但是和BERT不同的是，GPT的模型比BERT大得多，以至于连微调GPT都是很困难的，所以GPT在微调是不对GPT的参数进行调节的，并采用few-shot learning（提供问题和多个范例，让GPT给出答案）、one-shot learning（提供问题和一个范例，让GPT给出答案）甚至zero-shot learning（只提供问题，直接要求GPT给出答案）的方法进行训练。 附：李宏毅老师讲解chatGPT的训练 failed getting oembed item.(url=https://www.youtube.com/watch?v=e0aKI2GGZNg) 自监督学习在语音和图像领域上的应用# 概述# 自监督学习不仅可以用在文字上，也可以用在语音和图像领域上。语音/图片版本的BERT都可以通过大量未标注的资料来进行预训练。 语音领域上，李宏毅课题组提出了SUPERB（https://arxiv.org/abs/2105.01051、https://arxiv.org/abs/2203.06849），里面有十四项语音任务，用来评估语音版的BERT的表现。 视觉领域上，https://arxiv.org/abs/2110.09327内总结了各种模型在如物体检测、语义分割等多项视觉任务上的表现。 下面介绍五种训练语音/视觉领域的BERT的方法。 Generative Approaches# 文字版的BERT和GPT训练的中，我们会让BERT和GPT去生成一些字符（也许在是在文章中被遮挡，也许是预测接下来的字符）。所以就希望能把这个有效的方法迁移到语音和视觉领域。 语音# 但是语音和文字不同的是，masking时需要遮挡多个向量（https://arxiv.org/abs/1910.12638），因为语音中各个向量和其相邻的向量是差不多的，所以如果只遮挡一个向量，模型可能只是学会了对被遮挡向量的周围两个向量做一个内插罢了。 TERA则采用另一种遮挡方法，它并非遮挡某些向量，而是遮挡各个向量的部分维度，让模型去补全被遮挡的维度，这种方法可以让模型学到更多语音的知识。 如果想用GPT训练的方法，让模型预测接下来的语音信号也是可以的，只不过和文字不一样的是，需要预测n个向量（一般而言n&gt;3），理由和masking时遮挡多个向量理由是相似的。 视觉# 对于图像，只需要把图像拉成像素向量的序列就行了，接下来直接套用BERT和GPT的方法就行了。 Image GPT (openai.com) 但是这个方法的问题在于语音和图像包含了比文字更多的信息，所以用生成的方法让模型学习还原/产生语音或图像是比较困难的。 Predictive Approaches# 前面说到，让模型学习还原/生成完整的语音/图像是困难的，所以就希望用简单而又足够让模型学习到语音/图像特性的任务预训练模型。 https://arxiv.org/abs/1803.07728中提出，可以让通过模型学习判断一张图片被旋转的角度来预训练模型，除了这个方法以外，当然也可以用其他的简单任务来预训练模型。 语音上，也可以用相似的方法，比如让训练模型判断两端语音之间间隔的时间（https://ieeexplore.ieee.org/document/9060816）。 但是设置简单任务训练模型需要对语音/图像领域的信息有比较深入的理解，所以有没有一些更通用的方法呢？ HuBERT中采用对k-means的方法对语音中的向量进行聚类，然后采用训练模型去预测被遮挡的向量的类别，在图像领域上Deep Clustering for Unsupervised Learning of Visual Features一文中也提出了类似的方法。 Contrastive Learning# 前面的两类方法都需要让模型学习去产生一些东西，但是能不能让模型在不产生任何东西的情况下学到语音/图像的特征呢？ SimCLR# SimCLR中指出，可以通过把一张图像经过增强，得到不同的图像，训练模型使相似的图像（认为两张相似图像是“positive examples”）输出的向量是尽可能相近的，而训练模型使得不同的图像（认为两张不相似图像是“negative examples”）输出的向量是尽可能相差比较大的。对于语音领域，也有对应的Speech SimCLR。 但是对于Contrastive Learning而言，难的是判定什么是negative examples，两个为negative examples的样本不能差异太大（比如两张图像连背景颜色都不同，那么可能模型只学会了判断背景颜色），也不能差异太小（比如两张图片都是猫），所以需要复杂的方法选出negative examples。 那有没有什么方法能避开选择negative examples呢？下面两种方法可以做到 Bootstrapping Approaches# 如果为了避开negative examples，直接只用positive examples来训练模型，希望模型输出的向量尽可能相近，那么可能会发生“collapse”，就是模型对于任何图片都输出同样的向量。 Bootstrapping则采用一种很巧妙的方法，它让两张图片经过的路径不对称，其中一张图片只经过Encoder（暂且称为左路径），另一张图片经过Encoder后要再经过一个只有几个layer构成的简单predictor（暂且称为右路径），然后只通过右路劲更新参数，再把右路径中的Encoder中参数复制给左路径中Encoder。最后希望左右路径产生的向量是尽可能接近的，这种方法可以避开negative examples，还能避免出现“collapse”。 Simply Extra Regularization# VICReg# 这部分没太听懂quq，还希望dl教我……","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/"},{"title":"李宏毅机器学习-为什么训练会失败","text":"Training loss很高 这时发生了什么 训练方法 Batch Batch对训练效率影响 Batch对训练效果影响 Momentum Learning rate Loss function Training data的loss很小 过拟合 调整模型复杂度 添加更多数据 Mismatch Training loss很高# 这时发生了什么# 当微分接近0时，训练会停止。但是此时不一定是在极值点，也可能是在鞍点（saddle point）。 其实训练停止时，微分也不一定很接近0，也就不一定是在saddle point，这个后面会提到 通过求loss function的Hessian矩阵，判断Hessian矩阵正定性可以判断当前是否处在saddle point——H不定时则处在saddle point。 对于神经网络的loss而言，Hessian矩阵是一个维度非常高的矩阵，所以Hessian矩阵正定或负定的可能性很低，所以大部分训练停止的情况其实都是遇到了saddle point。 训练方法# Batch# 每一个epoch开始时，会随机分割batch，训练时每次依次用各个batch计算出梯度后更新参数。 Batch对训练效率影响# 一个很直观的想法是batch越大，训练时在一个batch上花的时间越久，但是由于GPU有并行运算的能力，其实batch在不是特别大的时候训练时间和batch size=1时相比几乎没有增长。所以适当的增大batch可以提高训练效率。 Batch对训练效果影响# 从下图可以看出，小batch的训练准确率比较高。 为什么小的batch训练准确率高呢？原因在于使用小的batch时，每个batch之间的loss function可能存在细微的差别，当一个batch训练时卡在saddle point，对另一个batch可能不是saddle point，可以继续训练。 Momentum# 训练时可以通过把上一步的移动和当前计算出的梯度结合，形成新的优化参数的方向，这有利于跳出saddle point，甚至可以帮助“翻越”比较小的山坡，使得loss function得到有效降低。 Learning rate# 其实如果learning rate比较大，那么可能会出现还没到saddle point就停下来的可能（此时梯度的模比0还是会大比较多的），如下图所示，loss function的值在两条绿线所指的点反复横跳。 所以我们需要动态调整learning rate，使得尽量不会出现还没到saddle point就停下来的情况 为了动态调整learning rate，我们可以采取两点 对每个参数动态调整learning rate θit+1=θit−ηgit→θit+1=θit−ησitgit\\theta_i^{t+1}=\\theta_i^t-\\eta g_i^t\\rightarrow\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}g_i^tθit+1​=θit​−ηgit​→θit+1​=θit​−σit​η​git​ t代表随着训练轮次而改变，i代表对不同的参数 常见的求σit\\sigma_i^tσit​的方法有Root mean square（σti=1t+1∑0t(git)2\\sigma_t^i=\\sqrt{\\frac{1}{t+1}\\sum_0^t (g_i^t)^2}σti​=t+11​∑0t​(git​)2​ ）和RMSProp（σit=α(σit−1)2+(1−α)(git)2\\sigma_{i}^{t}=\\sqrt{\\alpha\\left(\\sigma_{i}^{t-1}\\right)^{2}+(1-\\alpha)\\left(g_{i}^{t}\\right)^{2}}σit​=α(σit−1​)2+(1−α)(git​)2​ ，比较近的轮次的梯度影响比较大） 对所有参数进行learning decay 上式中η→ηt\\eta\\rightarrow\\eta_tη→ηt​ 我们在训练时，经常使用Adam，这个优化方法结合了动态调整RMSProp方法调整learning rate和momentum。一般来说比较有效。 Loss function# 选用不同的loss function，会有不同的error surface，所以选择合适的loss function，可以得到更容易进行优化的error surface。 Training data的loss很小# 若testing data的loss比training data的loss大很多，也不一定是过拟合~ 可能的原因 过拟合 mismatch 过拟合# 过拟合的解决方法有两种： 调整模型复杂度 添加更多数据 调整模型复杂度# 训练数据较多：增加复杂度 训练数据较少：降低复杂度 添加更多数据# 搜集更多数据 数据增强（Data augmentation） 左右翻转图片 放大图片局部 …… 但是数据增强要用合理的方式，比如图片识别不能出现使用中基本不出现的形式，如图片上下颠倒 Mismatch# mismatch指的是testing data的结构与training data的结构不一致","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/"},{"title":"记手搓ResNet的经历","text":"如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用 前言 ResNet基本思想 代码 各部分详解 Residual block resblock_basic类 resblock_bottlenect类 resnet类 References 前言# 在上学期的机器视觉大作业中我用到了ResNet50-Unet，寒假中做分类任务时又用到了ResNet，但是之前我用ResNet要么是pip之后直接import，要么是参照hw里面助教给的初始代码进行增删。本着搞懂ResNet这么一个经典模型的心态，我决定自己手搓一遍ResNet（好吧其实还是有参照，但是在参照的基础上加了一点东西）。 ResNet基本思想# ResNet通过引入直接连接的旁路（shortcut），减少了反向传播时梯度消失的问题，使得模型能搭的更深，更不容易过拟合。 下表是各种CNN架构在ImageNet数据集上的top-5 error rate，可以看到ResNet相比VGG等其它架构，有着更好的效果。 代码# 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190\"\"\"Implementation of ResNet with pytorchSimple usage: from resnet_pytorch import * classifier = resnet()All usage: demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=\"resnet101\") Input no parameters: classifier = resnet(class_num=16) # return resnet50References:[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1[2] https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py[3] 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197\"\"\"import torchimport torch.nn as nnclass resblock_basic(nn.Module): \"\"\" the block for resnet18 and resnet34 \"\"\" # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( # if the kernel size equals to 3, the padding should be 1 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size and number of channels are equal to the input, # the shortcut path do nothing self.shortcut = nn.Sequential() # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resblock_bottleneck(nn.Module): \"\"\" the block for resnet50, resnet101 and resnet152 \"\"\" # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 4 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), # maxpooling is replaced by convolution layer with stride unequal to 1 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resnet(nn.Module): def __init__(self, block=resblock_bottleneck, channel=3, filter_list=None, block_num_list=None, class_num=10, net_type=None): \"\"\" block: type of block, default: bottleneck channel: the channel of image, 1 for gray image and 3 for RGB image. default: 3 filter_list: the filter numbers of each blocks' first layer. default: None block_num_list: repeat times for each block, the length should be equal to filter_list. default: None class_num: the number of classes for classification. default: 10 net_type: the type of resnet, 'resnet50' for example. default: None demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=\"resnet101\") Input no parameters: classifier = resnet(class_num=16) # return resnet50 \"\"\" super().__init__() if block_num_list is None: block_num_list = [3, 4, 6, 3] if filter_list is None: filter_list = [64, 128, 256, 512] # different types of resnet in the original paper if net_type == 'resnet18': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [2, 2, 2, 2] elif net_type == 'resnet34': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet50': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet101': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 23, 3] elif net_type == 'resnet152': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 8, 36, 3] self.resblock_in_channel = 64 self.pre_conv_layer = nn.Sequential( nn.Conv2d(in_channels=channel, out_channels=self.resblock_in_channel, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(self.resblock_in_channel), nn.ReLU(inplace=True) ) stride_list = [1] + [2] * (len(filter_list) - 1) self.resblocks = nn.ModuleList() for i in range(len(filter_list)): self.resblocks.append(self._make_block(block, filter_list[i], block_num_list[i], stride_list[i])) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Sequential( nn.Dropout(0.25), nn.Linear(filter_list[-1], class_num) ) def _make_block(self, block, filter, block_num, stride): layers = [] strides = [stride] + [1] * (block_num - 1) for stride in strides: layers.append(block(self.resblock_in_channel, filter, stride)) self.resblock_in_channel = filter * block.expansion return nn.Sequential(*layers) def forward(self, x): output = self.pre_conv_layer(x) for block in self.resblocks: output = block(output) output = self.avg_pool(output) output = output.view(output.size(0), -1) output = self.fc(output) return output 各部分详解# Residual block# 在ResNet的原始论文中，提出了如下图两种residual block，右边的一种被称为bottleneck。前一种residual block在ResNet层数较浅时使用，如ResNet18，ResNet34；后一种residual block在ResNet层数较深时使用，如ResNet50、ResNet101、ResNet152。 resblock_basic类# 前向传播过程：residual block前向传播的过程，要经过两次卷积+batch normalization，其中第一次卷积、batch normaliztion后，需要经过ReLU，而第二次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，两个卷积层输出的channel数是相同的。 降采样方法：residual block里面不设置max pooling，而是通过卷积层中设置大于1的步长起到降采样的作用，一个block中只有第一层卷积层中的stride可能大于1，第二个卷积层的stride为1。 padding：由于卷积核大小为3x3，所以两个卷积层的padding都应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resblock_bottlenect类# 前向传播过程：residual block前向传播的过程，要经过三次卷积+batch normalization，其中前两次卷积、batch normaliztion后，需要经过ReLU，而第三次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，前两个卷积层输出的channel数是相同的，而第三个卷积层输出的channel数是前两层的四倍。 降采样方法：一个block中只有第二层3x3卷积层中的stride可能大于1，第一、三个1x1卷积层的stride为1。 padding：由于第二层卷积核大小为3x3，所以第二个卷积层的padding应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resnet类# 在原始论文中，ResNet要先经过一个7x7卷积层，然后在经过若干个residual block，最后通过FC得到输出。 预卷积层：原始论文中，预卷积层卷积核大小为7x7，所以padding=3，该卷积层步长为2，起到降采样作用，输出channel数设置为64。 residual block序列：中间的residual block序列可以用 nn.ModuleList存放，通过 _make_block函数循环添加。 自适应平均池化层：将特征图自适应转化为序列。 全连接层：设置0.25 dropout率，然后再全连接。 References# Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1 https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197","link":"/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/"},{"title":"李宏毅机器学习-lect8 Auto-encoder","text":"Auto-encoder的基本思想 Auto-encoder的训练 为什么Auto-encoder是有效的 Auto-encoder和BERT的关系 Auto-encoder的应用 Feature Disentanglement Discrete Latent Representation Generator Compression Anomaly Detection Auto-encoder的基本思想# Auto-encoder的训练# Auto-encoder的是以无监督学习训练的，训练时会把一个Encoder和一个Decoder拼接在一起，将Encoder产生的向量输入Decoder，目标是让Decoder尽可能去复原输入。如果要使得Decoder能复原Encoder的输入，就要求Encoder能有效地将输入编码为向量，所以训练好以后，Encoder就有强大的编码功能，就可以拿去做一些下游任务（这和BERT是类似的）。 其实Auto-encoder和Cycle-GAN的思想也是有点类似的（其实是先有Auto-encoder），Cycle-GAN是希望一个Generator产生的图片经过另一个Generator后能够复原，训练成功后得到两个Generator。 为什么Auto-encoder是有效的# 以图像的任务为例，把图片看成高维度的向量，Auto-encoder的Encoder就是将一个高维度的向量降维成低维度的向量。 由于维度降低，必然有信息丢失。Decoder能复原成功，主要是因为图片虽然是高维的向量，但是其变化是有限的，比如下图中也许3x3的图片可能实际上只有4种变化是合乎情理的，所以可以把9维向量降维为2维向量，实际任务中的图片也会符合某种分布，这会限制图片的变化种类，所以用低维度向量来表示图片依然保留了图片的特性，所以Decoder才能复原图片。 那么为什么Auto-encoder中的Encoder在下游任务中是有效的呢？因为训练好的Encoder能有效地将数据降维，从而把复杂的数据用简单的方式表示，所以在下游任务中只用需要一点带标注的资料对模型进行训练就可以得到一个很好的结果。这个其实和BERT很像，BERT也是讲高维的语言信息降维，然后输入给输出层，在微调的时候，由于BERT具有很好的提取特征的能力，只需要很少的训练资料就可以让模型学会下游任务。 Auto-encoder和BERT的关系# 从训练过程来看，在实际训练Auto-encoder的时候，我们其实会把数据（比如图片）加上一点噪声，然后希望Decoder输出的数据能和加上噪声前的数据接近。那么其实Auto-encoder的训练过程就很像BERT了，因为预训练BERT时也是会给数据加上噪声，比如给句子挖空让BERT学做“填空题”。 从作用来看BERT和Encoder的作用类似，都是完成特征的提取；线性输出层和Decoder作用类似，都是对特征进行处理。 Auto-encoder的应用# Auto-encoder在训练好以后，除了可以将Encoder拿出来迁移到下游任务上，Auto-encoder还有很多其他应用。 Feature Disentanglement# Feature Disentanglement，字面翻译就是特征解耦，就是将Encoder输出的向量包含的纠缠的特征（我们不知道哪些维度代表哪些特征）进行分离。举例来说，Auto-encoder可以做到输入一段声音，让Encoder输出的向量前一半代表说话内容，后一半代表说话的人。这样就有可能实现语音合成之类的应用，比如输入李宏毅老师说的\"How are you?“和新桓结衣说的\"Hello”，输出新垣结衣说的\"How are you?\"。 Discrete Latent Representation# 一般来说，Encoder输出的向量中的每一个数都是一般的实数，但是有时我们希望输出的向量中每一维都只有0和1，这样可能可以更好地解释Encoder输出，比如输入一张人物头像图片，第一维的1代表有戴眼镜，0代表没有戴眼镜。有时我们甚至可以让输出的向量是一个one hot向量，这样可能可以实现无监督学习训练模型做分类任务，比如手写数字识别，输出十维向量，观察向量中哪一维为1，就可以得出分类结果。 在Discrete Representation中，最知名的是VQVAE，Encoder输出一个实数向量，和codebook中的各个向量（也是通过学习得到的）计算相似度（有点像Self-attention，Encoder输出向量是query，codebook中向量是key），将相似度最大的向量拿出来，作为输入Decoder的向量。这可以使得Embedding的种类是有限的，这样可能可以做语音辨识，因为可能可以把Encoder的连续输出对应到离散的音标。 除此之外，Auto-encoder的思想在文本上也有很好的应用。我们希望Encoder（此时Encoder和Decoder都是seq2seq模型）可以将输入的文章进行特征提取，从而产生文章摘要，但是如果只是用Auto-Encoder的架构，会发现Encoder和Decoder之间会达成某种“契约”，就是Encoder生成的文字只有Decoder能看懂，而无法被人理解。所以为了产生让人能理解的摘要，需要让产生的摘要经过一个Discriminator，用来判定文章是不是人写的，那么其实仔细一想，这不是Cycle-GAN吗？（^_^） 所以Auto-encoder也是一种理解Cycle-GAN的方式？ Generator# 我们也可以将Decoder单独拿出来，作为一个Generator进行使用，可以实现从某个分布中随机抽取一个向量，然后产生图片，这其实就是Variational auto-encoder（VAE）的想法。 Compression# 利用训练好的Auto-encoder，我们可以进行图片的压缩。因为图片经过Encoder时会丢失一部分信息，所以经过Decoder后复原的图像相比原始图片会有一定失真，这是一种有损压缩的方式。 Anomaly Detection# Auto-encoder也可以做异常样本检测，我们把训练的数据的分布看成一个domain，训练成功的Auto-encoder是可以让这个domain上的数据在经过Encoder编码、Decoder解码后复原的。但是如果今天输入的数据不在这个domain内，比如训练的时候输入真人头像，输入二次元人物头像时就很有可能无法复原。应用Anomaly Detection，可以实现诈骗信息、异常交易记录的检测等具体的应用。 异常检测的问题很可能不能用训练分类器的方法去做，因为异常样本往往是很少的，会产生极端的样本不平衡。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/"},{"title":"李宏毅机器学习-lect9 Explainable ML","text":"为什么我们需要可解释的机器学习 可解释机器学习的目标 Local Explaination 理解对模型决策重要的部分 如何理解 限制 为什么要理解对模型输出重要的部分 理解模型对于输入的处理 将输出进行降维 观察模型参数 Probing Global Explaination 基于先验知识的限制 基于生成的限制 LIME 为什么我们需要可解释的机器学习# 在很多场合，仅仅让AI给出答案是不够的，我们还需要让AI告诉我们为什么它认为这是答案，比如： AI执法时除了给出判决，还需要给出它的依据 自动驾驶时汽车突然刹车，事后可能要分析做出突然刹车的决定的理由 AI开药时除了给出处方，还需要给出开这种处方的理由 可解释机器学习的目标# 关于这个话题有很多的观点，李宏毅老师的观点是我们并不是因为神经网络是黑箱而不敢用它的决策，因为我们的大脑其实也可以看成黑箱，但我们还是遵从大脑的决策。其实我们只是需要一个用这个决策的理由罢了。所以可解释机器学习的目标，也许只是让人有一个相信AI的理由而已。 Harvard做过一个心理学实验，这个实验是以不同方式询问排队使用打印机的同学是否可以插队并收集反应。 “对不起，我只有5面需要打印，你能让我先用打印机吗？”——60%的人同意 “对不起，我只有5面需要打印，你能让我先用打印机吗？因为我赶时间。”——94%的人同意 “对不起，我只有5面需要打印，你能让我先用打印机吗？因为我需要打印。”——93%的人同意 所以哪怕第三种询问方式的理由非常无厘头，也能大大提高同意插队的概率，所以也许决策时更多只是需要一个理由。这个实验一定程度上能佐证李宏毅老师的观点。 Local Explaination# 让我们了解一只猫在模型“眼里”是什么样的 理解对模型决策重要的部分# 如何理解# 比如今天我们输入一只猫，我们希望知道哪些部分对于模型判断它是只猫是重要的，那么我们可以将它的一部分遮挡，然后观察被遮挡后的图片被模型判断出是猫的概率，如果某个部分被遮挡后的图片被认为是猫的概率较低，就可以认为这部分对模型的判断而言是重要的。 比如下面这张图中，用灰色块去遮挡第一排的图片的不同位置，产生第二排的结果。第二排中蓝色部分代表这部分被遮挡后判断正确概率较低。 如果想要更精确地了解哪部分对于模型是重要的，可以在每一个像素上加上一个很小的值，然后观察模型输出概率的变化，然后通过变化率ΔeΔx\\frac{\\Delta e}{\\Delta x}ΔxΔe​ 来判断某个像素对于模型判断的重要性。那么其实这个方法就是把图片看成矩阵XXX，构造e=f(X)e=f(X)e=f(X)，然后进行矩阵求导，再代入图片XXX的值得到导数，也就是下图中第二排图片，即Saliency Map。 限制# 但是实际上，产生的Saliency Map大部分都是含有比较多的噪声的，所以https://arxiv.org/abs/1706.03825中提出可以将原图加上不同的随机噪声，然后得到多张Saliency Map后再取平均，可以起到平滑的作用。 除此之外，可能还会遇到Gradient Saturation的问题，比如判断鼻子长度和是大象的概率的相关性时，当鼻子比较短时，是大象的概率会随鼻子长度的增长而增长，但是鼻子长度比较长的时候，可能被判断为是大象的概率就不会增长了。这个例子是说明在生成Saliency Map中，可能某些重要要素会由于Gradient Saturation的问题导致没有被检测出来。 对于这个问题，可以通过https://arxiv.org/abs/1611.02639提到的Integrated gradient来解决。 下图是照相机图片的Integrated gradient图和gradient图的对比，明显可以发现Integrate gradient可以更好的解释模型为什么做出物体是照相机的判断。 为什么要理解对模型输出重要的部分# 因为有时候模型并不一定是看到猫眼、猫耳等特征才知道是猫的，比如下图中左下角有一串文字，模型认为这串文字才是分类的重点，这与我们的目标是不符合的。通过观察哪些部分对模型是重要的，我们可以了解模型是不是学会了我们真正想让模型学习的任务。 理解模型对于输入的处理# 将输出进行降维# Hinton在12年时就尝试过把语音识别系统的输出进行降维，然后发现不同语者（用不同颜色的点表示）说的同样的话的分布是接近的，所以可以认为语音识别系统真正学会了无视说话的人，而将注意力集中在说话的内容上。 观察模型参数# 比如在Self-attention中，我们可以观察attention score来了解模型对于某个输入的哪些部分比较在意，以及这个Self-attention层对这个输入做了什么处理。 Probing# 我们可以将模型中某一层的输出进行处理，来观察这一层之前的模型对输入做了什么处理。比如可以把BERT中某一层的输出输入一个分类器里面，进行词性的分类。但是要求分类器的性能比较好。 或者在语音识别系统中，我们把前几层的输出作为TTS（text to speech）的输入，假设发现TTS的输出的语音中说话的人已经无法被分辨，那么模型的前几层的作用可能就是把输入的声音中说话的人的特征模糊了。 Global Explaination# 让我们了解模型“心中”的猫是什么样的 在CNN中，可以把卷积层的输出理解为特征图，特征图的值越大，说明图像中特征越明显，那么我们可以固定卷积层的参数，用gradient ascent（和gradient decent是类似的）的方法去找出一张让卷积层输出的特征图的所有像素的值的累加最大的图片X∗X^{*}X∗。观察X∗X^{*}X∗我们可以知道各个卷积层对什么特征是感兴趣的。 比如一个做手写数字识别的12层CNN中我们对每个卷积层求它的X∗X^*X∗，得到左侧的图片，那么发现有些层侧重检测竖线，有些层侧重检测横线。 基于先验知识的限制# 通过之前提到的gradient ascent找出X∗X^{*}X∗的方法，我们知道了每个卷积层侧重处理什么特征。更进一步，以手写数字辨识模型为例，我们可以固定整个模型的参数，通过gradient ascent来找出使得输出为某个数字概率最大的图片，这样或许我们就可以看到模型“心中”的数字长什么样。 但是实际上，直接这么操作产生的图片并不会有很好的结果，而是产生下方图片左侧的马赛克状的图片。但是我们如果从adversarial attack（21年貌似先讲adversarial attack，22年才是先讲explainable ML）的角度来想，一个人眼不可见的杂讯可以使得模型判别的结果直接改变，那么就能比较好地理解为什么直接用gradient ascent会产生马赛克状的图片。 所以我们需要给优化函数加上一个R(X)R(X)R(X)项，这一项代表输出的图片有多像人眼看到的数字，构造这个函数可能需要一些先验知识，比如我们认为看到的数字中白色的像素点不会很多，所以图片总灰度值可能不会特别大，那么我们可以使R(X)=−∑i,j∣Xij∣R(X)=-\\sum_{i,j}|X_{ij}|R(X)=−∑i,j​∣Xij​∣，从而得到了人眼看起来有点像数字的模型“心中”的数字。 下图是https://arxiv.org/abs/1506.06579中得到的部分模型“心中”的各种物体的图像。 基于生成的限制# 在优化函数中加入限制项或许可以起到效果，但是需要一定的先验知识，还需要复杂的调参过程。另一种比较明朗的方法是先训练一个把向量变成图片的生成器（GAN、VAE之类），然后把生成器和分类器接在一起，固定整个模型的参数，用gradient ascent的方法找出使得输出为某个类别的概率最大的向量zzz，然后模型“心中”的图片就是向量zzz通过生成器GGG的输出G(z)G(z)G(z)。 下图是https://arxiv.org/abs/1612.00005中得到的一些模型“心中”的火山的图像 LIME# 线性的模型的可解释性是很强的，因为我们可以了解到每一个元素的权重，但是它在复杂任务上表现不佳。所以我们也许可以用线性模型来模拟复杂模型中的一小部分，来解释这一小部分的作用。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/"},{"title":"李宏毅机器学习-lect11 Domain Adaptation","text":"为什么需要Domain Adaptation 不同情境下的Domain Adaptation 新的Domain上有一点带标注的资料 新的Domain上有很多不带标注的资料 新的Domain上只有一点不带标注的资料 我们对新的Domain一无所知 为什么需要Domain Adaptation# 在之前的课程的作业中，训练资料和测试资料的分布都是类似的，但是在很多实际任务中，会出现训练资料和测试资料的分布不一样的情况（就像平时的练习题和高考题目出题风格存在差异），称为Domain shift，那么此时模型的表现可能就会下降很多。比如我们在Mnist上训练手写数字分类模型后，如果测试资料是彩色数字，模型的正确率可能会从超过99%下降到不到60%。但是我们很多时候是没有大量带标注的、和测试资料分布相同的训练资料的，所以我们需要Domain Adaptation。 常见的Domain shift有三种类型 Source Domain和Target Domain分布不同 Target Domain中各个类型数据量和Source Domain不同，可能某个类型特别多 Source Domain和Target Domain分布相似，但是两个Domain中相似的数据的标注不同 不同情境下的Domain Adaptation# 做Domain Adaptation时，我们对新的Domain可能有不同的“了解”程度，如下图所示，对新的Domain的了解程度从高到低为 新的Domain上有一点带标注的资料 新的Domain上有很多不带标注的资料 新的Domain上只有一点不带标注的资料 我们对新的Domain一无所知 新的Domain上有一点带标注的资料# 这种情况比较简单，只需要用Target Domain上的资料微调模型（一般训练几轮就可以了），就可以达到比较好的效果，但是要注意防止过拟合。 新的Domain上有很多不带标注的资料# 很多时候我们只知道新的Domain会有怎样的数据，而不知道这些数据应该有什么样的输出。对于这类问题，基本想法是训练一个特征提取器，希望Source Domain的数据和Target Domain上的数据在经过特征提取器后的分布是类似的。这样相当于是无视了Target Domain和Source Domain中的差异，只提取两个Domain上数据的共有特征。 为了训练特征提取器，我们还需要一个Domain Classifier来判断特征提取器的输出属于哪个Domain，其实这里就有点像GAN了。 但是，只让Source Domain的数据和Target Domain上的数据在经过特征提取器后的分布类似很可能会让特征提取器学会无论什么样的图片都输出相同的向量，所以我们在训练时还应该让提取出的特征经过输出层，在特征提取器的loss函数中加上输出标签和真实标签之间的loss，确保提取出的特征是有效的。 然后我们除了让Target Domain上的数据和Source Domain上的数据分布类似，我们也许还需要让Target Domain上的数据远离Source Domain中不同label的数据之间的边界。这个可以通过在训练特征提取器的时候尽可能使得提取出的特征经过label predictor后的分布比较集中（就是比较鲜明地属于某个类别），此时认为这个数据是远离Source Domain上的各类数据之间的边界的。 在之前的讨论中，我们好像都默认了Source Domain和Target Domain的label的类型都是一样的，就像下图左上所示。但是实际上，Target Domain中的类别可能是Source Domain的子集（下图右上）；也可能是Target Domain和Source Domain有交集，但都有各自独特的label（下图左下）；也可能Source Domain是Target Domain的子集（下图右下）。那么如果此时用之前的方法做Domain Adaptation，可能会因为强行让两个Domain重合而导致错误，比如在下图右上的情况中把Target Domain中的狗认成老虎。 Universal Domain Adaptation (thecvf.com)中提出了Universal Domain Adapation的方法，简单来说就是需要判断Target Domain上的数据是不是属于Target Domain和Source Domain的共有类别内，如果不在共有类别内，则输出unknown。 新的Domain上只有一点不带标注的资料# 有时我们面对Target Domain时，我们甚至没有大量不带标注的资料，那此时怎么做Domain Adaptation呢？ https://arxiv.org/abs/1909.13231中提到，可以通过Test time training的方法实现Domain Adaptation，这个方法的大致思路在Test-Time Training - 知乎 (zhihu.com)内讲得挺清楚的。 我们对新的Domain一无所知# 有时我们甚至不知道Target Domain会有怎样的数据，就像高考前不知道今年会出什么怪题。 此时我们可以通过Domain Generalization的方法，在训练时就给予不同的Domain的资料，从而让模型学会无视Domain之间的差距，实现当新的Domain中的数据输入时，模型也能有很好的效果。 但是有时我们的训练资料只有一个Domain，而测试资料却有不同的Domain。https://arxiv.org/abs/2003.13216中提出我们可以尝试用数据增强产生多个Domain的训练资料，然后进行训练，从而实现Domain Generalization。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/"},{"title":"李宏毅机器学习-lect10 Adversarial Attack","text":"攻击的例子 如何实现攻击 White Box Attack Black Box Attack One pixel attack Universal Adversarial Attack 视觉领域外的攻击 语音、文字上的攻击 物理世界中的攻击 Adversarial Reprogramming “Backdoor” in Model 如何对抗攻击 被动防御 主动防御 攻击的例子# 如何实现攻击# White Box Attack# Black Box Attack# One pixel attack# Universal Adversarial Attack# 视觉领域外的攻击# 语音、文字上的攻击# 物理世界中的攻击# Adversarial Reprogramming# “Backdoor” in Model# 如何对抗攻击# 被动防御# 主动防御#","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/"}],"tags":[{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"},{"name":"Learning rate优化","slug":"Learning-rate优化","link":"/tags/Learning-rate%E4%BC%98%E5%8C%96/"},{"name":"Test time augmentation","slug":"Test-time-augmentation","link":"/tags/Test-time-augmentation/"},{"name":"Self attention","slug":"Self-attention","link":"/tags/Self-attention/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","link":"/tags/Self-Supervised-Learning/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"},{"name":"Auto-encoder","slug":"Auto-encoder","link":"/tags/Auto-encoder/"},{"name":"Explainable ML","slug":"Explainable-ML","link":"/tags/Explainable-ML/"},{"name":"Domain Adaptation","slug":"Domain-Adaptation","link":"/tags/Domain-Adaptation/"},{"name":"Adversarial Attack","slug":"Adversarial-Attack","link":"/tags/Adversarial-Attack/"}],"categories":[{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"李宏毅机器学习","slug":"Deep-Learning/李宏毅机器学习","link":"/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"手搓记录","slug":"Deep-Learning/手搓记录","link":"/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/"}]}
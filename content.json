{"pages":[{"title":"关于我","text":"浙江大学光电科学与工程学院2020级本科生 正准备入计算成像+AI的坑 联系方式见网页左下","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"李宏毅机器学习-为什么训练会失败","text":"Training loss很高 这时发生了什么 训练方法 Batch Batch对训练效率影响 Batch对训练效果影响 Momentum Learning rate Loss function Training data的loss很小 过拟合 降低模型复杂度 添加更多数据 Mismatch Training loss很高 这时发生了什么 当微分接近0时，训练会停止。但是此时不一定是在极值点，也可能是在鞍点（saddle point）。 其实训练停止时，微分也不一定很接近0，也就不一定是在saddle point，这个后面会提到 通过求loss function的Hessian矩阵，判断Hessian矩阵正定性可以判断当前是否处在saddle point——H不定时则处在saddle point。 对于神经网络的loss而言，Hessian矩阵是一个维度非常高的矩阵，所以Hessian矩阵正定或负定的可能性很低，所以大部分训练停止的情况其实都是遇到了saddle point。 训练方法 Batch 每一个epoch开始时，会随机分割batch，训练时每次依次用各个batch计算出梯度后更新参数。 Batch对训练效率影响 一个很直观的想法是batch越大，训练时在一个batch上花的时间越久，但是由于GPU有并行运算的能力，其实batch在不是特别大的时候训练时间和batch size=1时相比几乎没有增长。所以适当的增大batch可以提高训练效率。 Batch对训练效果影响 从下图可以看出，小batch的训练准确率比较高。 为什么小的batch训练准确率高呢？原因在于使用小的batch时，每个batch之间的loss function可能存在细微的差别，当一个batch训练时卡在saddle point，对另一个batch可能不是saddle point，可以继续训练。 Momentum 训练时可以通过把上一步的移动和当前计算出的梯度结合，形成新的优化参数的方向，这有利于跳出saddle point，甚至可以帮助“翻越”比较小的山坡，使得loss function得到有效降低。 Learning rate 其实如果learning rate比较大，那么可能会出现还没到saddle point就停下来的可能（此时梯度的模比0还是会大比较多的），如下图所示，loss function的值在两条绿线所指的点反复横跳。 所以我们需要动态调整learning rate，使得尽量不会出现还没到saddle point就停下来的情况 为了动态调整learning rate，我们可以采取两点 对每个参数动态调整learning rate θit+1=θit−ηgit→θit+1=θit−ησitgit\\theta_i^{t+1}=\\theta_i^t-\\eta g_i^t\\rightarrow\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}g_i^tθit+1​=θit​−ηgit​→θit+1​=θit​−σit​η​git​ t代表随着训练轮次而改变，i代表对不同的参数 常见的求σit\\sigma_i^tσit​的方法有Root mean square（σti=1t+1∑0t(git)2\\sigma_t^i=\\sqrt{\\frac{1}{t+1}\\sum_0^t (g_i^t)^2}σti​=t+11​∑0t​(git​)2​ ）和RMSProp（σit=α(σit−1)2+(1−α)(git)2\\sigma_{i}^{t}=\\sqrt{\\alpha\\left(\\sigma_{i}^{t-1}\\right)^{2}+(1-\\alpha)\\left(g_{i}^{t}\\right)^{2}}σit​=α(σit−1​)2+(1−α)(git​)2​ ，比较近的轮次的梯度影响比较大） 对所有参数进行learning decay 上式中η→ηt\\eta\\rightarrow\\eta_tη→ηt​ 我们在训练时，经常使用Adam，这个优化方法结合了动态调整RMSProp方法调整learning rate和momentum。一般来说比较有效。 Loss function 选用不同的loss function，会有不同的error surface，所以选择合适的loss function，可以得到更容易进行优化的error surface。 Training data的loss很小 若testing data的loss比training data的loss大很多，也不一定是过拟合~ 可能的原因 过拟合 mismatch 过拟合 过拟合的解决方法有两种： 降低模型复杂度 添加更多数据 降低模型复杂度 e.g 减少神经网络层数 添加更多数据 搜集更多数据 数据增强（Data augmentation） 左右翻转图片 放大图片局部 …… 但是数据增强要用合理的方式，比如图片识别不能出现使用中基本不出现的形式，如图片上下颠倒 Mismatch mismatch指的是testing data的结构与training data的结构不一致","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/"},{"title":"李宏毅机器学习-lect3-CNN","text":"CNN 的想法 对于图片分类的任务，我们可以采用让特征识别（这与人类分类物体的方法是类似的）的方法，让每一个神经元只与部分区域（Receptive field）关联，而不需要每一层都full connect。 所以，CNN是为了影像的特性而生的，把CNN用于影像领域外的任务要仔细思考是否出现影像类似的特性。 卷积层（Convolutional layers） 卷积核 卷积核大小表示局部特征区域（即Receptive field）的大小 卷积核一般大小取3x3，同时包含RGB三个通道 Q：卷积核大小只有3x3，如果图片尺寸比较大，3x3会不会无法识别特征？ 这个问题将在下面回答 步长 （Stride） 每次卷积核移动的长度，一般设为1或2，因为我们希望Receptive field之间是有重叠的，因为如果Receptive field之间完全没有重叠，那么如果pattern出现在两个Receptive field的交界上，特征就难以被识别。 填充 （Padding） 移动卷积核时，如果步长大于1，则移动时卷积核可能会超出图片范围，则需要在边上填充一些值，常见的方法有补0法、取平均法等。 参数共享（Parameter Sharing） 对于一个特征，可能出现在图片不同的位置，而对于一个区域，有一组神经元负责，每个神经元负责识别不同的特征，所以此时可以让负责不同区域，但功能相同的神经元享有相同的参数，从而减少参数数量。 对于每一个区域，有一组神经元负责，每个神经元有一组参数，这一组组参数叫做filter，所有区域共享一组filter 卷积层的运行过程 之前提到，图片中每一个小区域有一组神经元负责，每一个神经元的参数叫做filter，所有小区域共享一组filter，那么卷积层可以看作每一个filter对图像分别作用，得到一组图像，所有的filter对图像作用后，得到了新的图像，图像的channel数则为filter的数量。这样的一张图片叫做特征图像。 所以一张图像经过卷积层后，会得到一张特征图像。之前有提到，卷积核大小只有3x3时会不会无法识别较大特征，这是不会的，因为在下一个卷积层中对特征图像做卷积时，若卷积核为3x3，步长为1时，则相当于对5x5大小区域卷积。 卷积层的两种理解方式 池化层（Pooling layers） 对于一张较大的图片而言，采样时少采样一些点并不会影响图像是什么 池化层并没有参数，其操作时固定的，相当于一个算符 常用的池化方法有Max Pooling，过程如下图所示，一般而言，池化时分组大小为2x2 一般而言，池化常常在卷积层后使用，如一个或两个卷积层后跟一个池化层，用于缩小图片，从而减小运算量。但是这对于网络的效果而言可能是由损害的，因为如果特征特别细小，则池化可能会漏过特征。 Flatten layers 图像经过一系列卷积、池化后，得到小的特征图像，此时这个特征图像代表图片中较大的、全局的特征，此时就可以把图像展平，然后通过全连接层，经过softmax归一化后，得出分类的结果。 其实到这里，和回归问题里用到的神经网络是类似的，只不过回归问题的特征是显式的，所以一开始就可能是全连接层，而影像类任务中，一开始需要先提取特征，最后再让特征经过全连接层计算。 CNN 的缺点 CNN很难处理图片的缩放、旋转，所以我们需要数据增强（data augmentation）","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/"},{"title":"hexo配置问题","text":"记录hexo配置中遇到的各种问题","link":"/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"}],"categories":[{"name":"李宏毅机器学习","slug":"李宏毅机器学习","link":"/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"}]}
{"pages":[{"title":"关于我","text":"浙江大学光电科学与工程学院2020级本科生 正准备入计算成像+AI的坑 联系方式见网页左下","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"李宏毅机器学习-为什么训练会失败","text":"Training loss很高 这时发生了什么 训练方法 Batch Batch对训练效率影响 Batch对训练效果影响 Momentum Learning rate Loss function Training data的loss很小 过拟合 调整模型复杂度 添加更多数据 Mismatch Training loss很高 这时发生了什么 当微分接近0时，训练会停止。但是此时不一定是在极值点，也可能是在鞍点（saddle point）。 其实训练停止时，微分也不一定很接近0，也就不一定是在saddle point，这个后面会提到 通过求loss function的Hessian矩阵，判断Hessian矩阵正定性可以判断当前是否处在saddle point——H不定时则处在saddle point。 对于神经网络的loss而言，Hessian矩阵是一个维度非常高的矩阵，所以Hessian矩阵正定或负定的可能性很低，所以大部分训练停止的情况其实都是遇到了saddle point。 训练方法 Batch 每一个epoch开始时，会随机分割batch，训练时每次依次用各个batch计算出梯度后更新参数。 Batch对训练效率影响 一个很直观的想法是batch越大，训练时在一个batch上花的时间越久，但是由于GPU有并行运算的能力，其实batch在不是特别大的时候训练时间和batch size=1时相比几乎没有增长。所以适当的增大batch可以提高训练效率。 Batch对训练效果影响 从下图可以看出，小batch的训练准确率比较高。 为什么小的batch训练准确率高呢？原因在于使用小的batch时，每个batch之间的loss function可能存在细微的差别，当一个batch训练时卡在saddle point，对另一个batch可能不是saddle point，可以继续训练。 Momentum 训练时可以通过把上一步的移动和当前计算出的梯度结合，形成新的优化参数的方向，这有利于跳出saddle point，甚至可以帮助“翻越”比较小的山坡，使得loss function得到有效降低。 Learning rate 其实如果learning rate比较大，那么可能会出现还没到saddle point就停下来的可能（此时梯度的模比0还是会大比较多的），如下图所示，loss function的值在两条绿线所指的点反复横跳。 所以我们需要动态调整learning rate，使得尽量不会出现还没到saddle point就停下来的情况 为了动态调整learning rate，我们可以采取两点 对每个参数动态调整learning rate θit+1=θit−ηgit→θit+1=θit−ησitgit\\theta_i^{t+1}=\\theta_i^t-\\eta g_i^t\\rightarrow\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}g_i^tθit+1​=θit​−ηgit​→θit+1​=θit​−σit​η​git​ t代表随着训练轮次而改变，i代表对不同的参数 常见的求σit\\sigma_i^tσit​的方法有Root mean square（σti=1t+1∑0t(git)2\\sigma_t^i=\\sqrt{\\frac{1}{t+1}\\sum_0^t (g_i^t)^2}σti​=t+11​∑0t​(git​)2​ ）和RMSProp（σit=α(σit−1)2+(1−α)(git)2\\sigma_{i}^{t}=\\sqrt{\\alpha\\left(\\sigma_{i}^{t-1}\\right)^{2}+(1-\\alpha)\\left(g_{i}^{t}\\right)^{2}}σit​=α(σit−1​)2+(1−α)(git​)2​ ，比较近的轮次的梯度影响比较大） 对所有参数进行learning decay 上式中η→ηt\\eta\\rightarrow\\eta_tη→ηt​ 我们在训练时，经常使用Adam，这个优化方法结合了动态调整RMSProp方法调整learning rate和momentum。一般来说比较有效。 Loss function 选用不同的loss function，会有不同的error surface，所以选择合适的loss function，可以得到更容易进行优化的error surface。 Training data的loss很小 若testing data的loss比training data的loss大很多，也不一定是过拟合~ 可能的原因 过拟合 mismatch 过拟合 过拟合的解决方法有两种： 调整模型复杂度 添加更多数据 调整模型复杂度 训练数据较多：增加复杂度 训练数据较少：降低复杂度 添加更多数据 搜集更多数据 数据增强（Data augmentation） 左右翻转图片 放大图片局部 …… 但是数据增强要用合理的方式，比如图片识别不能出现使用中基本不出现的形式，如图片上下颠倒 Mismatch mismatch指的是testing data的结构与training data的结构不一致","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/"},{"title":"李宏毅机器学习-lect3-CNN","text":"CNN 的想法 对于图片分类的任务，我们可以采用让特征识别（这与人类分类物体的方法是类似的）的方法，让每一个神经元只与部分区域（Receptive field）关联，而不需要每一层都full connect。 所以，CNN是为了影像的特性而生的，把CNN用于影像领域外的任务要仔细思考是否出现影像类似的特性。 卷积层（Convolutional layers） 卷积核 卷积核大小表示局部特征区域（即Receptive field）的大小 卷积核一般大小取3x3，同时包含RGB三个通道 Q：卷积核大小只有3x3，如果图片尺寸比较大，3x3会不会无法识别特征？ 这个问题将在下面回答 步长 （Stride） 每次卷积核移动的长度，一般设为1或2，因为我们希望Receptive field之间是有重叠的，因为如果Receptive field之间完全没有重叠，那么如果pattern出现在两个Receptive field的交界上，特征就难以被识别。 填充 （Padding） 移动卷积核时，如果步长大于1，则移动时卷积核可能会超出图片范围，则需要在边上填充一些值，常见的方法有补0法、取平均法等。 参数共享（Parameter Sharing） 对于一个特征，可能出现在图片不同的位置，而对于一个区域，有一组神经元负责，每个神经元负责识别不同的特征，所以此时可以让负责不同区域，但功能相同的神经元享有相同的参数，从而减少参数数量。 对于每一个区域，有一组神经元负责，每个神经元有一组参数，这一组组参数叫做filter，所有区域共享一组filter 卷积层的运行过程 之前提到，图片中每一个小区域有一组神经元负责，每一个神经元的参数叫做filter，所有小区域共享一组filter，那么卷积层可以看作每一个filter对图像分别作用，得到一组图像，所有的filter对图像作用后，得到了新的图像，图像的channel数则为filter的数量。这样的一张图片叫做特征图像。 所以一张图像经过卷积层后，会得到一张特征图像。之前有提到，卷积核大小只有3x3时会不会无法识别较大特征，这是不会的，因为在下一个卷积层中对特征图像做卷积时，若卷积核为3x3，步长为1时，则相当于对5x5大小区域卷积。 卷积层的两种理解方式 池化层（Pooling layers） 对于一张较大的图片而言，采样时少采样一些点并不会影响图像是什么 池化层并没有参数，其操作时固定的，相当于一个算符 常用的池化方法有Max Pooling，过程如下图所示，一般而言，池化时分组大小为2x2 一般而言，池化常常在卷积层后使用，如一个或两个卷积层后跟一个池化层，用于缩小图片，从而减小运算量。但是这对于网络的效果而言可能是由损害的，因为如果特征特别细小，则池化可能会漏过特征。 Flatten layers 图像经过一系列卷积、池化后，得到小的特征图像，此时这个特征图像代表图片中较大的、全局的特征，此时就可以把图像展平，然后通过全连接层，经过softmax归一化后，得出分类的结果。 其实到这里，和回归问题里用到的神经网络是类似的，只不过回归问题的特征是显式的，所以一开始就可能是全连接层，而影像类任务中，一开始需要先提取特征，最后再让特征经过全连接层计算。 CNN 的缺点 CNN很难处理图片的缩放、旋转，所以我们需要数据增强（data augmentation）","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/"},{"title":"hexo配置问题","text":"记录hexo配置中遇到的各种问题，随缘更新中…… OS：Windows 10 主页类型：gitpage 官方文档：文档 | Hexo 第一个网页 部署到github 配置主题 安装 配置 正文部分 写入正文 编辑文章的分类、标签 插入图片 插入公式 插入TOC目录 插入代码 About页 Tag页 第一个网页 此部分根据官方文档操作即可完成，故不赘述 安装：文档 | Hexo 建站：建站 | Hexo 部署到github 此部分根据官方文档操作即可完成，故不赘述 在 GitHub Pages 上部署 Hexo | Hexo 一键部署 | Hexo 配置主题 这里面有非常多主题，随便选一个自己喜欢的用就好了。 我选择的主题是Anatolo，这个主题的作者锦心是一位可爱的OIer妹妹（她还不到大一就能自己写theme，而我配置她的主题都磕磕绊绊quq），她的主页：Lhc_fl Home (lhcfl.github.io) 安装 参考Anatolo (lhcfl.github.io) 配置 复制_config.example.yml为_config.yml，修改hexo根目录下的 _config.yml ： theme: Anatolo 上面是锦心写的配置方法，但是当时我花了很久时间才理解，具体来说应该是把themes/Anatolo下的_config.example.yml文件复制到themes/Anatolo（一开始我以为是复制到根目录……），然后修改hexo项目根目录下的 _config.yml ： theme: Anatolo。 正文部分 写入正文 新建文章 ：用hexo new &lt;title&gt; ，可以在_post目录下新建&lt;title&gt;.md文件，之后便可以按照markdown语法开始写作。 编辑文章的分类、标签 参照如下写法即可 1234567title: hexo配置问题date: 2023-01-12 02:45:58tags: - 环境配置 - hexocategories: - 环境配置 插入图片 这篇文章十分详细地讲了如何方便地结合Typora插入图片：hexo博客如何插入图片 - 知乎 (zhihu.com) 插入公式 找不到参考的文章了quq 告诫我配置完环境一定要马上整理 警钟长鸣 插入TOC目录 直接用Typora中的TOC是不能插入目录的！ 正确插入方法如下： 安装hexo-toc插件：npm install hexo-toc --save 参照文档内的方法编辑_config.yml 在想插入目录的地方写入 1&lt;!-- toc --&gt; 插入代码 无需插件，正常写入即可。 TODO：学习利用插件对代码块进行优化的方法。 About页 用hexo new page &quot;about&quot;新建“关于”页面，然后正常写入内容即可 Tag页 用hexo new page &quot;tag&quot;新建标签统计页面即可，无需写入任何内容","link":"/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"},{"title":"李宏毅机器学习-lect4-Self attention","text":"Self attention 解决什么问题 FC(Fully Connected)有什么不足 Self attention 的架构 Self attention 的基本计算过程 求attention score 求解Self attention层输出的向量 矩阵视角下的计算过程 Self attention的优化 Multi-head Self attention Positional Encoding Self attention 和其他网络对比 CNN RNN GNN Self attention 解决什么问题 用一句话概括就是：Self attention用来解决当输入为向量的序列时的问题（像音频、文本都是经典的输入为向量序列的数据） Self attention的输出一般有如下三种类型 N个vector产生N个label ​ 例如输入一个句子，输出每个词的词性 N个vector产生1个label ​ 例如输入一个句子，判断这句话蕴含的情绪为positive or negative N个vector产生N‘（N≠N’）个label ​ 例如机器翻译，输入的句子和输出的句子词数很可能不一样 FC(Fully Connected)有什么不足 FC也可以用来解决输出为向量序列的问题，如天气预测等等，但是它相比self attention有一些不足之处。 FC如果要充分考虑“上下文”——一个向量和它相邻的很多个向量，甚至可能整个序列一起考虑，就需要把考虑的向量串联起来，通过fully connect产生新的向量，那么参数的矩阵可能会非常大（这个在后面会解释），这可能导致很大的运算量和overfitting。 Self attention 的架构 首先我们来看一个self attention层要做什么。 概况一下就是，一个self attention层要先算出每个向量和其他向量的关联性，这个关联性用attention score α\\alphaα 表示。然后再根据加权算出输出向量序列，值得一提的是，计算输出向量序列中的每个向量是并行的。 然后我们再看整个网络架构，经过self attention层后，产生了输出向量序列，这时候可以先对每一个向量进行fully connect，产生输入到下一个self attention层中的输入向量，然后就是重复，直至产生最终输出。 Self attention 的基本计算过程 求attention score 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求a1\\mathbf{a^1}a1与自身及其他向量的attention score α1,i′\\alpha'_{1,i}α1,i′​为例，来说明求解流程 对a1\\mathbf{a^1}a1求q1\\mathbf{q^1}q1，q1=Wq⋅a1\\mathbf{q^1}=\\mathbf{W}^q\\cdot\\mathbf{a^1}q1=Wq⋅a1，其中Wq\\mathbf{W}^qWq为参数矩阵，是要学习的参数 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…，ki=Wk⋅ai\\mathbf{k^i}=\\mathbf{W}^k\\cdot\\mathbf{a^i}ki=Wk⋅ai，其中Wk\\mathbf{W}^kWk为参数矩阵，是要学习的参数 求q1\\mathbf{q^1}q1和k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…的关联性，常见方法是向量点乘，如α1,2=(q1)T⋅k2\\alpha_{1,2}=(\\mathbf{q^1})^\\mathrm{T}\\cdot\\mathbf{k^2}α1,2​=(q1)T⋅k2 对上一步求得的α1,1,α1,2,…\\alpha_{1,1},\\alpha_{1,2},\\dotsα1,1​,α1,2​,…通过激活函数，常用softmax，最后得到attention score α1,1′,α1,2′,…\\alpha'_{1,1},\\alpha'_{1,2},\\dotsα1,1′​,α1,2′​,… 可以参照下图直观地理解上述步骤 求解Self attention层输出的向量 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求输出向量序列中的b1\\mathbf{b^1}b1为例，来说明求解流程 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求v1,v2,…\\mathbf{v^1},\\mathbf{v^2},\\dotsv1,v2,…，vi=Wv⋅ai\\mathbf{v^i}=\\mathbf{W}^v\\cdot\\mathbf{a^i}vi=Wv⋅ai，其中Wv\\mathbf{W}^vWv为参数矩阵，是要学习的参数 利用求得的attention score对vi\\mathbf{v^i}vi进行加权：b1=∑α1,i′v1\\mathbf{b^1}=\\sum \\alpha'_{1,i}\\mathbf{v^1}b1=∑α1,i′​v1，从而得到b1\\mathbf{b^1}b1 可以参照下图直观地理解上述步骤 矩阵视角下的计算过程 下面三张图非常直观地展示了 产生qi,ki,vi\\mathbf{q^i},\\mathbf{k^i},\\mathbf{v^i}qi,ki,vi 计算attention score 计算输出向量bi\\mathbf{b^i}bi 上面提到的矩阵视角下的计算过程可以归结为下图，我们也可以发现，对于一层self attention层而言，需要学习的参数只有Wq,Wk,Wv\\mathbf{W}^q,\\mathbf{W}^k,\\mathbf{W}^vWq,Wk,Wv 前面说到FC可能在考虑整个向量序列的情况下有大量的参数，比如说输入的向量序列为10000个100x1的向量，如果在考虑整个向量序列的情况下要产生10000个10x1的向量，用FC需要的参数数量级为(10000x100)x(10000x10)，而如果用self attention，那么Wq\\mathbf{W^q}Wq的元素个数数量级为100x100。 Self attention的优化 Multi-head Self attention 考虑到在不同的视角下，一个向量和其他向量会有不同的关联性，所以我们可以用不同系数矩阵，最终产生多组向量序列输出。 我们也可以对多组向量序列输出进行拼接、变换，得到一组输出向量序列 Positional Encoding 在之前的产生输出向量序列的操作中，我们可能忽略了输入向量序列在坐标或者时间尺度上的相关性，而这有时是很重要的，比如在音频识别中，识别一个音符考虑的就是在一小段时间内的信号，那么这一小段时间内向量的相关性就极强。 所以我们可以通过在输入向量序列上加一个用来代表位置的向量，来为输入向量增加坐标/时间信息。这个代表位置的向量可以是直接计算得到的，也可以是通过神经网络学习得到的。 Self attention 和其他网络对比 CNN 其实图像也可以看作是向量的序列——比如一张100x100x3的图片，可以看作是100x100个向量（一个像素的RGB三通道值作为向量）。 通过设置如果attention score合适，是不是也可以表现出CNN的效果？而且self attention可以打破CNN中由于卷积核带来的像素只与周边像素作用的限制，让一个像素可以和离它较远的像素作用。 所以，self attention是更灵活的CNN，而CNN是简化的self attention 在二者训练效果对比中也可以发现，对于小的数据集，自由度比较小的CNN表现得更好，对于大数据集而言，self attention表现的效果更好。 RNN 相比RNN而言，self attention由于并行运算可以获得更快的运算速度，同时相比RNN不易考虑到距离一个向量在坐标尺度下比较远的向量，self attention可以通过attention score充分考虑到一个向量在坐标尺度下距离比较远的向量。 GNN self attention可以看作是一种特殊的GNN，如果两个点直接由连线，那么attention可以设为1，否则直接设为0。","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/"},{"title":"李宏毅机器学习-lect5-Transformer","text":"Transformer是什么 Transformer的应用 Transformer的架构 Encoder Decoder Autoregressive Transformer Masked Self-attention Cross attention Non autoregressive Transformer(NAT) Transformer的训练 loss的来源 训练的一些tips Teacher Forcing Copy Mechanism Guided Attention Beam Search Exposure bias Transformer是什么 Transformer可以理解为是Sequence-to-sequence（简称Seq2seq）的模型，它接受向量序列作为输入，输出向量序列。 Transformer的应用 Transformer的应用除了上图提到的语音识别、机器翻译、语音翻译以外，还有 聊天机器人 句子词性分析 多标签分类 物体检测 Transformer的架构 Transformer由编码器（Encoder）和解码器（Decoder）组成，前向传播的过程是Encoder将输入向量序列编码产生新的向量序列，然后Decoder将编码的向量结合begin向量（标记着位置，是一个one hot向量）产生第一个输出向量，然后把产生的第一个向量再输入Decoder，产生第二个向量，直到产生end向量为止，代表着输出完成。 如下图所示，是Transformer的内部结构，左半边是Encoder，右半边是Decoder。接下来具体解释Encoder和Decoder的组成。 Encoder Encoder由一系列block组成，每一个block里面都包含了self-attention层和FC层。 实际上，Transformer里融合了ResNet的思想，再self-attention产生了输出向量序列后，还会加上输入的向量，然后在一起进行layer normalization，就是对每一个向量，减掉其平均值后再除以标准差。进行了layer normalization后的向量才会被输入FC。 如下图所示，FC的部分也有residual的部分，经过了FC、layer normalization后，才得到一个block的输出。 此时我们回顾Encoder的架构，首先对于Encoder中的一个block，要做的事情有 Positional Encoding（位置编码，lect4的笔记里有提到） 结合ResNet性质的Self-attention+layer normalization 结合ResNet性质的FC+layer normalization Decoder Autoregressive Transformer Autoregressive transformer的前向传播过程大致是结合Encoder的输出和Begin向量得到第一个输出向量，以语言识别为例，然后取输出向量中对应概率最大的字对应的one hot向量作为第一位输出的结果，然后再用第一位输出的结果输入Decoder，产生下一个输出，直至产生End向量，才代表输出结束。 下图是Decoder的内部架构，Positional encoding和FC两个部分和Encoder是差不多的，所以下面重点分析Masked Attention部分和Masked Attention后的Attention block两个部分。 Masked Self-attention Masked Self-attention和一般的Self-attention不同之处在于：产生第一个输出向量时，只能考虑第一个输入向量，产生第二个输入向量时，只能考虑第一、第二个输入向量，以此类推。下面两张图很好地解释了这个机制 Cross attention 这部分是Decoder内部架构图中第二个attention的模块，下图很好地说明了cross attention地机制。BEGIN向量和“机”向量进行Masked Self-attention，产生q向量，然后再同Encoder产生的输出得到attention score，再加权，通过FC layer得到第二个输出向量。 最重要的一点就是这个过程中的q向量不是来自于Encoder的输出，而是来自Mask Self-attention的输出。 Non autoregressive Transformer(NAT) 和autoregressive transformer（AT）不同，NAT一次性产生所有的输出向量，然后截取END向量之前的向量作为最后的输出序列。 相比AT，它的优势在于的产生速度快，并且可以控制输出长度，但是它的效果往往不如AT。 Transformer的训练 loss的来源 loss的来源可以是每个输出的向量与正确向量的交叉熵，在训练时我们希望交叉熵最小化。 训练的一些tips Teacher Forcing 为了防止训练时出现因第一个向量输出错误导致接下来的训练错误这样“一步错，步步错”的情况，所以我们在训练时把正确答案输入给decoder。 Copy Mechanism 有时适当地从输入中进行一些copy，可以更好地完成任务。比如在聊天机器人训练中，可以对人名进行copy，因为一个人名在训练资料中出现的次数可能很少，网络无法通过学习习得输出正确人名的能力，所以对人名进行直接copy。 Guided Attention Guided Attention，可以理解为用先验知识来限制self attention层的注意力机制，比如在语音识别中，一个音节的识别只与对应时刻的向量及其周边向量有关。所以应该把attention限制在这个时刻附近较小的范围内。 Beam Search 可以把找输出向量序列看作是树形搜索，所以就把问题看作是设计合适的搜索算法，使得全局的loss最小。 Exposure bias 训练时我们把正确答案直接喂给decoder，但是如果在测试的时候产生了错误的向量，可能就会产生mismatch，从而影响后面的输出。所以我们可以在训练时就适当地喂入错误的向量，使得模型的适应能力更强。","link":"/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/"},{"title":"记手搓ResNet的经历","text":"如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用 前言 ResNet基本思想 代码 各部分详解 Residual block resblock_basic类 resblock_bottlenect类 resnet类 References 前言 在上学期的机器视觉大作业中我用到了ResNet50-Unet，寒假中做分类任务时又用到了ResNet，但是之前我用ResNet要么是pip之后直接import，要么是参照hw里面助教给的初始代码进行增删。本着搞懂ResNet这么一个经典模型的心态，我决定自己手搓一遍ResNet（好吧其实还是有参照，但是在参照的基础上加了一点东西）。 ResNet基本思想 ResNet通过引入直接连接的旁路（shortcut），减少了反向传播时梯度消失的问题，使得模型能搭的更深，更不容易过拟合。 下表是各种CNN架构在ImageNet数据集上的top-5 error rate，可以看到ResNet相比VGG等其它架构，有着更好的效果。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190&quot;&quot;&quot;Implementation of ResNet with pytorchSimple usage: from resnet_pytorch import * classifier = resnet()All usage: demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;) Input no parameters: classifier = resnet(class_num=16) # return resnet50References:[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1[2] https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py[3] 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197&quot;&quot;&quot;import torchimport torch.nn as nnclass resblock_basic(nn.Module): &quot;&quot;&quot; the block for resnet18 and resnet34 &quot;&quot;&quot; # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( # if the kernel size equals to 3, the padding should be 1 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size and number of channels are equal to the input, # the shortcut path do nothing self.shortcut = nn.Sequential() # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resblock_bottleneck(nn.Module): &quot;&quot;&quot; the block for resnet50, resnet101 and resnet152 &quot;&quot;&quot; # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 4 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), # maxpooling is replaced by convolution layer with stride unequal to 1 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resnet(nn.Module): def __init__(self, block=resblock_bottleneck, channel=3, filter_list=None, block_num_list=None, class_num=10, net_type=None): &quot;&quot;&quot; block: type of block, default: bottleneck channel: the channel of image, 1 for gray image and 3 for RGB image. default: 3 filter_list: the filter numbers of each blocks' first layer. default: None block_num_list: repeat times for each block, the length should be equal to filter_list. default: None class_num: the number of classes for classification. default: 10 net_type: the type of resnet, 'resnet50' for example. default: None demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;) Input no parameters: classifier = resnet(class_num=16) # return resnet50 &quot;&quot;&quot; super().__init__() if block_num_list is None: block_num_list = [3, 4, 6, 3] if filter_list is None: filter_list = [64, 128, 256, 512] # different types of resnet in the original paper if net_type == 'resnet18': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [2, 2, 2, 2] elif net_type == 'resnet34': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet50': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet101': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 23, 3] elif net_type == 'resnet152': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 8, 36, 3] self.resblock_in_channel = 64 self.pre_conv_layer = nn.Sequential( nn.Conv2d(in_channels=channel, out_channels=self.resblock_in_channel, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(self.resblock_in_channel), nn.ReLU(inplace=True) ) stride_list = [1] + [2] * (len(filter_list) - 1) self.resblocks = nn.ModuleList() for i in range(len(filter_list)): self.resblocks.append(self._make_block(block, filter_list[i], block_num_list[i], stride_list[i])) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Sequential( nn.Dropout(0.25), nn.Linear(filter_list[-1], class_num) ) def _make_block(self, block, filter, block_num, stride): layers = [] strides = [stride] + [1] * (block_num - 1) for stride in strides: layers.append(block(self.resblock_in_channel, filter, stride)) self.resblock_in_channel = filter * block.expansion return nn.Sequential(*layers) def forward(self, x): output = self.pre_conv_layer(x) for block in self.resblocks: output = block(output) output = self.avg_pool(output) output = output.view(output.size(0), -1) output = self.fc(output) return output 各部分详解 Residual block 在ResNet的原始论文中，提出了如下图两种residual block，右边的一种被称为bottleneck。前一种residual block在ResNet层数较浅时使用，如ResNet18，ResNet34；后一种residual block在ResNet层数较深时使用，如ResNet50、ResNet101、ResNet152。 resblock_basic类 前向传播过程：residual block前向传播的过程，要经过两次卷积+batch normalization，其中第一次卷积、batch normaliztion后，需要经过ReLU，而第二次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，两个卷积层输出的channel数是相同的。 降采样方法：residual block里面不设置max pooling，而是通过卷积层中设置大于1的步长起到降采样的作用，一个block中只有第一层卷积层中的stride可能大于1，第二个卷积层的stride为1。 padding：由于卷积核大小为3x3，所以两个卷积层的padding都应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resblock_bottlenect类 前向传播过程：residual block前向传播的过程，要经过三次卷积+batch normalization，其中前两次卷积、batch normaliztion后，需要经过ReLU，而第三次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，前两个卷积层输出的channel数是相同的，而第三个卷积层输出的channel数是前两层的四倍。 降采样方法：一个block中只有第二层3x3卷积层中的stride可能大于1，第一、三个1x1卷积层的stride为1。 padding：由于第二层卷积核大小为3x3，所以第二个卷积层的padding应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resnet类 在原始论文中，ResNet要先经过一个7x7卷积层，然后在经过若干个residual block，最后通过FC得到输出。 预卷积层：原始论文中，预卷积层卷积核大小为7x7，所以padding=3，该卷积层步长为2，起到降采样作用，输出channel数设置为64。 residual block序列：中间的residual block序列可以用 nn.ModuleList存放，通过 _make_block函数循环添加。 自适应平均池化层：将特征图自适应转化为序列。 全连接层：设置0.25 dropout率，然后再全连接。 References Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1 https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197","link":"/Deep-Learning/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Self attention","slug":"Self-attention","link":"/tags/Self-attention/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"}],"categories":[{"name":"李宏毅机器学习","slug":"李宏毅机器学习","link":"/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"}]}
{"pages":[{"title":"关于我","text":"浙江大学光电科学与工程学院2020级本科生 正准备入计算成像+AI的坑 联系方式见网页左下","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"hexo配置问题","text":"记录hexo配置中遇到的各种问题，随缘更新中…… OS：Windows 10 主页类型：gitpage 官方文档：文档 | Hexo 第一个网页 部署到github 配置主题 安装 配置 正文部分 写入正文 编辑文章的分类、标签 插入图片 插入公式 插入TOC目录 插入代码 About页 Tag页 第一个网页 此部分根据官方文档操作即可完成，故不赘述 安装：文档 | Hexo 建站：建站 | Hexo 部署到github 此部分根据官方文档操作即可完成，故不赘述 在 GitHub Pages 上部署 Hexo | Hexo 一键部署 | Hexo 配置主题 这里面有非常多主题，随便选一个自己喜欢的用就好了。 我选择的主题是Anatolo，这个主题的作者锦心是一位可爱的OIer妹妹（她还不到大一就能自己写theme，而我配置她的主题都磕磕绊绊quq），她的主页：Lhc_fl Home (lhcfl.github.io) 安装 参考Anatolo (lhcfl.github.io) 配置 复制_config.example.yml为_config.yml，修改hexo根目录下的 _config.yml ： theme: Anatolo 上面是锦心写的配置方法，但是当时我花了很久时间才理解，具体来说应该是把themes/Anatolo下的_config.example.yml文件复制到themes/Anatolo（一开始我以为是复制到根目录……），然后修改hexo项目根目录下的 _config.yml ： theme: Anatolo。 正文部分 写入正文 新建文章 ：用hexo new &lt;title&gt; ，可以在_post目录下新建&lt;title&gt;.md文件，之后便可以按照markdown语法开始写作。 编辑文章的分类、标签 参照如下写法即可 1234567title: hexo配置问题date: 2023-01-12 02:45:58tags: - 环境配置 - hexocategories: - 环境配置 插入图片 这篇文章十分详细地讲了如何方便地结合Typora插入图片：hexo博客如何插入图片 - 知乎 (zhihu.com) 插入公式 找不到参考的文章了quq 告诫我配置完环境一定要马上整理 警钟长鸣 插入TOC目录 直接用Typora中的TOC是不能插入目录的！ 正确插入方法如下： 安装hexo-toc插件：npm install hexo-toc --save 参照文档内的方法编辑_config.yml 在想插入目录的地方写入 1&lt;!-- toc --&gt; 插入代码 无需插件，正常写入即可。 TODO：学习利用插件对代码块进行优化的方法。 About页 用hexo new page &quot;about&quot;新建“关于”页面，然后正常写入内容即可 Tag页 用hexo new page &quot;tag&quot;新建标签统计页面即可，无需写入任何内容","link":"/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"},{"title":"李宏毅机器学习-hw3-CNN-总结","text":"任务介绍 训练结果 训练方法 Data augmentation 模型设计 Loss function的选择与调参 Learning rate调整方案 Test time augmentation 进一步提升的可能 数据增强可能可以尝试mix up 模型设计可能可以继续尝试 Learning rate调整方案有待提升 Cross Validation + Ensemble 其它值得记录的东西 autodl tensorboard使用 layer的梯度变化 任务介绍 这个作业要求训练一个能对11种食物进行分类的CNN。使用的数据集为food-11，数据集划分如下： 训练集：9866张带有label的图像 验证集：3430张带有label的图像 测试集：3347张不带有label的图像 完成训练后，将用测试集进行测试，输出含有3347张图片的预测label的csv文件，上传至kaggle后系统根据预测准确率自动评分。 Baseline如下： Simple : 0.50099 Medium : 0.73207 Strong : 0.81872 Boss : 0.88446 训练结果 Times private score public score Improvement（相比前一步） 1 0.57063 0.55278 直接运行初始代码 2 0.71830 0.75298 数据增强、调整模型、调整loss function 3 0.75757 0.78884 调整数据增强、手搓ResNet、loss function调参 4 0.77592 0.80378 减少ResNet层数、调整learning rate 5 0.77251 0.80677 减少ResNet层数 6 0.82415 0.85358 增加ResNet层数 7 0.84208 0.87450 使用tta 训练方法 Data augmentation 在观察了部分数据之后，发现合理的图片增强方式包括但不限于 缩放裁剪 随机翻转 随机旋转 仿射变换 随机灰度化 最终代码如下所示： 123456789101112train_tfm = transforms.Compose([ # Resize the image into a fixed shape (height = width = 128) transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)), transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转 transforms.RandomVerticalFlip(p=0.5), # 随机上下翻转 transforms.RandomRotation(degrees=(0, 180)), # 图像随机旋转 transforms.RandomAffine(30), transforms.RandomGrayscale(0.2), # 随机灰度化 # You may add some transforms here. # ToTensor() should be the last one of the transforms. transforms.ToTensor(),]) 模型设计 模型使用的是ResNet，ResNet的搭建见：记手搓ResNet的经历 · 核子的Blog (hezj-opt.github.io) 在调整参数方面，最终调整预卷积层的通道数为32，全连接层的dropout为0.4，残差块采用两个3x3卷积层和shortcut path构成的残差块，最终使用通道数为64、128、256、512的残差块数量分别为1、2、1、1 1model = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 2, 1, 1], 11).to(device) 在第4次训练中，使用过层数较少的网络（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 2, 1], 11).to(device)），但是效果不佳，当时以为是over fitting，所以在第5次训练中把网络改得更简单了（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 1, 1], 11).to(device)），修改后发现效果不仅没有显著提升，而且训练时长增加了非常多（第4次训练大致12小时，第5次训练花了将近16小时）。所以发现是模型欠拟合，决定增加模型复杂度，才在第6次训练后通过strong baseline。 鉴于几次调参后的效果，推断三次训练在下图中位置红色线框覆盖的区间内。 Loss function的选择与调参 查阅资料发现，使用Focal loss会比Cross entropy有更快的收敛速度（参数γ导致），而且更适合各个标签的数据集不均的情况（参数α导致）。 下图显示了由于Focal loss的γ参数，Focal loss会收敛得更快 Focal loss中的α参数可以平衡训练集中各类图片数量的差异，比如在本次任务中，我先统计训练集中了11类食物的数量，然后对样本少的赋比较高的α值，样本多的食物赋比较低的α值。 123count = count / np.max(count) # count 为11类食物样本数的统计结果alpha = 1 / count# alpha = [1, 2.317, 0.663, 1.008, 1.172, 0.750, 2.259, 3.55, 1.163, 0.663, 1.402] Learning rate调整方案 优化器选择Adam，初始学习率为0.0004，设置decay=1e-5 在学习率调整上，通过余弦退火调整学习率，退火周期为16，即每过16轮，学习率突然增大，然后再慢慢减小。如下图除了蓝线、红线以外的曲线所示。 这么做好处有二，一是可以加快收敛速度，二则是可以帮助“翻越” loss surface上的一些小山峰，从而更好地跳出“局部最小”（其实往往不是局部最小，但是有“跳出”的作用）找到全局最优解。 Test time augmentation 训练时，我们对训练数据进行了增强，但是测试时用的就是正常的图片，但是我们想利用模型对增强的图片的识别效果辅助判断。所以可以让测试图片产生若干张增强的图片，依次求未增强图片和增强图片的预测向量，然后把向量相加后再求出最大值所在位置，就是最终预测结果。模型对同一个物体的原图和多张增强图像进行预测，一定程度上可以校正只对原图进行预测时的错误。 比如在本次hw中，我对每张测试图片产生五张增强图片，对六张图片进行预测，得到 preds列表，内含未增强图像的预测结果（preds[0]）和五张增强图像的预测结果。最终的预测结果为 12preds = 0.5* preds[0] + 0.1 * preds[1] + 0.1 * preds[2] + 0.1 * preds[3] + 0.1 * preds[4] + 0.1 * preds[5]prediction = np.argmax(preds, axis=1) 进一步提升的可能 数据增强可能可以尝试mix up mix up数据增强是一种进阶的数据增强，它可以使得模型在判断时不会那么绝对，可以减小过拟合。如果要实现mix up，要改写Dataset类，并且还需要写适用于mix up的loss function。 模型设计可能可以继续尝试 过了strong baseline以后我没有继续调整模型，表格中最后一次产生的结果实际上用的是第6次训练得到的模型，只是在测试时采取tta。所以可能可以继续尝试加深模型，直至出现过拟合，再适当减小模型。 Learning rate调整方案有待提升 我的实现中退火周期时固定的，但是看了原始论文后，发现设置合适的初始退火周期，然后在每个退火周期后，将退火周期增大一倍可能是个不错的策略。 Cross Validation + Ensemble 简单地来说，Cross Validation + Ensemble是先把训练集和验证集重新进行划分，进行k种划分，训练出k个模型，最终用k个模型对图片进行多次预测。效仿tta的方式融合产生预测结果，此时不同的模型之间可以互相弥补缺陷，所以可以得到比较好的预测结果。 但是这玩意真的耗时间，第6次训练花费时间为8-9h，如果训练四个模型得35h左右…… 其它值得记录的东西 autodl tensorboard使用 本次作业中我学习了autodl自带的tensorboard，发现操作十分简单。 开机后打开AutoPanel 用pip下载tensorboard 1$ pip install tensorboard 新建一个writer 12from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter('/root/tf-logs') 把想要看的东西放进tensorboard 123456789101112131415# loss 曲线writer.add_scalars(main_tag='Loss', tag_scalar_dict={'train': train_loss, 'valid': valid_loss}, global_step=epoch + 1)# Accuarcy曲线writer.add_scalars(main_tag='Accuracy', tag_scalar_dict={'train': float(train_acc), 'valid': float(valid_acc)}, global_step=epoch + 1)# 各层的参数的梯度绝对值曲线for name, parms in model.named_parameters(): writer.add_scalar(f&quot;Grad/{name}&quot;, torch.norm(parms.grad), epoch + 1) layer的梯度变化 下面两张图时预卷积层的weight的梯度模长变化和某个残差块中的一个卷积层的weight的梯度模长变化（经过平滑后的结果）。所以说可以发现哪怕训练到几乎停滞，其实并不是陷入局部最小值，甚至不在鞍点。所以之前推测学习率调整方案还可以进一步优化。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/"},{"title":"李宏毅机器学习-lect3-CNN","text":"CNN 的想法 卷积层（Convolutional layers） 卷积核 步长 （Stride） 填充 （Padding） 参数共享（Parameter Sharing） 卷积层的运行过程 卷积层的两种理解方式 池化层（Pooling layers） Flatten layers CNN 的缺点 CNN 的想法 对于图片分类的任务，我们可以采用让特征识别（这与人类分类物体的方法是类似的）的方法，让每一个神经元只与部分区域（Receptive field）关联，而不需要每一层都full connect。 所以，CNN是为了影像的特性而生的，把CNN用于影像领域外的任务要仔细思考是否出现影像类似的特性。 卷积层（Convolutional layers） 卷积核 卷积核大小表示局部特征区域（即Receptive field）的大小 卷积核一般大小取3x3，同时包含RGB三个通道 Q：卷积核大小只有3x3，如果图片尺寸比较大，3x3会不会无法识别特征？ 这个问题将在下面回答 步长 （Stride） 每次卷积核移动的长度，一般设为1或2，因为我们希望Receptive field之间是有重叠的，因为如果Receptive field之间完全没有重叠，那么如果pattern出现在两个Receptive field的交界上，特征就难以被识别。 填充 （Padding） 移动卷积核时，如果步长大于1，则移动时卷积核可能会超出图片范围，则需要在边上填充一些值，常见的方法有补0法、取平均法等。 参数共享（Parameter Sharing） 对于一个特征，可能出现在图片不同的位置，而对于一个区域，有一组神经元负责，每个神经元负责识别不同的特征，所以此时可以让负责不同区域，但功能相同的神经元享有相同的参数，从而减少参数数量。 对于每一个区域，有一组神经元负责，每个神经元有一组参数，这一组组参数叫做filter，所有区域共享一组filter 卷积层的运行过程 之前提到，图片中每一个小区域有一组神经元负责，每一个神经元的参数叫做filter，所有小区域共享一组filter，那么卷积层可以看作每一个filter对图像分别作用，得到一组图像，所有的filter对图像作用后，得到了新的图像，图像的channel数则为filter的数量。这样的一张图片叫做特征图像。 所以一张图像经过卷积层后，会得到一张特征图像。之前有提到，卷积核大小只有3x3时会不会无法识别较大特征，这是不会的，因为在下一个卷积层中对特征图像做卷积时，若卷积核为3x3，步长为1时，则相当于对5x5大小区域卷积。 卷积层的两种理解方式 池化层（Pooling layers） 对于一张较大的图片而言，采样时少采样一些点并不会影响图像是什么 池化层并没有参数，其操作时固定的，相当于一个算符 常用的池化方法有Max Pooling，过程如下图所示，一般而言，池化时分组大小为2x2 一般而言，池化常常在卷积层后使用，如一个或两个卷积层后跟一个池化层，用于缩小图片，从而减小运算量。但是这对于网络的效果而言可能是由损害的，因为如果特征特别细小，则池化可能会漏过特征。 Flatten layers 图像经过一系列卷积、池化后，得到小的特征图像，此时这个特征图像代表图片中较大的、全局的特征，此时就可以把图像展平，然后通过全连接层，经过softmax归一化后，得出分类的结果。 其实到这里，和回归问题里用到的神经网络是类似的，只不过回归问题的特征是显式的，所以一开始就可能是全连接层，而影像类任务中，一开始需要先提取特征，最后再让特征经过全连接层计算。 CNN 的缺点 CNN很难处理图片的缩放、旋转，所以我们需要数据增强（data augmentation）","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/"},{"title":"李宏毅机器学习-lect4-Self attention","text":"Self attention 解决什么问题 FC(Fully Connected)有什么不足 Self attention 的架构 Self attention 的基本计算过程 求attention score 求解Self attention层输出的向量 矩阵视角下的计算过程 Self attention的优化 Multi-head Self attention Positional Encoding Self attention 和其他网络对比 CNN RNN GNN Self attention 解决什么问题 用一句话概括就是：Self attention用来解决当输入为向量的序列时的问题（像音频、文本都是经典的输入为向量序列的数据） Self attention的输出一般有如下三种类型 N个vector产生N个label ​ 例如输入一个句子，输出每个词的词性 N个vector产生1个label ​ 例如输入一个句子，判断这句话蕴含的情绪为positive or negative N个vector产生N‘（N≠N’）个label ​ 例如机器翻译，输入的句子和输出的句子词数很可能不一样 FC(Fully Connected)有什么不足 FC也可以用来解决输出为向量序列的问题，如天气预测等等，但是它相比self attention有一些不足之处。 FC如果要充分考虑“上下文”——一个向量和它相邻的很多个向量，甚至可能整个序列一起考虑，就需要把考虑的向量串联起来，通过fully connect产生新的向量，那么参数的矩阵可能会非常大（这个在后面会解释），这可能导致很大的运算量和overfitting。 Self attention 的架构 首先我们来看一个self attention层要做什么。 概况一下就是，一个self attention层要先算出每个向量和其他向量的关联性，这个关联性用attention score α\\alphaα 表示。然后再根据加权算出输出向量序列，值得一提的是，计算输出向量序列中的每个向量是并行的。 然后我们再看整个网络架构，经过self attention层后，产生了输出向量序列，这时候可以先对每一个向量进行fully connect，产生输入到下一个self attention层中的输入向量，然后就是重复，直至产生最终输出。 Self attention 的基本计算过程 求attention score 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求a1\\mathbf{a^1}a1与自身及其他向量的attention score α1,i′\\alpha'_{1,i}α1,i′​为例，来说明求解流程 对a1\\mathbf{a^1}a1求q1\\mathbf{q^1}q1，q1=Wq⋅a1\\mathbf{q^1}=\\mathbf{W}^q\\cdot\\mathbf{a^1}q1=Wq⋅a1，其中Wq\\mathbf{W}^qWq为参数矩阵，是要学习的参数 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…，ki=Wk⋅ai\\mathbf{k^i}=\\mathbf{W}^k\\cdot\\mathbf{a^i}ki=Wk⋅ai，其中Wk\\mathbf{W}^kWk为参数矩阵，是要学习的参数 求q1\\mathbf{q^1}q1和k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…的关联性，常见方法是向量点乘，如α1,2=(q1)T⋅k2\\alpha_{1,2}=(\\mathbf{q^1})^\\mathrm{T}\\cdot\\mathbf{k^2}α1,2​=(q1)T⋅k2 对上一步求得的α1,1,α1,2,…\\alpha_{1,1},\\alpha_{1,2},\\dotsα1,1​,α1,2​,…通过激活函数，常用softmax，最后得到attention score α1,1′,α1,2′,…\\alpha'_{1,1},\\alpha'_{1,2},\\dotsα1,1′​,α1,2′​,… 可以参照下图直观地理解上述步骤 求解Self attention层输出的向量 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求输出向量序列中的b1\\mathbf{b^1}b1为例，来说明求解流程 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求v1,v2,…\\mathbf{v^1},\\mathbf{v^2},\\dotsv1,v2,…，vi=Wv⋅ai\\mathbf{v^i}=\\mathbf{W}^v\\cdot\\mathbf{a^i}vi=Wv⋅ai，其中Wv\\mathbf{W}^vWv为参数矩阵，是要学习的参数 利用求得的attention score对vi\\mathbf{v^i}vi进行加权：b1=∑α1,i′v1\\mathbf{b^1}=\\sum \\alpha'_{1,i}\\mathbf{v^1}b1=∑α1,i′​v1，从而得到b1\\mathbf{b^1}b1 可以参照下图直观地理解上述步骤 矩阵视角下的计算过程 下面三张图非常直观地展示了 产生qi,ki,vi\\mathbf{q^i},\\mathbf{k^i},\\mathbf{v^i}qi,ki,vi 计算attention score 计算输出向量bi\\mathbf{b^i}bi 上面提到的矩阵视角下的计算过程可以归结为下图，我们也可以发现，对于一层self attention层而言，需要学习的参数只有Wq,Wk,Wv\\mathbf{W}^q,\\mathbf{W}^k,\\mathbf{W}^vWq,Wk,Wv 前面说到FC可能在考虑整个向量序列的情况下有大量的参数，比如说输入的向量序列为10000个100x1的向量，如果在考虑整个向量序列的情况下要产生10000个10x1的向量，用FC需要的参数数量级为(10000x100)x(10000x10)，而如果用self attention，那么Wq\\mathbf{W^q}Wq的元素个数数量级为100x100。 Self attention的优化 Multi-head Self attention 考虑到在不同的视角下，一个向量和其他向量会有不同的关联性，所以我们可以用不同系数矩阵，最终产生多组向量序列输出。 我们也可以对多组向量序列输出进行拼接、变换，得到一组输出向量序列 Positional Encoding 在之前的产生输出向量序列的操作中，我们可能忽略了输入向量序列在坐标或者时间尺度上的相关性，而这有时是很重要的，比如在音频识别中，识别一个音符考虑的就是在一小段时间内的信号，那么这一小段时间内向量的相关性就极强。 所以我们可以通过在输入向量序列上加一个用来代表位置的向量，来为输入向量增加坐标/时间信息。这个代表位置的向量可以是直接计算得到的，也可以是通过神经网络学习得到的。 Self attention 和其他网络对比 CNN 其实图像也可以看作是向量的序列——比如一张100x100x3的图片，可以看作是100x100个向量（一个像素的RGB三通道值作为向量）。 通过设置如果attention score合适，是不是也可以表现出CNN的效果？而且self attention可以打破CNN中由于卷积核带来的像素只与周边像素作用的限制，让一个像素可以和离它较远的像素作用。 所以，self attention是更灵活的CNN，而CNN是简化的self attention 在二者训练效果对比中也可以发现，对于小的数据集，自由度比较小的CNN表现得更好，对于大数据集而言，self attention表现的效果更好。 RNN 相比RNN而言，self attention由于并行运算可以获得更快的运算速度，同时相比RNN不易考虑到距离一个向量在坐标尺度下比较远的向量，self attention可以通过attention score充分考虑到一个向量在坐标尺度下距离比较远的向量。 GNN self attention可以看作是一种特殊的GNN，如果两个点直接由连线，那么attention可以设为1，否则直接设为0。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/"},{"title":"李宏毅机器学习-lect5-Transformer","text":"Transformer是什么 Transformer的应用 Transformer的架构 Encoder Decoder Autoregressive Transformer Masked Self-attention Cross attention Non autoregressive Transformer(NAT) Transformer的训练 loss的来源 训练的一些tips Teacher Forcing Copy Mechanism Guided Attention Beam Search Exposure bias Transformer是什么 Transformer可以理解为是Sequence-to-sequence（简称Seq2seq）的模型，它接受向量序列作为输入，输出向量序列。 Transformer的应用 Transformer的应用除了上图提到的语音识别、机器翻译、语音翻译以外，还有 聊天机器人 句子词性分析 多标签分类 物体检测 Transformer的架构 Transformer由编码器（Encoder）和解码器（Decoder）组成，前向传播的过程是Encoder将输入向量序列编码产生新的向量序列，然后Decoder将编码的向量结合begin向量（标记着位置，是一个one hot向量）产生第一个输出向量，然后把产生的第一个向量再输入Decoder，产生第二个向量，直到产生end向量为止，代表着输出完成。 如下图所示，是Transformer的内部结构，左半边是Encoder，右半边是Decoder。接下来具体解释Encoder和Decoder的组成。 Encoder Encoder由一系列block组成，每一个block里面都包含了self-attention层和FC层。 实际上，Transformer里融合了ResNet的思想，再self-attention产生了输出向量序列后，还会加上输入的向量，然后在一起进行layer normalization，就是对每一个向量，减掉其平均值后再除以标准差。进行了layer normalization后的向量才会被输入FC。 如下图所示，FC的部分也有residual的部分，经过了FC、layer normalization后，才得到一个block的输出。 此时我们回顾Encoder的架构，首先对于Encoder中的一个block，要做的事情有 Positional Encoding（位置编码，lect4的笔记里有提到） 结合ResNet性质的Self-attention+layer normalization 结合ResNet性质的FC+layer normalization Decoder Autoregressive Transformer Autoregressive transformer的前向传播过程大致是结合Encoder的输出和Begin向量得到第一个输出向量，以语言识别为例，然后取输出向量中对应概率最大的字对应的one hot向量作为第一位输出的结果，然后再用第一位输出的结果输入Decoder，产生下一个输出，直至产生End向量，才代表输出结束。 下图是Decoder的内部架构，Positional encoding和FC两个部分和Encoder是差不多的，所以下面重点分析Masked Attention部分和Masked Attention后的Attention block两个部分。 Masked Self-attention Masked Self-attention和一般的Self-attention不同之处在于：产生第一个输出向量时，只能考虑第一个输入向量，产生第二个输入向量时，只能考虑第一、第二个输入向量，以此类推。下面两张图很好地解释了这个机制 Cross attention 这部分是Decoder内部架构图中第二个attention的模块，下图很好地说明了cross attention地机制。BEGIN向量和“机”向量进行Masked Self-attention，产生q向量，然后再同Encoder产生的输出得到attention score，再加权，通过FC layer得到第二个输出向量。 最重要的一点就是这个过程中的q向量不是来自于Encoder的输出，而是来自Mask Self-attention的输出。 Non autoregressive Transformer(NAT) 和autoregressive transformer（AT）不同，NAT一次性产生所有的输出向量，然后截取END向量之前的向量作为最后的输出序列。 相比AT，它的优势在于的产生速度快，并且可以控制输出长度，但是它的效果往往不如AT。 Transformer的训练 loss的来源 loss的来源可以是每个输出的向量与正确向量的交叉熵，在训练时我们希望交叉熵最小化。 训练的一些tips Teacher Forcing 为了防止训练时出现因第一个向量输出错误导致接下来的训练错误这样“一步错，步步错”的情况，所以我们在训练时把正确答案输入给decoder。 Copy Mechanism 有时适当地从输入中进行一些copy，可以更好地完成任务。比如在聊天机器人训练中，可以对人名进行copy，因为一个人名在训练资料中出现的次数可能很少，网络无法通过学习习得输出正确人名的能力，所以对人名进行直接copy。 Guided Attention Guided Attention，可以理解为用先验知识来限制self attention层的注意力机制，比如在语音识别中，一个音节的识别只与对应时刻的向量及其周边向量有关。所以应该把attention限制在这个时刻附近较小的范围内。 Beam Search 可以把找输出向量序列看作是树形搜索，所以就把问题看作是设计合适的搜索算法，使得全局的loss最小。 Exposure bias 训练时我们把正确答案直接喂给decoder，但是如果在测试的时候产生了错误的向量，可能就会产生mismatch，从而影响后面的输出。所以我们可以在训练时就适当地喂入错误的向量，使得模型的适应能力更强。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/"},{"title":"李宏毅机器学习-lect6-GAN","text":"GAN的基本思想 生成器 “对抗”的含义 训练过程概述 GAN的理论 目标函数 JS divergence的缺陷 Wasserstein distance 评价生成器 评价指标 图像质量 图像多样性 评价函数 Inception score FID Conditional GAN（条件式生成） Conditional-GAN+监督学习 Cycle-GAN 为什么需要Cycle-GAN Cycle-GAN的结构 Cycle-GAN的其他应用 GAN的基本思想 生成器 GAN的中文名叫做对抗生成网络，生成网络的一大特点是让AI具有创造力，所以和CNN、Transformer的一大不同点就是对于一个输入而言，由于生成器同时接受一个随机变量，所以输出并不是固定的。如下图所示，生成器接受x和随机变量z作为输入，其中随机变量z需要来自一个简单到我们可以写出其表达式的随机分布（这样我们才能从中采样），然后输出y，y是一个复杂的随机分布。 那么为什么输出需要是随机分布，而不是像CNN一样固定的值呢？最重要的原因是，在很多问题中答案不一定是唯一的，如果使用监督学习，可能有两大坏处： 给出的结果可能是模糊、不合实际的 比如在某个小游戏的视频预测任务中，AI需要根据之前的帧产生接下来的动画，如果使用监督学习，会发现在路口处小人可能会分裂成两个，原因是训练资料内在入口处小人可能向左转或向右转，这两种情况本来都是合理的，但是如果使用监督学习，AI在训练时不能明确哪种是正确的答案，那么为了使得loss最小，AI就会给出模棱两可的答案——分裂，而这可能是不合实际的。 AI缺乏“创造力” AI在某些特定的人物需要一定的“创造力”，比如让AI绘画，要求画出有红眼睛的人物，答案显然不是唯一的；又如让AI回答“你知道辉夜是谁吗？”，也有不止一种回答。 “对抗”的含义 为什么起名叫对抗生成网络，原因来自生成器的训练是伴随着它的“对手”——鉴别器的训练进行的。在生成器的训练中，我们会同时训练一个鉴别器，用来判定生成器的表现。 以生成二次元人物头像为例：生成器要生成二次元人物头像，而鉴别器要鉴别生成的图像像不像人工绘制的二次元人物头像，此时为了“骗过”鉴别器，生成器要让自己生成的图像更像真人绘制的二次元人物头像，而鉴别器为了更好地把生成器生成的二次元头像检查出来，也要提升自己的鉴别水平，最终二者相互促进，使得生成器生成非常逼真的二次元人物头像。 训练过程概述 GAN的训练过程，大致可以概述为如下步骤： 固定生成器参数，训练鉴别器 因为一开始未经训练的鉴别器是毫无鉴别功能的，所以我们应当先训练鉴别器。训练时同时给鉴别器喂入真实样本（标记为1）和生成器随机生成样本（标记为0）。 固定鉴别器参数，训练生成器 在鉴别器有一定鉴别功能、能给生成器的表现打一个相对合理的分数后，就可以开始训练生成器了。此时将生成器和鉴别器拼接成一个大网络，固定鉴别器参数，更新生成器参数，使得最终输出分数能提高。 重复前两个步骤 通过重复，最终可以获得良好的生成器。下图概述了上面两个步骤 GAN的理论 目标函数 让我们先考虑生成器的优化目标，以图像生成任务为例，对于生成器而言，目标就是让产生的图片的分布和真实图片的分布是类似的。数学表达式可以写成 G∗=arg⁡min⁡GDiv(PG,Pdata) G^{*}=\\arg \\underset{G}{\\min} Div(P_G, P_{data}) G∗=argGmin​Div(PG​,Pdata​)其中Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)表示产生图片的分布和真实图片分布的距离。 然后再考虑鉴别器的优化目标，对于鉴别器而言，目标是能鉴别出生成的图片。那么在鉴别器优化后，由于鉴别器能力的提升，算出的Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)会变大。我们就可以把目标写成 D∗=arg⁡max⁡DV(D,G) D^{*}=\\arg \\underset{D}{\\max} V(D, G) D∗=argDmax​V(D,G)其中V(D,G)V(D, G)V(D,G)的值和https://arxiv.org/abs/1406.2661里提到的JS divergence在数学形式上是相近的，可以写成 V(G,D)=Ey∼Pdata [log⁡D(y)]+Ez∼Pz(z)[log⁡(1−D(G(z)))] V(G, D)=\\mathbb{E}_{y \\sim P_{\\text {data }}}[\\log D(y)]+\\mathbb{E}_{z \\sim P_z{(z)}}[\\log (1-D(G(z)))] V(G,D)=Ey∼Pdata ​​[logD(y)]+Ez∼Pz​(z)​[log(1−D(G(z)))]上式中第一项代表鉴别器对真实图片的输出，第二项代表鉴别器对生成图片的输出，V(G,D)V(G,D)V(G,D)的形式是很像Cross Entropy的。在优化时，第一项只与鉴别器相关，第二项与鉴别器和生成器都相关。那么最终的优化目标表达式可以写作 min⁡Gmax⁡DEx∼pdata (x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))] \\min _{G} \\max _{D} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] Gmin​Dmax​Ex∼pdata ​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))]这是一个min-max模型，代表着鉴别器和生成器训练时的“对抗”。 JS divergence的缺陷 刚刚我们提到了用JS divergence来衡量生成图片和真实图片之间的差异，但是JS divergence存在比较严重的问题。 生成图片分布PGP_{G}PG​和真实图片分布经PGP_{G}PG​常重叠率是很低以至于可以忽略的，而且就算真的有一部分重叠了，也可能因为采样不足而在采样分布上没有重叠。 而在PGP_{G}PG​和PGP_{G}PG​重叠率为0时，JS divergence的值是恒定的（为log2），此时就无法衡量PGP_{G}PG​和PGP_{G}PG​的真实差距 Wasserstein distance 在https://arxiv.org/abs/1701.07875中，作者提出了WGAN所用到的Wasserstein distance。Wasserstein distance衡量的是将PGP_{G}PG​“移动”到PGP_{G}PG​的最小平均距离。表达式可以写成 min⁡Gmax⁡D∈1−LipschitzEx∼pdata (x)[D(x)]−Ez∼pz(z)[D(G(z))] \\min _{G} \\max _{D\\in{1-Lipschitz}} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[ D(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[D(G(\\boldsymbol{z}))] Gmin​D∈1−Lipschitzmax​Ex∼pdata ​(x)​[D(x)]−Ez∼pz​(z)​[D(G(z))]此时鉴别器有约束条件，这个条件要求鉴别器需要足够平滑，如果没有这个条件，鉴别器的训练会不收敛。 WGAN依然有缺点，有几种提升方案 参数限制（Force the parameters w between c and -c） WGAN-GP（gradient penalty）https://arxiv.org/abs/1704.00028 SNGAN（Spectral Normalization → Keep gradient norm smaller than 1 everywhere）https://arxiv.org/abs/1802.05957 评价生成器 评价指标 图像质量 为了评估生成的图像与真实图像是否相似，可以用一个CNN来对生成的图片进行分类，如果分类结果是明确的，即有一类的值很高，那么说明生成的图像质量很好。 图像多样性 除了评估图像质量以外，还需要评估图像多样性，因为哪怕生成的图像质量很好，如果遇到了mode collapse，那么可能生成的图像都十分相似，这显然不是我们想要的。 除了mode collapse以外，还可能遇到mode dropping的问题，就是生成的图片看上去多样性还行，但是实际上只是真实分布中的一部分而已。 评价函数 Inception score Inception score借助CNN，将所有生成图片经过CNN后的输出取平均，如果各类的值比较平均，那么说明多样性是足够的，此时Inception score较高。同时Inception score也会考虑单张图片经过CNN后的分布，如果分布集中，说明图像质量高，此时Inception score较高。 总的来说，Inception score综合考虑图像质量和图像多样性，图像质量和多样性越高，则Inception score越高。 FID FID衡量的是生成图片分布和真实图片分布的距离。同样借助CNN，将生成图片和真实图片经过CNN，不同的是此时收集的是softmax前的输出，最后得到两个分布，将两个分布视为高斯分布，然后计算两个分布间的Frechet distance。显然FID越小代表两个分布约接近，说明生成器性能更佳。 Conditional GAN（条件式生成） 很多时候，我们希望GAN能生成符合一定条件的结果，比如AI绘画里我们会根据自己的喜好提出一些要求。所以就需要Conditional GAN。 训练Conditional GAN的时候，以生成图片任务为例：对于生成器而言，我们以条件作为x，伴随着随机变量z输入；对于鉴别器而言，应当同时输入图片和条件，以及一个标量（这个标量仅当图片为真实图片，且与要求对应时才是1，否则都是0），鉴别器此时输出的标量一方面衡量图片是否是真实图片，另一方面还要衡量图片和条件是否对应。 Conditional-GAN+监督学习 但是有时AI会“过于具有想象力”，以至于超出条件的约束，比如在通过建筑结构图生成建筑照片的时候，只用GAN的结果相比结构图多了一个类似烟囱的结构。为了克服这个问题，有时会结合GAN和监督学习，将AI的“想象力”控制在合理的范围内。 比如在[1611.07004] Image-to-Image Translation with Conditional Adversarial Networks (arxiv.org)内，在目标函数中包含了L1范数LL1(G)=Ex,y,z[∥y−G(x,z)∥1]\\mathcal{L}_{L 1}(G)=\\mathbb{E}_{x, y, z}\\left[\\|y-G(x, z)\\|_{1}\\right]LL1​(G)=Ex,y,z​[∥y−G(x,z)∥1​]，最终优化目标为 G∗=arg⁡min⁡Gmax⁡DLcGAN(G,D)+λLL1(G) G^{*}=\\arg \\underset{G}{\\min} \\underset{D}{\\max} \\mathcal{L}_{c G A N}(G, D)+\\lambda \\mathcal{L}_{L 1}(G) G∗=argGmin​Dmax​LcGAN​(G,D)+λLL1​(G) Cycle-GAN 在很多时候，我们会遇到没有成对的输出和输出的数据集，比如想要做真人头像和二次元人物头像的风格转换，但是很多真人头像都没有对应的二次元人物头像。 为什么需要Cycle-GAN 以图片风格转换为例，如果使用一般的GAN，那么可能导致生成的图片风格正确，但是和输入的图像没什么关系，比如把李宏毅老师的头像输入，可能产生的是辉夜的头像。那么如果用Conditional-GAN做风格转换，往往又需要成对的训练资料。 Cycle-GAN的结构 以风格转换为例，为了保证风格转换后的图片和原始图片相关，会再加入一个用于将风格转换后的图片转换回原始风格的生成器，那么为了要将风格转换后的图片能转换回原始风格，就会要求生成的图片和原始图片是相关的。相应的，我们需要一个鉴别器来判断转换回原始图片的效果。所以如下图所示Cycle-GAN包含了将两种风格的图片互相转换的生成器和对应的鉴别器。 Cycle-GAN的其他应用","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/"},{"title":"李宏毅机器学习-为什么训练会失败","text":"Training loss很高 这时发生了什么 训练方法 Batch Batch对训练效率影响 Batch对训练效果影响 Momentum Learning rate Loss function Training data的loss很小 过拟合 调整模型复杂度 添加更多数据 Mismatch Training loss很高 这时发生了什么 当微分接近0时，训练会停止。但是此时不一定是在极值点，也可能是在鞍点（saddle point）。 其实训练停止时，微分也不一定很接近0，也就不一定是在saddle point，这个后面会提到 通过求loss function的Hessian矩阵，判断Hessian矩阵正定性可以判断当前是否处在saddle point——H不定时则处在saddle point。 对于神经网络的loss而言，Hessian矩阵是一个维度非常高的矩阵，所以Hessian矩阵正定或负定的可能性很低，所以大部分训练停止的情况其实都是遇到了saddle point。 训练方法 Batch 每一个epoch开始时，会随机分割batch，训练时每次依次用各个batch计算出梯度后更新参数。 Batch对训练效率影响 一个很直观的想法是batch越大，训练时在一个batch上花的时间越久，但是由于GPU有并行运算的能力，其实batch在不是特别大的时候训练时间和batch size=1时相比几乎没有增长。所以适当的增大batch可以提高训练效率。 Batch对训练效果影响 从下图可以看出，小batch的训练准确率比较高。 为什么小的batch训练准确率高呢？原因在于使用小的batch时，每个batch之间的loss function可能存在细微的差别，当一个batch训练时卡在saddle point，对另一个batch可能不是saddle point，可以继续训练。 Momentum 训练时可以通过把上一步的移动和当前计算出的梯度结合，形成新的优化参数的方向，这有利于跳出saddle point，甚至可以帮助“翻越”比较小的山坡，使得loss function得到有效降低。 Learning rate 其实如果learning rate比较大，那么可能会出现还没到saddle point就停下来的可能（此时梯度的模比0还是会大比较多的），如下图所示，loss function的值在两条绿线所指的点反复横跳。 所以我们需要动态调整learning rate，使得尽量不会出现还没到saddle point就停下来的情况 为了动态调整learning rate，我们可以采取两点 对每个参数动态调整learning rate θit+1=θit−ηgit→θit+1=θit−ησitgit\\theta_i^{t+1}=\\theta_i^t-\\eta g_i^t\\rightarrow\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}g_i^tθit+1​=θit​−ηgit​→θit+1​=θit​−σit​η​git​ t代表随着训练轮次而改变，i代表对不同的参数 常见的求σit\\sigma_i^tσit​的方法有Root mean square（σti=1t+1∑0t(git)2\\sigma_t^i=\\sqrt{\\frac{1}{t+1}\\sum_0^t (g_i^t)^2}σti​=t+11​∑0t​(git​)2​ ）和RMSProp（σit=α(σit−1)2+(1−α)(git)2\\sigma_{i}^{t}=\\sqrt{\\alpha\\left(\\sigma_{i}^{t-1}\\right)^{2}+(1-\\alpha)\\left(g_{i}^{t}\\right)^{2}}σit​=α(σit−1​)2+(1−α)(git​)2​ ，比较近的轮次的梯度影响比较大） 对所有参数进行learning decay 上式中η→ηt\\eta\\rightarrow\\eta_tη→ηt​ 我们在训练时，经常使用Adam，这个优化方法结合了动态调整RMSProp方法调整learning rate和momentum。一般来说比较有效。 Loss function 选用不同的loss function，会有不同的error surface，所以选择合适的loss function，可以得到更容易进行优化的error surface。 Training data的loss很小 若testing data的loss比training data的loss大很多，也不一定是过拟合~ 可能的原因 过拟合 mismatch 过拟合 过拟合的解决方法有两种： 调整模型复杂度 添加更多数据 调整模型复杂度 训练数据较多：增加复杂度 训练数据较少：降低复杂度 添加更多数据 搜集更多数据 数据增强（Data augmentation） 左右翻转图片 放大图片局部 …… 但是数据增强要用合理的方式，比如图片识别不能出现使用中基本不出现的形式，如图片上下颠倒 Mismatch mismatch指的是testing data的结构与training data的结构不一致","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/"},{"title":"李宏毅机器学习-lect7-Self-Supervised Learning","text":"自监督学习 在监督学习中，我们用成对的训练资料进行训练（比如中译英人物中中文文章与其对应的英文翻译），而在无监督学习当中，是没有成对的训练资料的（比如不带英文翻译的中文文章）。 自监督学习是无监督学习的一种，他将训练资料x的分为x’和x’‘，用x’进行训练，然后用x’'进行验证。 自监督学习现在被广泛用于大模型的预训练，比如BERT和GPT。 BERT 什么是BERT BERT是芝麻街里的人物一个巨大的预训练语言表征模型，它的全称是Bidirectional Encoder Representations from Transformers。它强调了不再像以往（Transformer）一样采用传统的单向（每一个token的注意力只能在之前的token上）语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，BERT自监督训练时每一个token的注意力可以放到全局。 BERT在预训练后，只需要接上一个额外的输出层，给予一点带标注的资料微调，就可以在翻译、问答等下游语言任务中有优秀的表现。 从BERT开始，一大批巨大模型开始出现，如GPT、T5等。 BERT的训练方法 Masking Input 用一句话概括Masking Input就是让BERT学会做“填空题”，这里的填空题是指在把输入的一些token换成特定的token或者随机的token，从而使完整的句子中出现空位，BERT需要学会预测这些被替换的token是什么，训练时目标是最小化输出token和正确token的交叉熵。 Next Sentence Prediction 这个方法是输入两个句子，判断sentence 1与sentence 2在一篇文章中的前后关系。 怎么使用BERT 经过预训练后的BERT学会了做“填空题”、“排序题”，此时的BERT像干细胞，没有具体的功能，但有分化为执行具体任务的能力。 要让BERT能做具体的任务，我们需要在BERT后加入一个输出层（BERT其实和Transformer的Encoder类似，所以后面需要输出层），然后再给予一点具体任务的成对资料进行训练（这个过程称为Fine-tune，即微调），就可以让BERT做具体的任务。微调前，输出层的参数是随机的，而BERT的参数是经过预训练的。 下面举四个例子具体说明如何使用BERT 文章情感判断 句子词性标记 逻辑判断 摘录型问答 BERT的评估 BERT常用GLUE评估，GLUE里面有九项语言相关任务，通过BERT在九项任务上的性能综合评估BERT。 https://arxiv.org/abs/1905.00537中总结：随着时间推进，BERT的表现越来越好，甚至在多项任务上都能超过人的表现（水平直线）。 为什么BERT有效 解释与佐证 最常见的一种解释是BERT通过做“填空题”或其他预训练任务，真的“学会了”语言，能根据上下文将文字映射到语义空间上，相近意思的词在语义空间上的映射会比较接近。比如同样是“苹”这个字，吃苹果中的“苹”和“果”、“草”的映射比较接近，苹果手机中的“苹”可能和“电”的映射比较接近。所以在BERT对每个字做了有效的映射后，输出层可以很容易地输出正确结果。 取BERT对不同语境下的“苹”的映射，求各个映射之间Cosine Similarity，发现相近语义的“苹”字之间Cosine Similarity比较高，不用语义的“苹”字之间Cosine Similarity比较低。这个实验在一定程度上能说明BERT为什么有效。 对之前解释的质疑 https://arxiv.org/abs/2103.07162中指出，用语言资料训练的BERT用在蛋白质、基因、音乐领域的任务也会有比不用预训练的方法表现更好，所以BERT有效的原因真的是因为BERT“学会了”语言吗？BERT有效的原因目前还在探索中。 Multi-lingual BERT 之前提到的BERT都是单一语言的BERT，但是实际上，预训练BERT的时候可以用多语言的资料进行训练。比如下图指的就是让BERT做不同语言的“填空题”。 https://arxiv.org/abs/1909.09587中提出在zero-shot（提问时没有给出任何参考案例）阅读理解任务中，多Multilingual-BERT在只经过英文问答题的微调后，做中文的阅读题时竟然还能有不低的正确率。 一种解释是对于Multilingual-BERT，其实不同的语言的相同语义的词在语义空间上的映射是很接近的，但是训练这样的Multilingual-BERT是需要大量的资料与运算资源，才能让BERT学到不同语言之间的联系。 但是，如果语言之间真的没有差距，那么比如做英文问答时不会冒出别的语言吗？而这实际上是不会的，因为其实不同语言之间还是有一定的距离的。所以李宏毅课题组进行了一项实验（https://arxiv.org/abs/2010.10041），把中文词汇映射的平均值减去英文词汇映射的平均值，得到两种语言之间的差距。然后让Multilingual-BERT读入英文句子，加上英文到中文之间的差距，竟然能输出中文的句子了，而且部分中英词汇在语义上是相同的。所以一定程度上说明了Multilingual-BERT中蕴藏了语言之间的联系。 GPT GPT的训练和使用其实和BERT是很类似的。 GPT的训练 与BERT相似的是，GPT同样使用自监督学习进行预训练，但是预训练的任务是预测句子中的下一个token。 GPT的使用 和BERT相似的是，GPT在迁移到下游任务时也需要一点成对的训练资料，但是和BERT不同的是，GPT的模型比BERT大得多，以至于连微调GPT都是很困难的，所以GPT在微调是不对GPT的参数进行调节的，并采用few-shot learning（提供问题和多个范例，让GPT给出答案）、one-shot learning（提供问题和一个范例，让GPT给出答案）甚至zero-shot learning（只提供问题，直接要求GPT给出答案）的方法进行训练。 自监督学习在语音和图像领域上的应用 概述 Generative Approaches Predictive Approaches Contrastive Learning Bootstrapping Approaches Simply Extra Regularization 总结","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/"},{"title":"记手搓ResNet的经历","text":"如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用 前言 ResNet基本思想 代码 各部分详解 Residual block resblock_basic类 resblock_bottlenect类 resnet类 References 前言 在上学期的机器视觉大作业中我用到了ResNet50-Unet，寒假中做分类任务时又用到了ResNet，但是之前我用ResNet要么是pip之后直接import，要么是参照hw里面助教给的初始代码进行增删。本着搞懂ResNet这么一个经典模型的心态，我决定自己手搓一遍ResNet（好吧其实还是有参照，但是在参照的基础上加了一点东西）。 ResNet基本思想 ResNet通过引入直接连接的旁路（shortcut），减少了反向传播时梯度消失的问题，使得模型能搭的更深，更不容易过拟合。 下表是各种CNN架构在ImageNet数据集上的top-5 error rate，可以看到ResNet相比VGG等其它架构，有着更好的效果。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190&quot;&quot;&quot;Implementation of ResNet with pytorchSimple usage: from resnet_pytorch import * classifier = resnet()All usage: demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;) Input no parameters: classifier = resnet(class_num=16) # return resnet50References:[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1[2] https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py[3] 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197&quot;&quot;&quot;import torchimport torch.nn as nnclass resblock_basic(nn.Module): &quot;&quot;&quot; the block for resnet18 and resnet34 &quot;&quot;&quot; # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( # if the kernel size equals to 3, the padding should be 1 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size and number of channels are equal to the input, # the shortcut path do nothing self.shortcut = nn.Sequential() # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resblock_bottleneck(nn.Module): &quot;&quot;&quot; the block for resnet50, resnet101 and resnet152 &quot;&quot;&quot; # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 4 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), # maxpooling is replaced by convolution layer with stride unequal to 1 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resnet(nn.Module): def __init__(self, block=resblock_bottleneck, channel=3, filter_list=None, block_num_list=None, class_num=10, net_type=None): &quot;&quot;&quot; block: type of block, default: bottleneck channel: the channel of image, 1 for gray image and 3 for RGB image. default: 3 filter_list: the filter numbers of each blocks' first layer. default: None block_num_list: repeat times for each block, the length should be equal to filter_list. default: None class_num: the number of classes for classification. default: 10 net_type: the type of resnet, 'resnet50' for example. default: None demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;) Input no parameters: classifier = resnet(class_num=16) # return resnet50 &quot;&quot;&quot; super().__init__() if block_num_list is None: block_num_list = [3, 4, 6, 3] if filter_list is None: filter_list = [64, 128, 256, 512] # different types of resnet in the original paper if net_type == 'resnet18': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [2, 2, 2, 2] elif net_type == 'resnet34': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet50': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet101': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 23, 3] elif net_type == 'resnet152': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 8, 36, 3] self.resblock_in_channel = 64 self.pre_conv_layer = nn.Sequential( nn.Conv2d(in_channels=channel, out_channels=self.resblock_in_channel, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(self.resblock_in_channel), nn.ReLU(inplace=True) ) stride_list = [1] + [2] * (len(filter_list) - 1) self.resblocks = nn.ModuleList() for i in range(len(filter_list)): self.resblocks.append(self._make_block(block, filter_list[i], block_num_list[i], stride_list[i])) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Sequential( nn.Dropout(0.25), nn.Linear(filter_list[-1], class_num) ) def _make_block(self, block, filter, block_num, stride): layers = [] strides = [stride] + [1] * (block_num - 1) for stride in strides: layers.append(block(self.resblock_in_channel, filter, stride)) self.resblock_in_channel = filter * block.expansion return nn.Sequential(*layers) def forward(self, x): output = self.pre_conv_layer(x) for block in self.resblocks: output = block(output) output = self.avg_pool(output) output = output.view(output.size(0), -1) output = self.fc(output) return output 各部分详解 Residual block 在ResNet的原始论文中，提出了如下图两种residual block，右边的一种被称为bottleneck。前一种residual block在ResNet层数较浅时使用，如ResNet18，ResNet34；后一种residual block在ResNet层数较深时使用，如ResNet50、ResNet101、ResNet152。 resblock_basic类 前向传播过程：residual block前向传播的过程，要经过两次卷积+batch normalization，其中第一次卷积、batch normaliztion后，需要经过ReLU，而第二次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，两个卷积层输出的channel数是相同的。 降采样方法：residual block里面不设置max pooling，而是通过卷积层中设置大于1的步长起到降采样的作用，一个block中只有第一层卷积层中的stride可能大于1，第二个卷积层的stride为1。 padding：由于卷积核大小为3x3，所以两个卷积层的padding都应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resblock_bottlenect类 前向传播过程：residual block前向传播的过程，要经过三次卷积+batch normalization，其中前两次卷积、batch normaliztion后，需要经过ReLU，而第三次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，前两个卷积层输出的channel数是相同的，而第三个卷积层输出的channel数是前两层的四倍。 降采样方法：一个block中只有第二层3x3卷积层中的stride可能大于1，第一、三个1x1卷积层的stride为1。 padding：由于第二层卷积核大小为3x3，所以第二个卷积层的padding应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resnet类 在原始论文中，ResNet要先经过一个7x7卷积层，然后在经过若干个residual block，最后通过FC得到输出。 预卷积层：原始论文中，预卷积层卷积核大小为7x7，所以padding=3，该卷积层步长为2，起到降采样作用，输出channel数设置为64。 residual block序列：中间的residual block序列可以用 nn.ModuleList存放，通过 _make_block函数循环添加。 自适应平均池化层：将特征图自适应转化为序列。 全连接层：设置0.25 dropout率，然后再全连接。 References Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1 https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197","link":"/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/"}],"tags":[{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"},{"name":"Learning rate优化","slug":"Learning-rate优化","link":"/tags/Learning-rate%E4%BC%98%E5%8C%96/"},{"name":"Test time augmentation","slug":"Test-time-augmentation","link":"/tags/Test-time-augmentation/"},{"name":"Self attention","slug":"Self-attention","link":"/tags/Self-attention/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","link":"/tags/Self-Supervised-Learning/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"}],"categories":[{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"李宏毅机器学习","slug":"Deep-Learning/李宏毅机器学习","link":"/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"手搓记录","slug":"Deep-Learning/手搓记录","link":"/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/"}]}
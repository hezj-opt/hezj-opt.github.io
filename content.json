{"pages":[{"title":"关于我","text":"浙江大学光电科学与工程学院2020级本科生 正准备入计算成像+AI的坑 邮箱：zijun_he@zju.edu.cn or zjheopt@gmail.com","link":"/about/index.html"},{"title":"分类","text":"这个页面暂时在施工中 目前唯一的作用是提醒你屏幕左侧有一个\"&gt;\"形小箭头，点它一下试试！","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"友链","text":"核子的朋友们# 按首字母排序 檬檬/WillaWilla &amp; legged robotics Zicx凡是过往，皆为序章 🌈 交换友链！# 1234name: 核子icon: https://hezj-opt.github.io/images/mylogo.webpurl: https://hezj-opt.github.iodescribe: 关注核子谢谢喵","link":"/links/index.html"}],"posts":[{"title":"hexo配置问题","text":"记录hexo配置中遇到的各种问题，随缘更新中…… OS：Windows 10 主页类型：gitpage 官方文档：文档 | Hexo 第一个网页 部署到github 配置主题 安装 配置 正文部分 写入正文 编辑文章的分类、标签 插入图片 插入公式 插入TOC目录 插入代码 嵌入youtube视频 About页 Tag页 开启评论区 首页显示文章摘要 第一个网页# 此部分根据官方文档操作即可完成，故不赘述 安装：文档 | Hexo 建站：建站 | Hexo 部署到github# 此部分根据官方文档操作即可完成，故不赘述 在 GitHub Pages 上部署 Hexo | Hexo 一键部署 | Hexo 配置主题# 这里面有非常多主题，随便选一个自己喜欢的用就好了。 我选择的主题是Anatolo，这个主题的作者锦心是一位可爱的OIer妹妹（她还不到大一就能自己写theme，而我配置她的主题都磕磕绊绊quq），她的主页：Lhc_fl Home (lhcfl.github.io) 安装# 参考Anatolo (lhcfl.github.io) 配置# 复制_config.example.yml为_config.yml，修改hexo根目录下的 _config.yml ： theme: Anatolo 上面是锦心写的配置方法，但是当时我花了很久时间才理解，具体来说应该是把themes/Anatolo下的_config.example.yml文件复制到themes/Anatolo（一开始我以为是复制到根目录……），然后修改hexo项目根目录下的 _config.yml ： theme: Anatolo。 正文部分# 写入正文# 新建文章 ：用hexo new &lt;title&gt; ，可以在_post目录下新建&lt;title&gt;.md文件，之后便可以按照markdown语法开始写作。 编辑文章的分类、标签# 参照如下写法即可 1234567title: hexo配置问题date: 2023-01-12 02:45:58tags: - 环境配置 - hexocategories: - 环境配置 插入图片# 这篇文章十分详细地讲了如何方便地结合Typora插入图片：hexo博客如何插入图片 - 知乎 (zhihu.com) 插入公式# 找不到参考的文章了quq 告诫我配置完环境一定要马上整理 警钟长鸣 插入TOC目录# 直接用Typora中的TOC是不能插入目录的！ 正确插入方法如下： 安装hexo-toc插件：npm install hexo-toc --save 参照文档内的方法编辑_config.yml 在想插入目录的地方写入 1&lt;!-- toc --&gt; 插入代码# 无需插件，正常写入即可。 TODO：学习利用插件对代码块进行优化的方法。 嵌入youtube视频# 打开一个youtube视频 点击视频下方的分享键 点击弹出窗口中“嵌入”上方的按钮 复制右侧iframe开头的代码 直接复制到md文件中即可（md文件是兼容html语法的） 下面是一个示例 1&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7XZR0-4uS5s\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt; About页# 用hexo new page \"about\"新建“关于”页面，然后正常写入内容即可 Tag页# 用hexo new page \"tag\"新建标签统计页面即可，无需写入任何内容 开启评论区# 我使用的是gitalk，配置参照如下两篇博客 Hexo-Anatolo主题添加gitalk评论系统 - 简书 (jianshu.com) 微调Hexo主题Anatolo接入gitalk · Csome 但是需要提醒一点： _config.yml下的repo属性只需要填写仓库名字即可，不要填写仓库链接！ 首页显示文章摘要# 配置步骤： Anatolo/layout/page.pug文件中block content中调用+make_post函数时的true改成false Anatolo/_config.yml文件中useSummary的值设置为false md文件中的front-matter里面加上excerpt: xxx，xxx为摘要内容 尝试对上面的步骤做一个解释（因为我不懂前端，这个步骤也没有文档说明，解释的可能不对） Anatolo/layout/page.pug 的content这个block可能是产生首页、about页内容的部分，我注意到里面调用了一个mixins中定义的make_post函数，page参数代表的是md文件中的front-matter和正文组成的struct。 123456block content .autopagerize_page_element: .content: .post-page include mixins +make_post(page, false) if page.comments include partial/comments 然后找到Anatolo/layout/mixins.pug中的make_post函数的，这边关注它的前半部分就可以了。我们呢发现如果is_detail的值是true，那么首页的摘要处只会显示文章内容和copyright两部分，所以要修改make_post中的输入参数（步骤1）。 然后我们发现无论config文件中的useSummary值如何，这时程序会去找item.excerpt，所以我们front-matter中要写入excerpt属性，内容为摘要（步骤3）。而如果useSummary的值是false，那么如果我们在front-matter中写入了excerpt属性，那么就会把excerpt的内容作为摘要，如果没有写入，则会把文章的内容直接作为摘要。 设置false的原因是我发现如果设置true，about页面会清楚格式（例如无序列表）。 123456789101112131415161718192021222324mixin make_post(item, is_detail) .post.animated.fadeInDown .post-title h3 if is_detail a= item.title else a(href= url_for(item.path))= item.title if is_detail .post-content p!= item.content if theme.copyright.show .tip!= (item.copyright || theme.copyright.default) + \"&lt;br&gt;\" + __(\"author\") + \": \" + (item.author || theme.author) else .post-content if theme.useSummary - var summary = item.excerpt || item.content p!= truncate(strip_html(summary), {length: 160}) else .card if item.excerpt p!= item.excerpt else p!= item.summary || item.content","link":"/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"},{"title":"李宏毅机器学习-Why Deep Learning?","text":"引言——鱼与熊掌可以兼得吗 为什么需要隐藏层 为什么需要“deep” 实验结果 形象的理解 举例说明 李宏毅老师的这节课没有讲到什么“干货”，但是讲了一个挺有意思的问题，也是解决了一个困扰了我很久的疑问。 引言——鱼与熊掌可以兼得吗# 在机器学习中，我们常常构建一个模型，并希望通过训练让模型找到合适的参数，即从集合H\\mathcal{H}H中找出适合的模型hhh，理论上当参数量足够大的时候，理想的loss可以无限低，但是由于over fitting或其它各种各样的原因，实际的loss无法达到理想。 如下图所示，当模型参数量大，模型可选择面比较大时，理想loss比较小，但是“理想”和“现实”差距大；模型参数少，模型可选择面比较小时，模型的理想loss比较大，但是和“现实”差距小。 而深度学习，是一个能用比较少的参数（这里的少是相对达到同样效果的非深度学习模型而言的），却能达到比较低的理想loss的方法，同时由于参数较少，“理想”和“现实”差距较小。所以说深度学习是“鱼与熊掌可以兼得”的机器学习。 为什么需要隐藏层# 训练模型，其实就是在寻找一个以人力不可能写出的复杂非线性函数。而对于任何一个非线性函数，其实都可以通过合适的采样，然后把采样点连起来形成折线去近似，随着折线数的增加，近似会越来越精确。 而对于折线，可以用一系列的阶梯函数（如下图上的aia_iai​上面的蓝线）叠加生成，这些阶梯函数可以通过线性函数经过sigmoid直接得到或者由两个线性函数经过ReLU后再叠加得到。因此在理论上，只需要一个隐藏层，就可以构造出任意的非线性函数。 为什么需要“deep”# 那么曾经有人就提出了一个问题，既然一个隐藏层就可以表达出任意非线性函数，为什么要Deep Learning而不是“Fat Learning”呢？ 概括成一句话就是：Deep Learning可以从简单的非线性函数构成复杂的非线性函数，相比简单的叠加方式产生非线性函数，Deep Learning能高效地利用比较少的参数产生复杂的非线性函数 实验结果# Conversational Speech Transcription Using Context-Dependent Deep Neural Networks 里面对比了Deep Learning架构和“Fat Learning”架构对于相同任务的效果，对于Deep Learning而言，5层、每层2000个神经元的模型和1层、3772个神经元的模型参数数是接近的，但是5层网络的架构效果更好。 形象的理解# 假设今天我们要实现四个二进制数中1的个数的统计，4个二进制数有16种可能，那么如果电路只有两层，就需要16个门电路，但是如果用下图中的多层同或门电路，就可以只用三个门形成的电路，相对而言就比较高效。 举例说明# 下面举一个构成2k2^k2k条线段组成的折线的例子来说明Deep Learning的优势 xxx到a1a_1a1​，构成了2条线段构成的V形折线，而a2=f(a1)a_2=f(a_1)a2​=f(a1​)、a3=g(a2)a_3=g(a_2)a3​=g(a2​)又是两个相同的V形函数，最终a3=h(x)a_3=h(x)a3​=h(x)就是一个232^323条线段组成的折线，共使用3层网络，6个神经元。 对于2k2^k2k条线段组成的折线，如果用一层隐藏层（激活函数同样使用ReLU）构成，那么需要2k2^k2k个神经元，但是如果用多层具有两个神经元的网络构成，只需要kkk层网络，共2k2k2k个神经元就足够了。 这个例子表示在构成复杂但是有一定规律的函数上，Deep Learning 相比“Fat Learning”是具有优势的。对于图像、语音、文字等任务，由于图像、语音、文字有一些内在特性、规律，需要的函数也可能会有一定规律（比如图像分类模型需要模型能提取特征），所以用Deep Learning方法训练的模型会有比较好的效果。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/"},{"title":"李宏毅机器学习-hw3-CNN-总结","text":"任务介绍 训练结果 训练方法 Data augmentation 模型设计 Loss function的选择与调参 Learning rate调整方案 Test time augmentation 进一步提升的可能 数据增强可能可以尝试mix up 模型设计可能可以继续尝试 Learning rate调整方案有待提升 Cross Validation + Ensemble 其它值得记录的东西 autodl tensorboard使用 layer的梯度变化 任务介绍# 这个作业要求训练一个能对11种食物进行分类的CNN。使用的数据集为food-11，数据集划分如下： 训练集：9866张带有label的图像 验证集：3430张带有label的图像 测试集：3347张不带有label的图像 完成训练后，将用测试集进行测试，输出含有3347张图片的预测label的csv文件，上传至kaggle后系统根据预测准确率自动评分。 Baseline如下： Simple : 0.50099 Medium : 0.73207 Strong : 0.81872 Boss : 0.88446 训练结果# Times Private Score Public Score Improvement（相比上一步） 1 0.57063 0.55278 直接运行初始代码 2 0.71830 0.75298 数据增强、调整模型、调整loss function 3 0.75757 0.78884 调整数据增强、手搓ResNet、loss function调参 4 0.77592 0.80378 减少ResNet层数，调整learning rate 5 0.77251 0.80677 减少ResNet层数 6 0.82415 0.85358 增加ResNet层数 7 0.84208 0.87450 使用tta 训练方法# Data augmentation# 在观察了部分数据之后，发现合理的图片增强方式包括但不限于 缩放裁剪 随机翻转 随机旋转 仿射变换 随机灰度化 最终代码如下所示： 123456789101112train_tfm = transforms.Compose([ # Resize the image into a fixed shape (height = width = 128) transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)), transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转 transforms.RandomVerticalFlip(p=0.5), # 随机上下翻转 transforms.RandomRotation(degrees=(0, 180)), # 图像随机旋转 transforms.RandomAffine(30), transforms.RandomGrayscale(0.2), # 随机灰度化 # You may add some transforms here. # ToTensor() should be the last one of the transforms. transforms.ToTensor(),]) 模型设计# 模型使用的是ResNet，ResNet的搭建见：记手搓ResNet的经历 · 核子的Blog (hezj-opt.github.io) 在调整参数方面，最终调整预卷积层的通道数为32，全连接层的dropout为0.4，残差块采用两个3x3卷积层和shortcut path构成的残差块，最终使用通道数为64、128、256、512的残差块数量分别为1、2、1、1 1model = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 2, 1, 1], 11).to(device) 在第4次训练中，使用过层数较少的网络（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 2, 1], 11).to(device)），但是效果不佳，当时以为是over fitting，所以在第5次训练中把网络改得更简单了（model = resnet(resblock_basic, 3, [64, 128, 256], [2, 1, 1], 11).to(device)），修改后发现效果不仅没有显著提升，而且训练时长增加了非常多（第4次训练大致12小时，第5次训练花了将近16小时）。所以发现是模型欠拟合，决定增加模型复杂度，才在第6次训练后通过strong baseline。 鉴于几次调参后的效果，推断三次训练在下图中位置红色线框覆盖的区间内。 Loss function的选择与调参# 查阅资料发现，使用Focal loss会比Cross entropy有更快的收敛速度（参数γ导致），而且更适合各个标签的数据集不均的情况（参数α导致）。 下图显示了由于Focal loss的γ参数，Focal loss会收敛得更快 Focal loss中的α参数可以平衡训练集中各类图片数量的差异，比如在本次任务中，我先统计训练集中了11类食物的数量，然后对样本少的赋比较高的α值，样本多的食物赋比较低的α值。 123count = count / np.max(count) # count 为11类食物样本数的统计结果alpha = 1 / count# alpha = [1, 2.317, 0.663, 1.008, 1.172, 0.750, 2.259, 3.55, 1.163, 0.663, 1.402] Learning rate调整方案# 优化器选择Adam，初始学习率为0.0004，设置decay=1e-5 在学习率调整上，通过余弦退火调整学习率，退火周期为16，即每过16轮，学习率突然增大，然后再慢慢减小。如下图除了蓝线、红线以外的曲线所示。 这么做好处有二，一是可以加快收敛速度，二则是可以帮助“翻越” loss surface上的一些小山峰，从而更好地跳出“局部最小”（其实往往不是局部最小，但是有“跳出”的作用）找到全局最优解。 Test time augmentation# 训练时，我们对训练数据进行了增强，但是测试时用的就是正常的图片，但是我们想利用模型对增强的图片的识别效果辅助判断。所以可以让测试图片产生若干张增强的图片，依次求未增强图片和增强图片的预测向量，然后把向量相加后再求出最大值所在位置，就是最终预测结果。模型对同一个物体的原图和多张增强图像进行预测，一定程度上可以校正只对原图进行预测时的错误。 比如在本次hw中，我对每张测试图片产生五张增强图片，对六张图片进行预测，得到 preds列表，内含未增强图像的预测结果（preds[0]）和五张增强图像的预测结果。最终的预测结果为 12preds = 0.5* preds[0] + 0.1 * preds[1] + 0.1 * preds[2] + 0.1 * preds[3] + 0.1 * preds[4] + 0.1 * preds[5]prediction = np.argmax(preds, axis=1) 进一步提升的可能# 数据增强可能可以尝试mix up# mix up数据增强是一种进阶的数据增强，它可以使得模型在判断时不会那么绝对，可以减小过拟合。如果要实现mix up，要改写Dataset类，并且还需要写适用于mix up的loss function。 模型设计可能可以继续尝试# 过了strong baseline以后我没有继续调整模型，表格中最后一次产生的结果实际上用的是第6次训练得到的模型，只是在测试时采取tta。所以可能可以继续尝试加深模型，直至出现过拟合，再适当减小模型。 Learning rate调整方案有待提升# 我的实现中退火周期时固定的，但是看了原始论文后，发现设置合适的初始退火周期，然后在每个退火周期后，将退火周期增大一倍可能是个不错的策略。 Cross Validation + Ensemble# 简单地来说，Cross Validation + Ensemble是先把训练集和验证集重新进行划分，进行k种划分，训练出k个模型，最终用k个模型对图片进行多次预测。效仿tta的方式融合产生预测结果，此时不同的模型之间可以互相弥补缺陷，所以可以得到比较好的预测结果。 但是这玩意真的耗时间，第6次训练花费时间为8-9h，如果训练四个模型得35h左右…… 其它值得记录的东西# autodl tensorboard使用# 本次作业中我学习了autodl自带的tensorboard，发现操作十分简单。 开机后打开AutoPanel 用pip下载tensorboard 1$ pip install tensorboard 新建一个writer 12from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter('/root/tf-logs') 把想要看的东西放进tensorboard 123456789101112131415# loss 曲线writer.add_scalars(main_tag='Loss', tag_scalar_dict={'train': train_loss, 'valid': valid_loss}, global_step=epoch + 1)# Accuarcy曲线writer.add_scalars(main_tag='Accuracy', tag_scalar_dict={'train': float(train_acc), 'valid': float(valid_acc)}, global_step=epoch + 1)# 各层的参数的梯度绝对值曲线for name, parms in model.named_parameters(): writer.add_scalar(f\"Grad/{name}\", torch.norm(parms.grad), epoch + 1) layer的梯度变化# 下面两张图时预卷积层的weight的梯度模长变化和某个残差块中的一个卷积层的weight的梯度模长变化（经过平滑后的结果）。所以说可以发现哪怕训练到几乎停滞，其实并不是陷入局部最小值，甚至不在鞍点。所以之前推测学习率调整方案还可以进一步优化。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/"},{"title":"李宏毅机器学习-lect4 Self attention","text":"Self attention 解决什么问题 FC(Fully Connected)有什么不足 Self attention 的架构 Self attention 的基本计算过程 求attention score 求解Self attention层输出的向量 矩阵视角下的计算过程 Self attention的优化 Multi-head Self attention Positional Encoding Self attention 和其他网络对比 CNN RNN GNN Self attention 解决什么问题# 用一句话概括就是：Self attention用来解决当输入为向量的序列时的问题（像音频、文本都是经典的输入为向量序列的数据） Self attention的输出一般有如下三种类型 N个vector产生N个label ​ 例如输入一个句子，输出每个词的词性 N个vector产生1个label ​ 例如输入一个句子，判断这句话蕴含的情绪为positive or negative N个vector产生N’（N≠N’）个label ​ 例如机器翻译，输入的句子和输出的句子词数很可能不一样 FC(Fully Connected)有什么不足# FC也可以用来解决输出为向量序列的问题，如天气预测等等，但是它相比self attention有一些不足之处。 FC如果要充分考虑“上下文”——一个向量和它相邻的很多个向量，甚至可能整个序列一起考虑，就需要把考虑的向量串联起来，通过fully connect产生新的向量，那么参数的矩阵可能会非常大（这个在后面会解释），这可能导致很大的运算量和overfitting。 Self attention 的架构# 首先我们来看一个self attention层要做什么。 概况一下就是，一个self attention层要先算出每个向量和其他向量的关联性，这个关联性用attention score α\\alphaα 表示。然后再根据加权算出输出向量序列，值得一提的是，计算输出向量序列中的每个向量是并行的。 然后我们再看整个网络架构，经过self attention层后，产生了输出向量序列，这时候可以先对每一个向量进行fully connect，产生输入到下一个self attention层中的输入向量，然后就是重复，直至产生最终输出。 Self attention 的基本计算过程# 求attention score# 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求a1\\mathbf{a^1}a1与自身及其他向量的attention score α1,i′\\alpha'_{1,i}α1,i′​为例，来说明求解流程 对a1\\mathbf{a^1}a1求q1\\mathbf{q^1}q1，q1=Wq⋅a1\\mathbf{q^1}=\\mathbf{W}^q\\cdot\\mathbf{a^1}q1=Wq⋅a1，其中Wq\\mathbf{W}^qWq为参数矩阵，是要学习的参数 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…，ki=Wk⋅ai\\mathbf{k^i}=\\mathbf{W}^k\\cdot\\mathbf{a^i}ki=Wk⋅ai，其中Wk\\mathbf{W}^kWk为参数矩阵，是要学习的参数 求q1\\mathbf{q^1}q1和k1,k2,…\\mathbf{k^1},\\mathbf{k^2},\\dotsk1,k2,…的关联性，常见方法是向量点乘，如α1,2=(q1)T⋅k2\\alpha_{1,2}=(\\mathbf{q^1})^\\mathrm{T}\\cdot\\mathbf{k^2}α1,2​=(q1)T⋅k2 对上一步求得的α1,1,α1,2,…\\alpha_{1,1},\\alpha_{1,2},\\dotsα1,1​,α1,2​,…通过激活函数，常用softmax，最后得到attention score α1,1′,α1,2′,…\\alpha'_{1,1},\\alpha'_{1,2},\\dotsα1,1′​,α1,2′​,… 可以参照下图直观地理解上述步骤 求解Self attention层输出的向量# 对于输入向量序列a1,a2,…\\mathbf{a^1},\\mathbf{a^2},\\dotsa1,a2,…，我们以求输出向量序列中的b1\\mathbf{b^1}b1为例，来说明求解流程 对a1,a2…\\mathbf{a^1},\\mathbf{a^2}\\dotsa1,a2… 求v1,v2,…\\mathbf{v^1},\\mathbf{v^2},\\dotsv1,v2,…，vi=Wv⋅ai\\mathbf{v^i}=\\mathbf{W}^v\\cdot\\mathbf{a^i}vi=Wv⋅ai，其中Wv\\mathbf{W}^vWv为参数矩阵，是要学习的参数 利用求得的attention score对vi\\mathbf{v^i}vi进行加权：b1=∑α1,i′v1\\mathbf{b^1}=\\sum \\alpha'_{1,i}\\mathbf{v^1}b1=∑α1,i′​v1，从而得到b1\\mathbf{b^1}b1 可以参照下图直观地理解上述步骤 矩阵视角下的计算过程# 下面三张图非常直观地展示了 产生qi,ki,vi\\mathbf{q^i},\\mathbf{k^i},\\mathbf{v^i}qi,ki,vi 计算attention score 计算输出向量bi\\mathbf{b^i}bi 上面提到的矩阵视角下的计算过程可以归结为下图，我们也可以发现，对于一层self attention层而言，需要学习的参数只有Wq,Wk,Wv\\mathbf{W}^q,\\mathbf{W}^k,\\mathbf{W}^vWq,Wk,Wv 前面说到FC可能在考虑整个向量序列的情况下有大量的参数，比如说输入的向量序列为10000个100x1的向量，如果在考虑整个向量序列的情况下要产生10000个10x1的向量，用FC需要的参数数量级为(10000x100)x(10000x10)，而如果用self attention，那么Wq\\mathbf{W^q}Wq的元素个数数量级为100x100。 Self attention的优化# Multi-head Self attention# 考虑到在不同的视角下，一个向量和其他向量会有不同的关联性，所以我们可以用不同系数矩阵，最终产生多组向量序列输出。 我们也可以对多组向量序列输出进行拼接、变换，得到一组输出向量序列 Positional Encoding# 在之前的产生输出向量序列的操作中，我们可能忽略了输入向量序列在坐标或者时间尺度上的相关性，而这有时是很重要的，比如在音频识别中，识别一个音符考虑的就是在一小段时间内的信号，那么这一小段时间内向量的相关性就极强。 所以我们可以通过在输入向量序列上加一个用来代表位置的向量，来为输入向量增加坐标/时间信息。这个代表位置的向量可以是直接计算得到的，也可以是通过神经网络学习得到的。 Self attention 和其他网络对比# CNN# 其实图像也可以看作是向量的序列——比如一张100x100x3的图片，可以看作是100x100个向量（一个像素的RGB三通道值作为向量）。 通过设置如果attention score合适，是不是也可以表现出CNN的效果？而且self attention可以打破CNN中由于卷积核带来的像素只与周边像素作用的限制，让一个像素可以和离它较远的像素作用。 所以，self attention是更灵活的CNN，而CNN是简化的self attention 在二者训练效果对比中也可以发现，对于小的数据集，自由度比较小的CNN表现得更好，对于大数据集而言，self attention表现的效果更好。 RNN# 相比RNN而言，self attention由于并行运算可以获得更快的运算速度，同时相比RNN不易考虑到距离一个向量在坐标尺度下比较远的向量，self attention可以通过attention score充分考虑到一个向量在坐标尺度下距离比较远的向量。 GNN# self attention可以看作是一种特殊的GNN，如果两个点直接由连线，那么attention可以设为1，否则直接设为0。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/"},{"title":"李宏毅机器学习-lect6 GAN","text":"GAN的基本思想 生成器 “对抗”的含义 训练过程概述 GAN的理论 目标函数 JS divergence的缺陷 Wasserstein distance 评价生成器 评价指标 图像质量 图像多样性 评价函数 Inception score FID Conditional GAN（条件式生成） Conditional-GAN+监督学习 Cycle-GAN 为什么需要Cycle-GAN Cycle-GAN的结构 Cycle-GAN的其他应用 GAN的基本思想# 生成器# GAN的中文名叫做对抗生成网络，生成网络的一大特点是让AI具有创造力，所以和CNN、Transformer的一大不同点就是对于一个输入而言，由于生成器同时接受一个随机变量，所以输出并不是固定的。如下图所示，生成器接受x和随机变量z作为输入，其中随机变量z需要来自一个简单到我们可以写出其表达式的随机分布（这样我们才能从中采样），然后输出y，y是一个复杂的随机分布。 那么为什么输出需要是随机分布，而不是像CNN一样固定的值呢？最重要的原因是，在很多问题中答案不一定是唯一的，如果使用监督学习，可能有两大坏处： 给出的结果可能是模糊、不合实际的 比如在某个小游戏的视频预测任务中，AI需要根据之前的帧产生接下来的动画，如果使用监督学习，会发现在路口处小人可能会分裂成两个，原因是训练资料内在入口处小人可能向左转或向右转，这两种情况本来都是合理的，但是如果使用监督学习，AI在训练时不能明确哪种是正确的答案，那么为了使得loss最小，AI就会给出模棱两可的答案——分裂，而这可能是不合实际的。 AI缺乏“创造力” AI在某些特定的人物需要一定的“创造力”，比如让AI绘画，要求画出有红眼睛的人物，答案显然不是唯一的；又如让AI回答“你知道辉夜是谁吗？”，也有不止一种回答。 “对抗”的含义# 为什么起名叫对抗生成网络，原因来自生成器的训练是伴随着它的“对手”——鉴别器的训练进行的。在生成器的训练中，我们会同时训练一个鉴别器，用来判定生成器的表现。 以生成二次元人物头像为例：生成器要生成二次元人物头像，而鉴别器要鉴别生成的图像像不像人工绘制的二次元人物头像，此时为了“骗过”鉴别器，生成器要让自己生成的图像更像真人绘制的二次元人物头像，而鉴别器为了更好地把生成器生成的二次元头像检查出来，也要提升自己的鉴别水平，最终二者相互促进，使得生成器生成非常逼真的二次元人物头像。 训练过程概述# GAN的训练过程，大致可以概述为如下步骤： 固定生成器参数，训练鉴别器 因为一开始未经训练的鉴别器是毫无鉴别功能的，所以我们应当先训练鉴别器。训练时同时给鉴别器喂入真实样本（标记为1）和生成器随机生成样本（标记为0）。 固定鉴别器参数，训练生成器 在鉴别器有一定鉴别功能、能给生成器的表现打一个相对合理的分数后，就可以开始训练生成器了。此时将生成器和鉴别器拼接成一个大网络，固定鉴别器参数，更新生成器参数，使得最终输出分数能提高。 重复前两个步骤 通过重复，最终可以获得良好的生成器。下图概述了上面两个步骤 GAN的理论# 目标函数# 让我们先考虑生成器的优化目标，以图像生成任务为例，对于生成器而言，目标就是让产生的图片的分布和真实图片的分布是类似的。数学表达式可以写成 G∗=arg⁡min⁡GDiv(PG,Pdata) G^{*}=\\arg \\underset{G}{\\min} Div(P_G, P_{data}) G∗=argGmin​Div(PG​,Pdata​)其中Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)表示产生图片的分布和真实图片分布的距离。 然后再考虑鉴别器的优化目标，对于鉴别器而言，目标是能鉴别出生成的图片。那么在鉴别器优化后，由于鉴别器能力的提升，算出的Div(PG,Pdata)Div(P_G,P_{data})Div(PG​,Pdata​)会变大。我们就可以把目标写成 D∗=arg⁡max⁡DV(D,G) D^{*}=\\arg \\underset{D}{\\max} V(D, G) D∗=argDmax​V(D,G)其中V(D,G)V(D, G)V(D,G)的值和https://arxiv.org/abs/1406.2661里提到的JS divergence在数学形式上是相近的，可以写成 V(G,D)=Ey∼Pdata [log⁡D(y)]+Ez∼Pz(z)[log⁡(1−D(G(z)))] V(G, D)=\\mathbb{E}_{y \\sim P_{\\text {data }}}[\\log D(y)]+\\mathbb{E}_{z \\sim P_z{(z)}}[\\log (1-D(G(z)))] V(G,D)=Ey∼Pdata ​​[logD(y)]+Ez∼Pz​(z)​[log(1−D(G(z)))]上式中第一项代表鉴别器对真实图片的输出，第二项代表鉴别器对生成图片的输出，V(G,D)V(G,D)V(G,D)的形式是很像Cross Entropy的。在优化时，第一项只与鉴别器相关，第二项与鉴别器和生成器都相关。那么最终的优化目标表达式可以写作 min⁡Gmax⁡DEx∼pdata (x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))] \\min _{G} \\max _{D} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] Gmin​Dmax​Ex∼pdata ​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))]这是一个min-max模型，代表着鉴别器和生成器训练时的“对抗”。 JS divergence的缺陷# 刚刚我们提到了用JS divergence来衡量生成图片和真实图片之间的差异，但是JS divergence存在比较严重的问题。 生成图片分布PGP_{G}PG​和真实图片分布经PGP_{G}PG​常重叠率是很低以至于可以忽略的，而且就算真的有一部分重叠了，也可能因为采样不足而在采样分布上没有重叠。 而在PGP_{G}PG​和PGP_{G}PG​重叠率为0时，JS divergence的值是恒定的（为log2），此时就无法衡量PGP_{G}PG​和PGP_{G}PG​的真实差距 Wasserstein distance# 在https://arxiv.org/abs/1701.07875中，作者提出了WGAN所用到的Wasserstein distance。Wasserstein distance衡量的是将PGP_{G}PG​“移动”到PGP_{G}PG​的最小平均距离。表达式可以写成 min⁡Gmax⁡D∈1−LipschitzEx∼pdata (x)[D(x)]−Ez∼pz(z)[D(G(z))] \\min _{G} \\max _{D\\in{1-Lipschitz}} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[ D(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[D(G(\\boldsymbol{z}))] Gmin​D∈1−Lipschitzmax​Ex∼pdata ​(x)​[D(x)]−Ez∼pz​(z)​[D(G(z))]此时鉴别器有约束条件，这个条件要求鉴别器需要足够平滑，如果没有这个条件，鉴别器的训练会不收敛。 WGAN依然有缺点，有几种提升方案 参数限制（Force the parameters w between c and -c） WGAN-GP（gradient penalty）https://arxiv.org/abs/1704.00028 SNGAN（Spectral Normalization → Keep gradient norm smaller than 1 everywhere）https://arxiv.org/abs/1802.05957 评价生成器# 评价指标# 图像质量# 为了评估生成的图像与真实图像是否相似，可以用一个CNN来对生成的图片进行分类，如果分类结果是明确的，即有一类的值很高，那么说明生成的图像质量很好。 图像多样性# 除了评估图像质量以外，还需要评估图像多样性，因为哪怕生成的图像质量很好，如果遇到了mode collapse，那么可能生成的图像都十分相似，这显然不是我们想要的。 除了mode collapse以外，还可能遇到mode dropping的问题，就是生成的图片看上去多样性还行，但是实际上只是真实分布中的一部分而已。 评价函数# Inception score# Inception score借助CNN，将所有生成图片经过CNN后的输出取平均，如果各类的值比较平均，那么说明多样性是足够的，此时Inception score较高。同时Inception score也会考虑单张图片经过CNN后的分布，如果分布集中，说明图像质量高，此时Inception score较高。 总的来说，Inception score综合考虑图像质量和图像多样性，图像质量和多样性越高，则Inception score越高。 FID# FID衡量的是生成图片分布和真实图片分布的距离。同样借助CNN，将生成图片和真实图片经过CNN，不同的是此时收集的是softmax前的输出，最后得到两个分布，将两个分布视为高斯分布，然后计算两个分布间的Frechet distance。显然FID越小代表两个分布约接近，说明生成器性能更佳。 Conditional GAN（条件式生成）# 很多时候，我们希望GAN能生成符合一定条件的结果，比如AI绘画里我们会根据自己的喜好提出一些要求。所以就需要Conditional GAN。 训练Conditional GAN的时候，以生成图片任务为例：对于生成器而言，我们以条件作为x，伴随着随机变量z输入；对于鉴别器而言，应当同时输入图片和条件，以及一个标量（这个标量仅当图片为真实图片，且与要求对应时才是1，否则都是0），鉴别器此时输出的标量一方面衡量图片是否是真实图片，另一方面还要衡量图片和条件是否对应。 Conditional-GAN+监督学习# 但是有时AI会“过于具有想象力”，以至于超出条件的约束，比如在通过建筑结构图生成建筑照片的时候，只用GAN的结果相比结构图多了一个类似烟囱的结构。为了克服这个问题，有时会结合GAN和监督学习，将AI的“想象力”控制在合理的范围内。 比如在[1611.07004] Image-to-Image Translation with Conditional Adversarial Networks (arxiv.org)内，在目标函数中包含了L1范数LL1(G)=Ex,y,z[∥y−G(x,z)∥1]\\mathcal{L}_{L 1}(G)=\\mathbb{E}_{x, y, z}\\left[\\|y-G(x, z)\\|_{1}\\right]LL1​(G)=Ex,y,z​[∥y−G(x,z)∥1​]，最终优化目标为 G∗=arg⁡min⁡Gmax⁡DLcGAN(G,D)+λLL1(G) G^{*}=\\arg \\underset{G}{\\min} \\underset{D}{\\max} \\mathcal{L}_{c G A N}(G, D)+\\lambda \\mathcal{L}_{L 1}(G) G∗=argGmin​Dmax​LcGAN​(G,D)+λLL1​(G) Cycle-GAN# 在很多时候，我们会遇到没有成对的输出和输出的数据集，比如想要做真人头像和二次元人物头像的风格转换，但是很多真人头像都没有对应的二次元人物头像。 为什么需要Cycle-GAN# 以图片风格转换为例，如果使用一般的GAN，那么可能导致生成的图片风格正确，但是和输入的图像没什么关系，比如把李宏毅老师的头像输入，可能产生的是辉夜的头像。那么如果用Conditional-GAN做风格转换，往往又需要成对的训练资料。 Cycle-GAN的结构# 以风格转换为例，为了保证风格转换后的图片和原始图片相关，会再加入一个用于将风格转换后的图片转换回原始风格的生成器，那么为了要将风格转换后的图片能转换回原始风格，就会要求生成的图片和原始图片是相关的。相应的，我们需要一个鉴别器来判断转换回原始图片的效果。所以如下图所示Cycle-GAN包含了将两种风格的图片互相转换的生成器和对应的鉴别器。 Cycle-GAN的其他应用#","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/"},{"title":"李宏毅机器学习-lect3 CNN","text":"CNN 的想法 卷积层（Convolutional layers） 卷积核 步长 （Stride） 填充 （Padding） 参数共享（Parameter Sharing） 卷积层的运行过程 卷积层的两种理解方式 池化层（Pooling layers） Flatten layers CNN 的缺点 CNN 的想法# 对于图片分类的任务，我们可以采用让特征识别（这与人类分类物体的方法是类似的）的方法，让每一个神经元只与部分区域（Receptive field）关联，而不需要每一层都full connect。 所以，CNN是为了影像的特性而生的，把CNN用于影像领域外的任务要仔细思考是否出现影像类似的特性。 卷积层（Convolutional layers）# 卷积核# 卷积核大小表示局部特征区域（即Receptive field）的大小 对于输入图像，卷积核一般大小取3x3，同时包含RGB三个通道，所以参数数是3x3x3。对于channel数为k的feature map而言，一个3x3大小卷积核含有3x3xk个参数。 Q：卷积核大小只有3x3，如果图片尺寸比较大，3x3会不会无法识别特征？ 这个问题将在下面回答 步长 （Stride）# 每次卷积核移动的长度，一般设为1或2，因为我们希望Receptive field之间是有重叠的，因为如果Receptive field之间完全没有重叠，那么如果pattern出现在两个Receptive field的交界上，特征就难以被识别。 填充 （Padding）# 移动卷积核时，如果步长大于1，则移动时卷积核可能会超出图片范围，则需要在边上填充一些值，常见的方法有补0法、取平均法等。 参数共享（Parameter Sharing）# 对于一个特征，可能出现在图片不同的位置，而对于一个区域，有一组神经元负责，每个神经元负责识别不同的特征，所以此时可以让负责不同区域，但功能相同的神经元享有相同的参数，从而减少参数数量。 对于每一个区域，有一组神经元负责，每个神经元有一组参数，这一组组参数叫做filter，所有区域共享一组filter 卷积层的运行过程# 之前提到，图片中每一个小区域有一组神经元负责，每一个神经元的参数叫做filter，所有小区域共享一组filter，那么卷积层可以看作每一个filter对图像分别作用，得到一组图像，所有的filter对图像作用后，得到了新的图像，图像的channel数则为filter的数量。这样的一张图片叫做特征图像（feature map）。 所以一张图像经过卷积层后，会得到一张特征图像。之前有提到，卷积核大小只有3x3时会不会无法识别较大特征，这是不会的，因为在下一个卷积层中对特征图像做卷积时，若卷积核为3x3，步长为1时，则相当于对5x5大小区域卷积。 在已知输入图像大小（长宽相同）、卷积核大小（长宽相同）、步长、填充后，我们可以得到feature map的大小的计算公式为No=(Ni−k+2p)s+1N_o=\\frac{(N_i-k+2p)}{s}+1No​=s(Ni​−k+2p)​+1，其中NiN_iNi​为输入feature map大小，kkk为kernel size，ppp为padding，sss为步长。 一个卷积层的参数数目计算公式为N=k2⋅ci⋅coN=k^2\\cdot c_i\\cdot c_oN=k2⋅ci​⋅co​，其中kkk是kernel size，cic_ici​是输入feature map的channel数，coc_oco​是输出feature map的channel数。 卷积层的两种理解方式# 池化层（Pooling layers）# 对于一张较大的图片而言，采样时少采样一些点并不会影响图像是什么 池化层并没有参数，其操作时固定的，相当于一个算符 常用的池化方法有Max Pooling，过程如下图所示，一般而言，池化时分组大小为2x2 一般而言，池化常常在卷积层后使用，如一个或两个卷积层后跟一个池化层，用于缩小图片，从而减小运算量。但是这对于网络的效果而言可能是由损害的，因为如果特征特别细小，则池化可能会漏过特征。 Flatten layers# 图像经过一系列卷积、池化后，得到小的特征图像，此时这个特征图像代表图片中较大的、全局的特征，此时就可以把图像展平，然后通过全连接层，经过softmax归一化后，得出分类的结果。 其实到这里，和回归问题里用到的神经网络是类似的，只不过回归问题的特征是显式的，所以一开始就可能是全连接层，而影像类任务中，一开始需要先提取特征，最后再让特征经过全连接层计算。 CNN 的缺点# CNN很难处理图片的缩放、旋转，所以我们需要数据增强（data augmentation）","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/"},{"title":"李宏毅机器学习-lect5 Transformer","text":"Transformer是什么 Transformer的应用 Transformer的架构 Encoder Decoder-Autoregressive (AT) Masked Self-attention Cross attention Non autoregressive Transformer(NAT) Transformer的训练 loss的来源 训练的一些tips Teacher Forcing Copy Mechanism Guided Attention Beam Search Exposure bias Transformer是什么# Transformer可以理解为是Sequence-to-sequence（简称Seq2seq）的模型，它接受向量序列作为输入，输出向量序列。 Transformer的应用# Transformer的应用除了上图提到的语音识别、机器翻译、语音翻译以外，还有 聊天机器人 句子词性分析 多标签分类 物体检测 Transformer的架构# Transformer由编码器（Encoder）和解码器（Decoder）组成，前向传播的过程是Encoder将输入向量序列编码产生新的向量序列，然后Decoder将编码的向量结合begin向量（标记着位置，是一个one hot向量）产生第一个输出向量，然后把产生的第一个向量再输入Decoder，产生第二个向量，直到产生end向量为止，代表着输出完成。 如下图所示，是\"Attention is all you need\"文中提出的Transformer的内部结构，左半边是Encoder，右半边是Decoder。接下来具体解释Encoder和Decoder的组成。 Encoder# Encoder由一系列block组成，每一个block里面都包含了self-attention层和FC层。 实际上，Transformer里融合了ResNet的思想，再self-attention产生了输出向量序列后，还会加上输入的向量，然后在一起进行layer normalization，就是对每一个向量，减掉其平均值后再除以标准差。进行了layer normalization后的向量才会被输入FC。 如下图所示，FC的部分也有residual的部分，经过了FC、layer normalization后，才得到一个block的输出。 此时我们回顾Encoder的架构，首先对于Encoder中的一个block，要做的事情有 Positional Encoding（位置编码，lect4的笔记里有提到） 结合ResNet性质的Self-attention+layer normalization 结合ResNet性质的FC+layer normalization Decoder-Autoregressive (AT)# Autoregressive transformer的前向传播过程大致是结合Encoder的输出和Begin向量得到第一个输出向量，以语言识别为例，然后取输出向量中对应概率最大的字对应的one hot向量作为第一位输出的结果，然后再用第一位输出的结果输入Decoder，产生下一个输出，直至产生End向量，才代表输出结束。 下图是Decoder的内部架构，Positional encoding和FC两个部分和Encoder是差不多的，所以下面重点分析Masked Attention部分和Masked Attention后的Attention block两个部分。 Masked Self-attention# Masked Self-attention和一般的Self-attention不同之处在于：产生第一个输出向量时，只能考虑第一个输入向量，产生第二个输入向量时，只能考虑第一、第二个输入向量，以此类推。下面两张图很好地解释了这个机制 Cross attention# 这部分是Decoder内部架构图中第二个attention的模块，下图很好地说明了cross attention地机制。BEGIN向量和“机”向量进行Masked Self-attention，产生q向量，然后再同Encoder产生的输出得到attention score，再加权，通过FC layer得到第二个输出向量。 最重要的一点就是这个过程中的q向量不是来自于Encoder的输出，而是来自Mask Self-attention的输出。 Non autoregressive Transformer(NAT)# 和autoregressive transformer（AT）不同，NAT一次性产生所有的输出向量，然后截取END向量之前的向量作为最后的输出序列。 相比AT，它的优势在于的产生速度快，并且可以控制输出长度，但是它的效果往往不如AT。 Transformer的训练# loss的来源# loss的来源可以是每个输出的向量与正确向量的交叉熵，在训练时我们希望交叉熵最小化。 训练的一些tips# Teacher Forcing# 为了防止训练时出现因第一个向量输出错误导致接下来的训练错误这样“一步错，步步错”的情况，所以我们在训练时把正确答案输入给decoder。 Copy Mechanism# 有时适当地从输入中进行一些copy，可以更好地完成任务。比如在聊天机器人训练中，可以对人名进行copy，因为一个人名在训练资料中出现的次数可能很少，网络无法通过学习习得输出正确人名的能力，所以对人名进行直接copy。 Guided Attention# Guided Attention，可以理解为用先验知识来限制self attention层的注意力机制，比如在语音识别中，一个音节的识别只与对应时刻的向量及其周边向量有关。所以应该把attention限制在这个时刻附近较小的范围内。 Beam Search# 可以把找输出向量序列看作是树形搜索，所以就把问题看作是设计合适的搜索算法，使得全局的loss最小。 Exposure bias# 训练时我们把正确答案直接喂给decoder，但是如果在测试的时候产生了错误的向量，可能就会产生mismatch，从而影响后面的输出。所以我们可以在训练时就适当地喂入错误的向量，使得模型的适应能力更强。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/"},{"title":"李宏毅机器学习-lect7 Self-Supervised Learning","text":"自监督学习 BERT 什么是BERT BERT的训练方法 Masking Input Next Sentence Prediction 怎么使用BERT 文章情感判断 句子词性标记 逻辑判断 摘录型问答 BERT的评估 为什么BERT有效 解释与佐证 对之前解释的质疑 Multi-lingual BERT GPT GPT的训练 GPT的使用 自监督学习在语音和图像领域上的应用 概述 Generative Approaches 语音 视觉 Predictive Approaches Contrastive Learning SimCLR Bootstrapping Approaches Simply Extra Regularization VICReg 自监督学习# 在监督学习中，我们用成对的训练资料进行训练（比如中译英任务中中文文章与其对应的英文翻译），而在无监督学习当中，是没有成对的训练资料的（比如不带英文翻译的中文文章）。 自监督学习是无监督学习的一种，他将训练资料x的分为x’和x’‘，用x’进行训练，然后用x’'进行验证。 自监督学习现在被广泛用于大模型的预训练，比如BERT和GPT。 BERT# 什么是BERT# BERT是芝麻街里的人物一个巨大的预训练语言表征模型，它的全称是Bidirectional Encoder Representations from Transformers。它强调了不再像以往（Transformer）一样采用传统的单向（每一个token的注意力只能在之前的token上）语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，BERT自监督训练时每一个token的注意力可以放到全局。 BERT在预训练后，只需要接上一个额外的输出层，给予一点带标注的资料微调，就可以在翻译、问答等下游语言任务中有优秀的表现。 从BERT开始，一大批巨大模型开始出现，如GPT、T5等。 BERT的训练方法# Masking Input# 用一句话概括Masking Input就是让BERT学会做“填空题”，这里的填空题是指在把输入的一些token换成特定的token或者随机的token，从而使完整的句子中出现空位，BERT需要学会预测这些被替换的token是什么，训练时目标是最小化输出token和正确token的交叉熵。 Next Sentence Prediction# 这个方法是输入两个句子，判断sentence 1与sentence 2在一篇文章中的前后关系。 怎么使用BERT# 经过预训练后的BERT学会了做“填空题”、“排序题”，此时的BERT像干细胞，没有具体的功能，但有分化为执行具体任务的能力。 要让BERT能做具体的任务，我们需要在BERT后加入一个输出层（BERT其实和Transformer的Encoder类似，所以后面需要输出层），然后再给予一点具体任务的成对资料进行训练（这个过程称为Fine-tune，即微调），就可以让BERT做具体的任务。微调前，输出层的参数是随机的，而BERT的参数是经过预训练的。 下面举四个例子具体说明如何使用BERT 文章情感判断# 预训练的BERT接受句子和CLS token作为输入，在预训练的BERT后接一个参数随机初始化的线性输出层，线性输出层将BERT输出的其中一个向量作为输入，然后用（文章，情感）这样成对的资料微调模型，就可以得到良好的效果。 句子词性标记# 和刚刚不同的是，此时需要输出的长度和文章长度是一致的，需要把句子中每个词在BERT中的输出都接入输出层，同样给予一点成对资料训练即可。 逻辑判断# 逻辑判断是指输入两个句子，判断两个句子是否有逻辑关系。此时只需要在两个句子之间加入一个SEP token即可。 摘录型问答# 摘录型问答是指让模型从文章中摘录一部分作为答案（这种问题保证答案在文中可以摘录），输出层需要输出两个token，代表答案在文章中的始末位置。 BERT接受输入的方式和逻辑判断中相似，只是两个句子换成了问题+文章，此时需要两个向量（长度和BERT输出的向量是相同的，参数是随机的），向量和文章经过BERT后产生的各个向量作内积，产生每个位置作为答案开始或结束时的概率，然后分别选择概率最高的位置作为答案的开始/结束。 BERT的评估# BERT常用GLUE评估，GLUE里面有九项语言相关任务，通过BERT在九项任务上的性能综合评估BERT。 https://arxiv.org/abs/1905.00537中总结：随着时间推进，BERT的表现越来越好，甚至在多项任务上都能超过人的表现（水平直线）。 为什么BERT有效# 解释与佐证# 最常见的一种解释是BERT通过做“填空题”或其他预训练任务，真的“学会了”语言，能根据上下文将文字映射到语义空间上，相近意思的词在语义空间上的映射会比较接近。比如同样是“苹”这个字，吃苹果中的“苹”和“果”、“草”的映射比较接近，苹果手机中的“苹”可能和“电”的映射比较接近。所以在BERT对每个字做了有效的映射后，输出层可以很容易地输出正确结果。 取BERT对不同语境下的“苹”的映射，求各个映射之间Cosine Similarity，发现相近语义的“苹”字之间Cosine Similarity比较高，不用语义的“苹”字之间Cosine Similarity比较低。这个实验在一定程度上能说明BERT为什么有效。 对之前解释的质疑# https://arxiv.org/abs/2103.07162中指出，用语言资料训练的BERT用在蛋白质、基因、音乐领域的任务也会有比不用预训练的方法表现更好，所以BERT有效的原因真的是因为BERT“学会了”语言吗？BERT有效的原因目前还在探索中。 Multi-lingual BERT# 之前提到的BERT都是单一语言的BERT，但是实际上，预训练BERT的时候可以用多语言的资料进行训练。比如下图指的就是让BERT做不同语言的“填空题”。 https://arxiv.org/abs/1909.09587中提出在zero-shot（提问时没有给出任何参考案例）阅读理解任务中，多Multilingual-BERT在只经过英文问答题的微调后，做中文的阅读题时竟然还能有不低的正确率。 一种解释是对于Multilingual-BERT，其实不同的语言的相同语义的词在语义空间上的映射是很接近的，但是训练这样的Multilingual-BERT是需要大量的资料与运算资源，才能让BERT学到不同语言之间的联系。 但是，如果语言之间真的没有差距，那么比如做英文问答时不会冒出别的语言吗？而这实际上是不会的，因为其实不同语言之间还是有一定的距离的。所以李宏毅课题组进行了一项实验（https://arxiv.org/abs/2010.10041），把中文词汇映射的平均值减去英文词汇映射的平均值，得到两种语言之间的差距。然后让Multilingual-BERT读入英文句子，加上英文到中文之间的差距，竟然能输出中文的句子了，而且部分中英词汇在语义上是相同的。所以一定程度上说明了Multilingual-BERT中蕴藏了语言之间的联系。 GPT# GPT的训练和使用其实和BERT是很类似的。 GPT的训练# 与BERT相似的是，GPT同样使用自监督学习进行预训练，但是预训练的任务是预测句子中的下一个token。 GPT的使用# 和BERT相似的是，GPT在迁移到下游任务时也需要一点成对的训练资料，但是和BERT不同的是，GPT的模型比BERT大得多，以至于连微调GPT都是很困难的，所以GPT在微调是不对GPT的参数进行调节的，并采用few-shot learning（提供问题和多个范例，让GPT给出答案）、one-shot learning（提供问题和一个范例，让GPT给出答案）甚至zero-shot learning（只提供问题，直接要求GPT给出答案）的方法进行训练。 附：李宏毅老师讲解chatGPT的训练 自监督学习在语音和图像领域上的应用# 概述# 自监督学习不仅可以用在文字上，也可以用在语音和图像领域上。语音/图片版本的BERT都可以通过大量未标注的资料来进行预训练。 语音领域上，李宏毅课题组提出了SUPERB（https://arxiv.org/abs/2105.01051、https://arxiv.org/abs/2203.06849），里面有十四项语音任务，用来评估语音版的BERT的表现。 视觉领域上，https://arxiv.org/abs/2110.09327内总结了各种模型在如物体检测、语义分割等多项视觉任务上的表现。 下面介绍五种训练语音/视觉领域的BERT的方法。 Generative Approaches# 文字版的BERT和GPT训练的中，我们会让BERT和GPT去生成一些字符（也许在是在文章中被遮挡，也许是预测接下来的字符）。所以就希望能把这个有效的方法迁移到语音和视觉领域。 语音# 但是语音和文字不同的是，masking时需要遮挡多个向量（https://arxiv.org/abs/1910.12638），因为语音中各个向量和其相邻的向量是差不多的，所以如果只遮挡一个向量，模型可能只是学会了对被遮挡向量的周围两个向量做一个内插罢了。 TERA则采用另一种遮挡方法，它并非遮挡某些向量，而是遮挡各个向量的部分维度，让模型去补全被遮挡的维度，这种方法可以让模型学到更多语音的知识。 如果想用GPT训练的方法，让模型预测接下来的语音信号也是可以的，只不过和文字不一样的是，需要预测n个向量（一般而言n&gt;3），理由和masking时遮挡多个向量理由是相似的。 视觉# 对于图像，只需要把图像拉成像素向量的序列就行了，接下来直接套用BERT和GPT的方法就行了。 Image GPT (openai.com) 但是这个方法的问题在于语音和图像包含了比文字更多的信息，所以用生成的方法让模型学习还原/产生语音或图像是比较困难的。 Predictive Approaches# 前面说到，让模型学习还原/生成完整的语音/图像是困难的，所以就希望用简单而又足够让模型学习到语音/图像特性的任务预训练模型。 https://arxiv.org/abs/1803.07728中提出，可以让通过模型学习判断一张图片被旋转的角度来预训练模型，除了这个方法以外，当然也可以用其他的简单任务来预训练模型。 语音上，也可以用相似的方法，比如让训练模型判断两端语音之间间隔的时间（https://ieeexplore.ieee.org/document/9060816）。 但是设置简单任务训练模型需要对语音/图像领域的信息有比较深入的理解，所以有没有一些更通用的方法呢？ HuBERT中采用对k-means的方法对语音中的向量进行聚类，然后采用训练模型去预测被遮挡的向量的类别，在图像领域上Deep Clustering for Unsupervised Learning of Visual Features一文中也提出了类似的方法。 Contrastive Learning# 前面的两类方法都需要让模型学习去产生一些东西，但是能不能让模型在不产生任何东西的情况下学到语音/图像的特征呢？ SimCLR# SimCLR中指出，可以通过把一张图像经过增强，得到不同的图像，训练模型使相似的图像（认为两张相似图像是“positive examples”）输出的向量是尽可能相近的，而训练模型使得不同的图像（认为两张不相似图像是“negative examples”）输出的向量是尽可能相差比较大的。对于语音领域，也有对应的Speech SimCLR。 但是对于Contrastive Learning而言，难的是判定什么是negative examples，两个为negative examples的样本不能差异太大（比如两张图像连背景颜色都不同，那么可能模型只学会了判断背景颜色），也不能差异太小（比如两张图片都是猫），所以需要复杂的方法选出negative examples。 那有没有什么方法能避开选择negative examples呢？下面两种方法可以做到 Bootstrapping Approaches# 如果为了避开negative examples，直接只用positive examples来训练模型，希望模型输出的向量尽可能相近，那么可能会发生“collapse”，就是模型对于任何图片都输出同样的向量。 Bootstrapping则采用一种很巧妙的方法，它让两张图片经过的路径不对称，其中一张图片只经过Encoder（暂且称为左路径），另一张图片经过Encoder后要再经过一个只有几个layer构成的简单predictor（暂且称为右路径），然后只通过右路劲更新参数，再把右路径中的Encoder中参数复制给左路径中Encoder。最后希望左右路径产生的向量是尽可能接近的，这种方法可以避开negative examples，还能避免出现“collapse”。 Simply Extra Regularization# VICReg# 这部分没太听懂quq，还希望dl教我……","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/"},{"title":"李宏毅机器学习-为什么训练会失败","text":"Training loss很高 这时发生了什么 训练方法 Batch Batch对训练效率影响 Batch对训练效果影响 Momentum Learning rate Loss function Training data的loss很小 过拟合 调整模型复杂度 添加更多数据 Mismatch Training loss很高# 这时发生了什么# 当微分接近0时，训练会停止。但是此时不一定是在极值点，也可能是在鞍点（saddle point）。 其实训练停止时，微分也不一定很接近0，也就不一定是在saddle point，这个后面会提到 通过求loss function的Hessian矩阵，判断Hessian矩阵正定性可以判断当前是否处在saddle point——H不定时则处在saddle point。 对于神经网络的loss而言，Hessian矩阵是一个维度非常高的矩阵，所以Hessian矩阵正定或负定的可能性很低，所以大部分训练停止的情况其实都是遇到了saddle point。 训练方法# Batch# 每一个epoch开始时，会随机分割batch，训练时每次依次用各个batch计算出梯度后更新参数。 Batch对训练效率影响# 一个很直观的想法是batch越大，训练时在一个batch上花的时间越久，但是由于GPU有并行运算的能力，其实batch在不是特别大的时候训练时间和batch size=1时相比几乎没有增长。所以适当的增大batch可以提高训练效率。 Batch对训练效果影响# 从下图可以看出，小batch的训练准确率比较高。 为什么小的batch训练准确率高呢？原因在于使用小的batch时，每个batch之间的loss function可能存在细微的差别，当一个batch训练时卡在saddle point，对另一个batch可能不是saddle point，可以继续训练。 Momentum# 训练时可以通过把上一步的移动和当前计算出的梯度结合，形成新的优化参数的方向，这有利于跳出saddle point，甚至可以帮助“翻越”比较小的山坡，使得loss function得到有效降低。 Learning rate# 其实如果learning rate比较大，那么可能会出现还没到saddle point就停下来的可能（此时梯度的模比0还是会大比较多的），如下图所示，loss function的值在两条绿线所指的点反复横跳。 所以我们需要动态调整learning rate，使得尽量不会出现还没到saddle point就停下来的情况 为了动态调整learning rate，我们可以采取两点 对每个参数动态调整learning rate θit+1=θit−ηgit→θit+1=θit−ησitgit\\theta_i^{t+1}=\\theta_i^t-\\eta g_i^t\\rightarrow\\theta_i^{t+1}=\\theta_i^t-\\frac{\\eta}{\\sigma_i^t}g_i^tθit+1​=θit​−ηgit​→θit+1​=θit​−σit​η​git​ t代表随着训练轮次而改变，i代表对不同的参数 常见的求σit\\sigma_i^tσit​的方法有Root mean square（σti=1t+1∑0t(git)2\\sigma_t^i=\\sqrt{\\frac{1}{t+1}\\sum_0^t (g_i^t)^2}σti​=t+11​∑0t​(git​)2​ ）和RMSProp（σit=α(σit−1)2+(1−α)(git)2\\sigma_{i}^{t}=\\sqrt{\\alpha\\left(\\sigma_{i}^{t-1}\\right)^{2}+(1-\\alpha)\\left(g_{i}^{t}\\right)^{2}}σit​=α(σit−1​)2+(1−α)(git​)2​ ，比较近的轮次的梯度影响比较大） 对所有参数进行learning decay 上式中η→ηt\\eta\\rightarrow\\eta_tη→ηt​ 我们在训练时，经常使用Adam，这个优化方法结合了动态调整RMSProp方法调整learning rate和momentum。一般来说比较有效。 Loss function# 选用不同的loss function，会有不同的error surface，所以选择合适的loss function，可以得到更容易进行优化的error surface。 Training data的loss很小# 若testing data的loss比training data的loss大很多，也不一定是过拟合~ 可能的原因 过拟合 mismatch 过拟合# 过拟合的解决方法有两种： 调整模型复杂度 添加更多数据 调整模型复杂度# 训练数据较多：增加复杂度 训练数据较少：降低复杂度 添加更多数据# 搜集更多数据 数据增强（Data augmentation） 左右翻转图片 放大图片局部 …… 但是数据增强要用合理的方式，比如图片识别不能出现使用中基本不出现的形式，如图片上下颠倒 Mismatch# mismatch指的是testing data的结构与training data的结构不一致","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/"},{"title":"记手搓ResNet的经历","text":"如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用 前言 ResNet基本思想 代码 各部分详解 Residual block resblock_basic类 resblock_bottlenect类 resnet类 References 前言# 在上学期的机器视觉大作业中我用到了ResNet50-Unet，寒假中做分类任务时又用到了ResNet，但是之前我用ResNet要么是pip之后直接import，要么是参照hw里面助教给的初始代码进行增删。本着搞懂ResNet这么一个经典模型的心态，我决定自己手搓一遍ResNet（好吧其实还是有参照，但是在参照的基础上加了一点东西）。 ResNet基本思想# ResNet通过引入直接连接的旁路（shortcut），减少了反向传播时梯度消失的问题，使得模型能搭的更深，更不容易过拟合。 下表是各种CNN架构在ImageNet数据集上的top-5 error rate，可以看到ResNet相比VGG等其它架构，有着更好的效果。 代码# 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190\"\"\"Implementation of ResNet with pytorchSimple usage: from resnet_pytorch import * classifier = resnet()All usage: demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=\"resnet101\") Input no parameters: classifier = resnet(class_num=16) # return resnet50References:[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1[2] https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py[3] 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197\"\"\"import torchimport torch.nn as nnclass resblock_basic(nn.Module): \"\"\" the block for resnet18 and resnet34 \"\"\" # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( # if the kernel size equals to 3, the padding should be 1 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size and number of channels are equal to the input, # the shortcut path do nothing self.shortcut = nn.Sequential() # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resblock_bottleneck(nn.Module): \"\"\" the block for resnet50, resnet101 and resnet152 \"\"\" # in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1 # in bottleneck, the expansion of filters in the last layer of the block is 4 expansion = 4 def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.res_function = nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), # maxpooling is replaced by convolution layer with stride unequal to 1 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1), nn.BatchNorm2d(out_channels * self.expansion) ) # If the output size or number of channels is unequal to the input, # the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * self.expansion) ) def forward(self, x): return nn.ReLU(inplace=True)(self.res_function(x) + self.shortcut(x))class resnet(nn.Module): def __init__(self, block=resblock_bottleneck, channel=3, filter_list=None, block_num_list=None, class_num=10, net_type=None): \"\"\" block: type of block, default: bottleneck channel: the channel of image, 1 for gray image and 3 for RGB image. default: 3 filter_list: the filter numbers of each blocks' first layer. default: None block_num_list: repeat times for each block, the length should be equal to filter_list. default: None class_num: the number of classes for classification. default: 10 net_type: the type of resnet, 'resnet50' for example. default: None demo: Customization resnet: classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11) Only specify the type of resnet: classifier = resnet(class_num=12, net_type=\"resnet101\") Input no parameters: classifier = resnet(class_num=16) # return resnet50 \"\"\" super().__init__() if block_num_list is None: block_num_list = [3, 4, 6, 3] if filter_list is None: filter_list = [64, 128, 256, 512] # different types of resnet in the original paper if net_type == 'resnet18': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [2, 2, 2, 2] elif net_type == 'resnet34': block = resblock_basic filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet50': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 6, 3] elif net_type == 'resnet101': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 4, 23, 3] elif net_type == 'resnet152': block = resblock_bottleneck filter_list = [64, 128, 256, 512] block_num_list = [3, 8, 36, 3] self.resblock_in_channel = 64 self.pre_conv_layer = nn.Sequential( nn.Conv2d(in_channels=channel, out_channels=self.resblock_in_channel, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(self.resblock_in_channel), nn.ReLU(inplace=True) ) stride_list = [1] + [2] * (len(filter_list) - 1) self.resblocks = nn.ModuleList() for i in range(len(filter_list)): self.resblocks.append(self._make_block(block, filter_list[i], block_num_list[i], stride_list[i])) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Sequential( nn.Dropout(0.25), nn.Linear(filter_list[-1], class_num) ) def _make_block(self, block, filter, block_num, stride): layers = [] strides = [stride] + [1] * (block_num - 1) for stride in strides: layers.append(block(self.resblock_in_channel, filter, stride)) self.resblock_in_channel = filter * block.expansion return nn.Sequential(*layers) def forward(self, x): output = self.pre_conv_layer(x) for block in self.resblocks: output = block(output) output = self.avg_pool(output) output = output.view(output.size(0), -1) output = self.fc(output) return output 各部分详解# Residual block# 在ResNet的原始论文中，提出了如下图两种residual block，右边的一种被称为bottleneck。前一种residual block在ResNet层数较浅时使用，如ResNet18，ResNet34；后一种residual block在ResNet层数较深时使用，如ResNet50、ResNet101、ResNet152。 resblock_basic类# 前向传播过程：residual block前向传播的过程，要经过两次卷积+batch normalization，其中第一次卷积、batch normaliztion后，需要经过ReLU，而第二次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，两个卷积层输出的channel数是相同的。 降采样方法：residual block里面不设置max pooling，而是通过卷积层中设置大于1的步长起到降采样的作用，一个block中只有第一层卷积层中的stride可能大于1，第二个卷积层的stride为1。 padding：由于卷积核大小为3x3，所以两个卷积层的padding都应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resblock_bottlenect类# 前向传播过程：residual block前向传播的过程，要经过三次卷积+batch normalization，其中前两次卷积、batch normaliztion后，需要经过ReLU，而第三次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。 输出channel数：block中，前两个卷积层输出的channel数是相同的，而第三个卷积层输出的channel数是前两层的四倍。 降采样方法：一个block中只有第二层3x3卷积层中的stride可能大于1，第一、三个1x1卷积层的stride为1。 padding：由于第二层卷积核大小为3x3，所以第二个卷积层的padding应该为1。 shortcut path：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。 resnet类# 在原始论文中，ResNet要先经过一个7x7卷积层，然后在经过若干个residual block，最后通过FC得到输出。 预卷积层：原始论文中，预卷积层卷积核大小为7x7，所以padding=3，该卷积层步长为2，起到降采样作用，输出channel数设置为64。 residual block序列：中间的residual block序列可以用 nn.ModuleList存放，通过 _make_block函数循环添加。 自适应平均池化层：将特征图自适应转化为序列。 全连接层：设置0.25 dropout率，然后再全连接。 References# Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385v1 https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197","link":"/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/"},{"title":"李宏毅机器学习-lect8 Auto-encoder","text":"Auto-encoder的基本思想 Auto-encoder的训练 为什么Auto-encoder是有效的 Auto-encoder和BERT的关系 Auto-encoder的应用 Feature Disentanglement Discrete Latent Representation Generator Compression Anomaly Detection Auto-encoder的基本思想# Auto-encoder的训练# Auto-encoder的是以无监督学习训练的，训练时会把一个Encoder和一个Decoder拼接在一起，将Encoder产生的向量输入Decoder，目标是让Decoder尽可能去复原输入。如果要使得Decoder能复原Encoder的输入，就要求Encoder能有效地将输入编码为向量，所以训练好以后，Encoder就有强大的编码功能，就可以拿去做一些下游任务（这和BERT是类似的）。 其实Auto-encoder和Cycle-GAN的思想也是有点类似的（其实是先有Auto-encoder），Cycle-GAN是希望一个Generator产生的图片经过另一个Generator后能够复原，训练成功后得到两个Generator。 为什么Auto-encoder是有效的# 以图像的任务为例，把图片看成高维度的向量，Auto-encoder的Encoder就是将一个高维度的向量降维成低维度的向量。 由于维度降低，必然有信息丢失。Decoder能复原成功，主要是因为图片虽然是高维的向量，但是其变化是有限的，比如下图中也许3x3的图片可能实际上只有4种变化是合乎情理的，所以可以把9维向量降维为2维向量，实际任务中的图片也会符合某种分布，这会限制图片的变化种类，所以用低维度向量来表示图片依然保留了图片的特性，所以Decoder才能复原图片。 那么为什么Auto-encoder中的Encoder在下游任务中是有效的呢？因为训练好的Encoder能有效地将数据降维，从而把复杂的数据用简单的方式表示，所以在下游任务中只用需要一点带标注的资料对模型进行训练就可以得到一个很好的结果。这个其实和BERT很像，BERT也是讲高维的语言信息降维，然后输入给输出层，在微调的时候，由于BERT具有很好的提取特征的能力，只需要很少的训练资料就可以让模型学会下游任务。 Auto-encoder和BERT的关系# 从训练过程来看，在实际训练Auto-encoder的时候，我们其实会把数据（比如图片）加上一点噪声，然后希望Decoder输出的数据能和加上噪声前的数据接近。那么其实Auto-encoder的训练过程就很像BERT了，因为预训练BERT时也是会给数据加上噪声，比如给句子挖空让BERT学做“填空题”。 从作用来看BERT和Encoder的作用类似，都是完成特征的提取；线性输出层和Decoder作用类似，都是对特征进行处理。 Auto-encoder的应用# Auto-encoder在训练好以后，除了可以将Encoder拿出来迁移到下游任务上，Auto-encoder还有很多其他应用。 Feature Disentanglement# Feature Disentanglement，字面翻译就是特征解耦，就是将Encoder输出的向量包含的纠缠的特征（我们不知道哪些维度代表哪些特征）进行分离。举例来说，Auto-encoder可以做到输入一段声音，让Encoder输出的向量前一半代表说话内容，后一半代表说话的人。这样就有可能实现语音合成之类的应用，比如输入李宏毅老师说的\"How are you?“和新桓结衣说的\"Hello”，输出新垣结衣说的\"How are you?\"。 Discrete Latent Representation# 一般来说，Encoder输出的向量中的每一个数都是一般的实数，但是有时我们希望输出的向量中每一维都只有0和1，这样可能可以更好地解释Encoder输出，比如输入一张人物头像图片，第一维的1代表有戴眼镜，0代表没有戴眼镜。有时我们甚至可以让输出的向量是一个one hot向量，这样可能可以实现无监督学习训练模型做分类任务，比如手写数字识别，输出十维向量，观察向量中哪一维为1，就可以得出分类结果。 在Discrete Representation中，最知名的是VQVAE，Encoder输出一个实数向量，和codebook中的各个向量（也是通过学习得到的）计算相似度（有点像Self-attention，Encoder输出向量是query，codebook中向量是key），将相似度最大的向量拿出来，作为输入Decoder的向量。这可以使得Embedding的种类是有限的，这样可能可以做语音辨识，因为可能可以把Encoder的连续输出对应到离散的音标。 除此之外，Auto-encoder的思想在文本上也有很好的应用。我们希望Encoder（此时Encoder和Decoder都是seq2seq模型）可以将输入的文章进行特征提取，从而产生文章摘要，但是如果只是用Auto-Encoder的架构，会发现Encoder和Decoder之间会达成某种“契约”，就是Encoder生成的文字只有Decoder能看懂，而无法被人理解。所以为了产生让人能理解的摘要，需要让产生的摘要经过一个Discriminator，用来判定文章是不是人写的，那么其实仔细一想，这不是Cycle-GAN吗？（^_^） 所以Auto-encoder也是一种理解Cycle-GAN的方式？ Generator# 我们也可以将Decoder单独拿出来，作为一个Generator进行使用，可以实现从某个分布中随机抽取一个向量，然后产生图片，这其实就是Variational auto-encoder（VAE）的想法。 Compression# 利用训练好的Auto-encoder，我们可以进行图片的压缩。因为图片经过Encoder时会丢失一部分信息，所以经过Decoder后复原的图像相比原始图片会有一定失真，这是一种有损压缩的方式。 Anomaly Detection# Auto-encoder也可以做异常样本检测，我们把训练的数据的分布看成一个domain，训练成功的Auto-encoder是可以让这个domain上的数据在经过Encoder编码、Decoder解码后复原的。但是如果今天输入的数据不在这个domain内，比如训练的时候输入真人头像，输入二次元人物头像时就很有可能无法复原。应用Anomaly Detection，可以实现诈骗信息、异常交易记录的检测等具体的应用。 异常检测的问题很可能不能用训练分类器的方法去做，因为异常样本往往是很少的，会产生极端的样本不平衡。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/"},{"title":"李宏毅机器学习-lect9 Explainable ML","text":"为什么我们需要可解释的机器学习 可解释机器学习的目标 Local Explaination 理解对模型决策重要的部分 如何理解 限制 为什么要理解对模型输出重要的部分 理解模型对于输入的处理 将输出进行降维 观察模型参数 Probing Global Explaination 基于先验知识的限制 基于生成的限制 LIME 为什么我们需要可解释的机器学习# 在很多场合，仅仅让AI给出答案是不够的，我们还需要让AI告诉我们为什么它认为这是答案，比如： AI执法时除了给出判决，还需要给出它的依据 自动驾驶时汽车突然刹车，事后可能要分析做出突然刹车的决定的理由 AI开药时除了给出处方，还需要给出开这种处方的理由 可解释机器学习的目标# 关于这个话题有很多的观点，李宏毅老师的观点是我们并不是因为神经网络是黑箱而不敢用它的决策，因为我们的大脑其实也可以看成黑箱，但我们还是遵从大脑的决策。其实我们只是需要一个用这个决策的理由罢了。所以可解释机器学习的目标，也许只是让人有一个相信AI的理由而已。 Harvard做过一个心理学实验，这个实验是以不同方式询问排队使用打印机的同学是否可以插队并收集反应。 “对不起，我只有5面需要打印，你能让我先用打印机吗？”——60%的人同意 “对不起，我只有5面需要打印，你能让我先用打印机吗？因为我赶时间。”——94%的人同意 “对不起，我只有5面需要打印，你能让我先用打印机吗？因为我需要打印。”——93%的人同意 所以哪怕第三种询问方式的理由非常无厘头，也能大大提高同意插队的概率，所以也许决策时更多只是需要一个理由。这个实验一定程度上能佐证李宏毅老师的观点。 Local Explaination# 让我们了解一只猫在模型“眼里”是什么样的 理解对模型决策重要的部分# 如何理解# 比如今天我们输入一只猫，我们希望知道哪些部分对于模型判断它是只猫是重要的，那么我们可以将它的一部分遮挡，然后观察被遮挡后的图片被模型判断出是猫的概率，如果某个部分被遮挡后的图片被认为是猫的概率较低，就可以认为这部分对模型的判断而言是重要的。 比如下面这张图中，用灰色块去遮挡第一排的图片的不同位置，产生第二排的结果。第二排中蓝色部分代表这部分被遮挡后判断正确概率较低。 如果想要更精确地了解哪部分对于模型是重要的，可以在每一个像素上加上一个很小的值，然后观察模型输出概率的变化，然后通过变化率ΔeΔx\\frac{\\Delta e}{\\Delta x}ΔxΔe​ 来判断某个像素对于模型判断的重要性。那么其实这个方法就是把图片看成矩阵XXX，构造e=f(X)e=f(X)e=f(X)，然后进行矩阵求导，再代入图片XXX的值得到导数，也就是下图中第二排图片，即Saliency Map。 限制# 但是实际上，产生的Saliency Map大部分都是含有比较多的噪声的，所以https://arxiv.org/abs/1706.03825中提出可以将原图加上不同的随机噪声，然后得到多张Saliency Map后再取平均，可以起到平滑的作用。 除此之外，可能还会遇到Gradient Saturation的问题，比如判断鼻子长度和是大象的概率的相关性时，当鼻子比较短时，是大象的概率会随鼻子长度的增长而增长，但是鼻子长度比较长的时候，可能被判断为是大象的概率就不会增长了。这个例子是说明在生成Saliency Map中，可能某些重要要素会由于Gradient Saturation的问题导致没有被检测出来。 对于这个问题，可以通过https://arxiv.org/abs/1611.02639提到的Integrated gradient来解决。 下图是照相机图片的Integrated gradient图和gradient图的对比，明显可以发现Integrate gradient可以更好的解释模型为什么做出物体是照相机的判断。 为什么要理解对模型输出重要的部分# 因为有时候模型并不一定是看到猫眼、猫耳等特征才知道是猫的，比如下图中左下角有一串文字，模型认为这串文字才是分类的重点，这与我们的目标是不符合的。通过观察哪些部分对模型是重要的，我们可以了解模型是不是学会了我们真正想让模型学习的任务。 理解模型对于输入的处理# 将输出进行降维# Hinton在12年时就尝试过把语音识别系统的输出进行降维，然后发现不同语者（用不同颜色的点表示）说的同样的话的分布是接近的，所以可以认为语音识别系统真正学会了无视说话的人，而将注意力集中在说话的内容上。 观察模型参数# 比如在Self-attention中，我们可以观察attention score来了解模型对于某个输入的哪些部分比较在意，以及这个Self-attention层对这个输入做了什么处理。 Probing# 我们可以将模型中某一层的输出进行处理，来观察这一层之前的模型对输入做了什么处理。比如可以把BERT中某一层的输出输入一个分类器里面，进行词性的分类。但是要求分类器的性能比较好。 或者在语音识别系统中，我们把前几层的输出作为TTS（text to speech）的输入，假设发现TTS的输出的语音中说话的人已经无法被分辨，那么模型的前几层的作用可能就是把输入的声音中说话的人的特征模糊了。 Global Explaination# 让我们了解模型“心中”的猫是什么样的 在CNN中，可以把卷积层的输出理解为特征图，特征图的值越大，说明图像中特征越明显，那么我们可以固定卷积层的参数，用gradient ascent（和gradient decent是类似的）的方法去找出一张让卷积层输出的特征图的所有像素的值的累加最大的图片X∗X^{*}X∗。观察X∗X^{*}X∗我们可以知道各个卷积层对什么特征是感兴趣的。 比如一个做手写数字识别的12层CNN中我们对每个卷积层求它的X∗X^*X∗，得到左侧的图片，那么发现有些层侧重检测竖线，有些层侧重检测横线。 基于先验知识的限制# 通过之前提到的gradient ascent找出X∗X^{*}X∗的方法，我们知道了每个卷积层侧重处理什么特征。更进一步，以手写数字辨识模型为例，我们可以固定整个模型的参数，通过gradient ascent来找出使得输出为某个数字概率最大的图片，这样或许我们就可以看到模型“心中”的数字长什么样。 但是实际上，直接这么操作产生的图片并不会有很好的结果，而是产生下方图片左侧的马赛克状的图片。但是我们如果从adversarial attack（21年貌似先讲adversarial attack，22年才是先讲explainable ML）的角度来想，一个人眼不可见的杂讯可以使得模型判别的结果直接改变，那么就能比较好地理解为什么直接用gradient ascent会产生马赛克状的图片。 所以我们需要给优化函数加上一个R(X)R(X)R(X)项，这一项代表输出的图片有多像人眼看到的数字，构造这个函数可能需要一些先验知识，比如我们认为看到的数字中白色的像素点不会很多，所以图片总灰度值可能不会特别大，那么我们可以使R(X)=−∑i,j∣Xij∣R(X)=-\\sum_{i,j}|X_{ij}|R(X)=−∑i,j​∣Xij​∣，从而得到了人眼看起来有点像数字的模型“心中”的数字。 下图是https://arxiv.org/abs/1506.06579中得到的部分模型“心中”的各种物体的图像。 基于生成的限制# 在优化函数中加入限制项或许可以起到效果，但是需要一定的先验知识，还需要复杂的调参过程。另一种比较明朗的方法是先训练一个把向量变成图片的生成器（GAN、VAE之类），然后把生成器和分类器接在一起，固定整个模型的参数，用gradient ascent的方法找出使得输出为某个类别的概率最大的向量zzz，然后模型“心中”的图片就是向量zzz通过生成器GGG的输出G(z)G(z)G(z)。 下图是https://arxiv.org/abs/1612.00005中得到的一些模型“心中”的火山的图像 LIME# 线性的模型的可解释性是很强的，因为我们可以了解到每一个元素的权重，但是它在复杂任务上表现不佳。所以我们也许可以用线性模型来模拟复杂模型中的一小部分，来解释这一小部分的作用。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/"},{"title":"李宏毅机器学习-lect11 Domain Adaptation","text":"为什么需要Domain Adaptation 不同情境下的Domain Adaptation 新的Domain上有一点带标注的资料 新的Domain上有很多不带标注的资料 新的Domain上只有一点不带标注的资料 我们对新的Domain一无所知 为什么需要Domain Adaptation# 在之前的课程的作业中，训练资料和测试资料的分布都是类似的，但是在很多实际任务中，会出现训练资料和测试资料的分布不一样的情况（就像平时的练习题和高考题目出题风格存在差异），称为Domain shift，那么此时模型的表现可能就会下降很多。比如我们在Mnist上训练手写数字分类模型后，如果测试资料是彩色数字，模型的正确率可能会从超过99%下降到不到60%。但是我们很多时候是没有大量带标注的、和测试资料分布相同的训练资料的，所以我们需要Domain Adaptation。 常见的Domain shift有三种类型 Source Domain和Target Domain分布不同 Target Domain中各个类型数据量和Source Domain不同，可能某个类型特别多 Source Domain和Target Domain分布相似，但是两个Domain中相似的数据的标注不同 不同情境下的Domain Adaptation# 做Domain Adaptation时，我们对新的Domain可能有不同的“了解”程度，如下图所示，对新的Domain的了解程度从高到低为 新的Domain上有一点带标注的资料 新的Domain上有很多不带标注的资料 新的Domain上只有一点不带标注的资料 我们对新的Domain一无所知 新的Domain上有一点带标注的资料# 这种情况比较简单，只需要用Target Domain上的资料微调模型（一般训练几轮就可以了），就可以达到比较好的效果，但是要注意防止过拟合。 新的Domain上有很多不带标注的资料# 很多时候我们只知道新的Domain会有怎样的数据，而不知道这些数据应该有什么样的输出。对于这类问题，基本想法是训练一个特征提取器，希望Source Domain的数据和Target Domain上的数据在经过特征提取器后的分布是类似的。这样相当于是无视了Target Domain和Source Domain中的差异，只提取两个Domain上数据的共有特征。 为了训练特征提取器，我们还需要一个Domain Classifier来判断特征提取器的输出属于哪个Domain，其实这里就有点像GAN了。 但是，只让Source Domain的数据和Target Domain上的数据在经过特征提取器后的分布类似很可能会让特征提取器学会无论什么样的图片都输出相同的向量，所以我们在训练时还应该让提取出的特征经过输出层，在特征提取器的loss函数中加上输出标签和真实标签之间的loss，确保提取出的特征是有效的。 然后我们除了让Target Domain上的数据和Source Domain上的数据分布类似，我们也许还需要让Target Domain上的数据远离Source Domain中不同label的数据之间的边界。这个可以通过在训练特征提取器的时候尽可能使得提取出的特征经过label predictor后的分布比较集中（就是比较鲜明地属于某个类别），此时认为这个数据是远离Source Domain上的各类数据之间的边界的。 在之前的讨论中，我们好像都默认了Source Domain和Target Domain的label的类型都是一样的，就像下图左上所示。但是实际上，Target Domain中的类别可能是Source Domain的子集（下图右上）；也可能是Target Domain和Source Domain有交集，但都有各自独特的label（下图左下）；也可能Source Domain是Target Domain的子集（下图右下）。那么如果此时用之前的方法做Domain Adaptation，可能会因为强行让两个Domain重合而导致错误，比如在下图右上的情况中把Target Domain中的狗认成老虎。 Universal Domain Adaptation (thecvf.com)中提出了Universal Domain Adapation的方法，简单来说就是需要判断Target Domain上的数据是不是属于Target Domain和Source Domain的共有类别内，如果不在共有类别内，则输出unknown。 新的Domain上只有一点不带标注的资料# 有时我们面对Target Domain时，我们甚至没有大量不带标注的资料，那此时怎么做Domain Adaptation呢？ https://arxiv.org/abs/1909.13231中提到，可以通过Test time training的方法实现Domain Adaptation，这个方法的大致思路在Test-Time Training - 知乎 (zhihu.com)内讲得挺清楚的。 我们对新的Domain一无所知# 有时我们甚至不知道Target Domain会有怎样的数据，就像高考前不知道今年会出什么怪题。 此时我们可以通过Domain Generalization的方法，在训练时就给予不同的Domain的资料，从而让模型学会无视Domain之间的差距，实现当新的Domain中的数据输入时，模型也能有很好的效果。 但是有时我们的训练资料只有一个Domain，而测试资料却有不同的Domain。https://arxiv.org/abs/2003.13216中提出我们可以尝试用数据增强产生多个Domain的训练资料，然后进行训练，从而实现Domain Generalization。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/"},{"title":"李宏毅机器学习-lect10 Adversarial Attack","text":"攻击的例子 如何实现攻击 白盒攻击 黑盒攻击 One pixel attack Universal Adversarial Attack 视觉领域外的攻击 语音、文字上的攻击 物理世界中的攻击 Adversarial Reprogramming “Backdoor” in Model 如何对抗攻击 被动防御 主动防御 本文包含youtube视频，建议更换ip后浏览 攻击的例子# 对下图中左侧的猫，ResNet-50有64%的把握认为这是猫，而右侧的猫只是左侧的猫加上一个人眼不可辨别的杂讯，却可以使得ResNet-50 100%地认为这是一只海星。 如何实现攻击# 白盒攻击# 白盒攻击（White Box Attack），是指今天我们攻击的模型参数已知的情况，这种攻击是比较容易的，因为我们可以用梯度下降法（模型参数不变，图片作为参数），找到一张图片，使得模型的L(x)=−e(y,y^)L(x)=-e(y,\\hat{y})L(x)=−e(y,y^​)（交叉熵乘-1）最小，此时交叉熵会很大，那么代表着模型对这张图片会产生错误的预测结果。 但是刚刚提到的攻击是Non-targeted attack，也就是攻击没有特定的目标，我们不知道产生的这张图片会让模型产生怎样的预测结果。如果我们想要操控模型的预测结果，那么需要Targeted attack，那么此时L(x)=−e(y,y^)+e(y,ytarget)L(x)=-e(y,\\hat{y})+e(y,y^{target})L(x)=−e(y,y^​)+e(y,ytarget)，后面的一项为模型对xxx的预测yyy和我们希望操控模型得到的结果ytargety^{target}ytarget的交叉熵。 此外，攻击时还应该保证杂讯不会被人眼察觉，这就要求求解得到的图片和原来的图片差值Δx\\Delta xΔx符合一定的约束。常用的约束一般有二范数和无穷范数。由于一个像素的值改变很大容易被人眼察觉，所以一般来说使用无穷范数进行约束。 将梯度下降方法和无穷范数的约束结合，最后优化问题的表达式为 x∗=arg⁡min⁡d(x0,x)≤εL(x) x* = \\arg \\min _{d(x^0, x)\\le\\varepsilon}L(x) x∗=argd(x0,x)≤εmin​L(x) 优化的过程如下图黄色框内所示，如果更新xxx后无穷范数超过限制（下图中蓝色点），就将xxx拉回xxx与x0x^0x0（下图中黄色点）的连线上恰好满足无穷范数约束条件的点（下图中橙色点）。 黑盒攻击# 但是在大多数情况下，我们是不知道模型的参数的，在这种情况下对模型的攻击称为黑盒攻击（Black Box Attack）。如果我们知道模型使用哪个数据集训练的，我们可以用这个数据集训练一个Proxy Network用来模拟要攻击的模型，然后此时问题转化为白盒攻击。 http://arxiv.org/abs/1611.02770的使用ResNet-152、ResNet101、ResNet-50、VGG-16、GoogLeNet分别作为Proxy Network和Target Network，每一行代表用某种模型作为Proxy Network，每一列代表用某种模型作为Target Network（所以下表中对角线代表白盒攻击，非对角线代表黑盒攻击），表中数据代表正确率，正确率越低说明攻击效果越好。可以发现黑盒攻击也会有一定的效果，但是效果不如白盒攻击。 http://arxiv.org/abs/1611.02770中也探讨了为什么黑盒攻击可以成功，下图中各个子图的水平方向是VGG-16的梯度方向（图像沿着这个方向变化可以攻击成功），垂直方向则是随机正交方向，可以发现5个模型的决策区域（图片处于深蓝色区域中不会被攻击成功）是类似的，所以可能是数据集上的问题使得黑盒攻击可以成功。 One pixel attack# https://arxiv.org/abs/1806.11146提出的One pixel attack只改变图片中的一个像素，同样能起到攻击模型的作用。 Universal Adversarial Attack# Universal Adversarial Attack，是指对每个模型，可以找到一个杂讯，使得这个杂讯加在任何图片上都能攻击成功。如果能实现在模型输入前将图片加上这个杂讯，可能就可以比较高效地破坏模型的工作。 视觉领域外的攻击# 语音、文字上的攻击# 语音模型上攻击的方法类似图像模型，都是加入一个杂讯。对于文字模型上的攻击，https://arxiv.org/abs/1908.07125尝试在文章后加上\"why how because to kill american people.“，发现模型在问答题上给出的答案都会变成\"to kill american people”。 物理世界中的攻击# 有时候对模型的攻击可能直接来自物理世界，比如下面的限速牌中的“3”的中间的横线部分被延长了一些，就能够干扰自动驾驶系统的判断（观察视频中的仪表盘可以发现限速功能失效了）。 Adversarial Reprogramming# https://arxiv.org/abs/1806.11146中提出的Adversarial reprogramming相当于是通过攻击别人的模型实现自己想要完成的任务，比如在杂讯图片的中间加上方块阵列，不同数目的白色方块数与特定label对应。那么只要观察模型的输出，就可以知道杂讯图片中间的方块阵列有几个方块。 “Backdoor” in Model# https://arxiv.org/abs/1806.11146提出了通过给数据集“投毒”的方法来在训练阶段攻击模型。被“投毒”的数据集中看起来图片和标签的对应都是没有问题的，但是可能会让模型在面对某个特定测试数据时出现问题。比如给狗的图片“投毒”，在狗的图片上加上不透明度很低的特定的鱼的图片的水印，使得模型在测试时会把特定的鱼的图片辨识为狗。 如何对抗攻击# 被动防御# 被动防御是指在图像输入模型前，让图像通过一个filter，这个filter相当于一个盾牌，可以抵挡对模型的攻击。 常见的被动防御方式有将图像平滑、图像压缩、通过generator重新生成等方法，但是这些方法如果被攻击者知道，攻击者也可以做出针对特定防御方式的攻击。 https://arxiv.org/abs/1711.01991中提出了一种通过对图像进行随机增强的方式来对抗对模型的攻击，这种防御方式会有比较好的效果（毕竟自己都不知道自己要干什么，别人怎么能猜透^_^）。 主动防御# 主动防御是指用训练数据集训练模型后，对每一个训练数据，用梯度下降求出用于攻击模型的数据，然后再用这些新的数据攻击模型，这个过程可以重复多次，相当于是一个不断“查缺补漏”的过程。也可以把这种方法理解成是一种数据增强的方式。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/"},{"title":"李宏毅机器学习-lect12 Deep Reinforcement Learning","text":"引言 为什么需要强化学习 强化学习中的要素 强化学习的过程 未知函数 优化目标 优化 Policy Gradient Actor-Critic Value function Value function的作用 Reward Shaping No Reward: Learning from Demonstration 引言# 为什么需要强化学习# 在讨论什么是强化学习之前，我们先讨论为什么需要强化学习。在诸如图像分类等很多任务上，我们知道给模型某种输入，模型应该产生什么输出，但是如果是下围棋这样复杂的任务，也许我们可以通过让模型学习人类高手的棋谱学会一些章法，但是人类棋手下的棋也未必是当前形式下最好的招式。总之在连人类都不知道怎么做是最好的时候，我们可能需要强化学习。 强化学习中的要素# 强化学习的训练中一般有以下五个要素 Actor：我们需要训练的模型（比如AlphaGo） Environment：任务的环境（在围棋中即轮到自己下棋时的棋局） Observation：某个时刻下Actor对环境的观察（AlphaGo以当前棋局作为输入，用于计算下一步怎么走） Action：Actor根据Observation做出的反应（AlphaGo计算出的下法） Reward：评判某一个Action的好坏 强化学习的过程# 在一般的机器学习中，可以分为三步 定义一个未知函数 定义损失函数 优化 虽然强化学习不同我们以前熟知的机器学习，但是大致步骤也类似于上面三步。 未知函数# Deep Reinforcement Learning中，我们定义的函数也是一个神经网络，以space invader游戏为例，游戏画面作为输入，输出选择各个操作（向左移动、向右移动、开火）的概率分布，这相当于是做了一个分类任务。 优化目标# 在强化学习中，我们希望模型在全局而不仅仅是某一个局部中取得一个良好的效果，那么我们就应该把所有Action的reward相加，并以reward的和R=∑rtR=\\sum r_tR=∑rt​作为优化目标。 比如在space invader游戏中，开火并消灭敌人能得到5分reward，左右移动得到0分reward。但是一直开火可能只能消灭一列上的敌人而无法得到最好的表现。所以应当以总的reward为优化目标，才能让模型学会在合适的时候左右移动。 优化# Deep Reinforcement Learning中，优化是比较困难的问题，将在下一节展开讨论。 Policy Gradient# 本部分从如何“控制”模型的行为出发，最后总结policy gradient的方法。 假设我们知道每一步我们期待模型去采取的行为a^\\hat{a}a^，那么我们可以让loss为模型的输出aaa和a^\\hat{a}a^的交叉熵eee，假设我们不希望模型采取行为a^\\hat{a}a^，那么我们应当让loss为−e-e−e。 那么这种情况下loss function 但是很多时候仅以±1来衡量希不希望模型做某个行为是不够的，所以对每一个行为an^\\hat{a_n}an​^​引入AnA_nAn​，来评估多想/不想模型采取某个行为。 下面我们讨论如何产生合适的AnA_nAn​，Version 0是一种很直观的想法，它让AnA_nAn​等于每一步的reward，即An=rnA_n=r_nAn​=rn​，但是这样的模型是“短视”的，因为在评估每一个举措的优劣时，只会根据当前步骤后直接的reward。 Version 1（cumulated reward）的方法是让At=∑n=tNrnA_t=\\sum _{n=t}^N r_nAt​=∑n=tN​rn​ ，这样模型在评估某一步行为的优劣时，可以考虑这一步对后续会造成什么影响。 Version 1的方法的问题在于它认为某一个步之前的所有步骤对这一个步之后reward的作用是均等的，而实际上比较符合直觉的是距离较远的步骤对当前步骤的reward的影响小，距离较近的步骤对当前步骤的reward影响大。所以Version 2改为用cumulated discount reward衡量行为优劣，即在求和时，式子改写为At=∑n=tNγn−trnA_t=\\sum_{n=t}^N \\gamma^{n-t}r_nAt​=∑n=tN​γn−trn​ Version 3是在cumulated discount reward的基础上对AnA_nAn​减去一个常数bbb，这样可以让AnA_nAn​有正有负，否则如果所有行为的reward都是正值，那么可能会鼓励模型去做我们不想让它学到的行为。 最后，得到AnA_nAn​后，我们可以做Policy gradient，步骤如下，需要注意的是，每一轮训练时，我们都需要让模型重新产生自己的行为。 实际上我们不知道模型应该采取怎样的行为，也不能这么计算loss，这样写只是便于理解，具体的数学推导参考Understanding deep learning 中对于Policy gradient的讲解。优化的目标和迭代表达式如下： θ=argmax⁡θ[Eτ[r[τ]]]=argmax⁡θ[∫Pr⁡(τ∣θ)r[τ]dτ] \\boldsymbol{\\theta}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\left[\\mathbb{E}_{\\boldsymbol{\\tau}}[r[\\boldsymbol{\\tau}]]\\right]=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\left[\\int \\operatorname{Pr}(\\boldsymbol{\\tau} \\mid \\boldsymbol{\\theta}) r[\\boldsymbol{\\tau}] d \\boldsymbol{\\tau}\\right] θ=θargmax​[Eτ​[r[τ]]]=θargmax​[∫Pr(τ∣θ)r[τ]dτ] θ←θ+α⋅∫∂∂θPr⁡(τ∣θ)r[τ]dτ=θ+α⋅∫Pr⁡(τ∣θ)1Pr⁡(τ∣θ)∂Pr⁡(τi∣θ)∂θr[τ]dτ≈θ+α⋅1I∑i=1I1Pr⁡(τi∣θ)∂Pr⁡(τi∣θ)∂θr[τi]. \\begin{aligned} \\boldsymbol{\\theta} &amp; \\leftarrow \\boldsymbol{\\theta}+\\alpha \\cdot \\int \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\operatorname{Pr}(\\boldsymbol{\\tau} \\mid \\boldsymbol{\\theta}) r[\\boldsymbol{\\tau}] d \\boldsymbol{\\tau} \\\\ &amp; =\\boldsymbol{\\theta}+\\alpha \\cdot \\int \\operatorname{Pr}(\\boldsymbol{\\tau} \\mid \\boldsymbol{\\theta}) \\frac{1}{\\operatorname{Pr}(\\boldsymbol{\\tau} \\mid \\boldsymbol{\\theta})} \\frac{\\partial \\operatorname{Pr}\\left(\\boldsymbol{\\tau}_{i} \\mid \\boldsymbol{\\theta}\\right)}{\\partial \\boldsymbol{\\theta}} r[\\boldsymbol{\\tau}] d \\boldsymbol{\\tau} \\\\ &amp; \\approx \\boldsymbol{\\theta}+\\alpha \\cdot \\frac{1}{I} \\sum_{i=1}^{I} \\frac{1}{\\operatorname{Pr}\\left(\\boldsymbol{\\tau}_{i} \\mid \\boldsymbol{\\theta}\\right)} \\frac{\\partial \\operatorname{Pr}\\left(\\boldsymbol{\\tau}_{i} \\mid \\boldsymbol{\\theta}\\right)}{\\partial \\boldsymbol{\\theta}} r\\left[\\boldsymbol{\\tau}_{i}\\right] . \\end{aligned} θ​←θ+α⋅∫∂θ∂​Pr(τ∣θ)r[τ]dτ=θ+α⋅∫Pr(τ∣θ)Pr(τ∣θ)1​∂θ∂Pr(τi​∣θ)​r[τ]dτ≈θ+α⋅I1​i=1∑I​Pr(τi​∣θ)1​∂θ∂Pr(τi​∣θ)​r[τi​].​Actor-Critic# Value function# 在Policy gradient中，每一轮训练我们都需要让模型“玩完整个游戏”，以此得到AnA_nAn​。如果能定义一个函数，直接能把AnA_nAn​算出来，或许能避免随机性带来的偏差。所以我们定义Value function，来计算cumulated discount reward。这里Value function Vθ(s)V^\\theta(s)Vθ(s)代表的是模型参数为θ\\thetaθ时某一个行为sss的平均cumulated discount value。 第一种得到Value function的方法是Monte-Carlo（MC）方法，这个方法让模型随机地进行多次任务，然后训练得到Value function。 第二种方法是Temporal-difference（TD），这种方法利用Vθ(s)=∑n=tNγn−trnV^\\theta(s)=\\sum_{n=t}^N \\gamma^{n-t} r_nVθ(s)=∑n=tN​γn−trn​，导出Vθ(st)=γVθ(st+1)+rtV^{\\theta}\\left(s_{t}\\right)=\\gamma V^{\\theta}\\left(s_{t+1}\\right)+r_{t}Vθ(st​)=γVθ(st+1​)+rt​，最后训练Value function使得Vθ(st)−γVθ(st+1)→rtV^{\\theta}\\left(s_{t}\\right)-\\gamma V^{\\theta}\\left(s_{t+1}\\right)\\rightarrow r_{t}Vθ(st​)−γVθ(st+1​)→rt​。 Value function的作用# 有了Value function后，我们可以改进\"Version 3\"中计算AnA_nAn​的方法为\"Version 3.5\"，将常数bbb改为这个步骤进行后的Value function，想法是如果这个行为是我们希望看到的，那么这个步骤的cumulated discount reward就会超过平均的cumulated discount reward，得到正的AnA_nAn​，反之得到负的AnA_nAn​。 但是Version 3.5中的方法没有考虑到某次行为之后的行为的随机性，这样可能导致误判这个行为的优劣，比如ata_tat​是我们希望看到的行为，但是由于采样的随机性at+1a_{t+1}at+1​或之后的行为可能是我们不希望看到的行为，那么可能就会误判ata_tat​为我们不希望看到的行为。为了避免这一点，我们将“玩一次游戏”产生的cumulated discount value改成由value function求出的平均cumulated discount value，这样可以避免随机性带来的影响。 Reward Shaping# 在很多任务中，只有任务结束的时候才会产生reward，而中间过程中没有reward（比如下围棋）。那么这时候我们需要在任务规则自带的reward（比如围棋胜负）之外额外构造一些reward。 比如https://openreview.net/forum?id=Hk3mPK5gg&amp;noteId=Hk3mPK5gg中让模型学习玩枪战游戏，除了游戏规则本身的死亡和存活会产生reward，可以设置掉血、开火、待在原地（防止模型一动不动）、存活（防止模型学会边缘ob）会得到负reward，捡到血包或弹药、移动可以得到正的reward。 但是reward shaping需要人对任务本身有足够充分的了解，否则很难设置合理的reward。 No Reward: Learning from Demonstration# 在很多场景下，可能连reward都没有，同时人类也不知道如何设置合适的reward，那么可以让模型通过模仿来进行学习。 但是如果直接使用监督学习，那么人没有“教”模型的操作可能模型是学不会的。比如自动驾驶任务中，人无法穷尽所有的极端情况下的操作，那么自动驾驶汽车可能在极端条件下无法正确反应，这可能是致命的。 我们可以使用Inverse Reinforcement Learning的方法来解决这个问题。这个方法是在每一轮训练中同时收集人类和模型面对某种任务时的行为{τ1^,τ2^,…,τK^}\\{ \\hat{\\tau_1},\\hat{\\tau_2},\\dots,\\hat{\\tau_K}\\}{τ1​^​,τ2​^​,…,τK​^​}和{τ1,τ2,…,τK}\\{ \\tau_1,\\tau_2,\\dots,\\tau_K\\}{τ1​,τ2​,…,τK​}，把人类行为和模型行为用于训练reward function，要求reward function对人类的行为给出的reward和大于对模型的行为给出的reward和，即∑R(τn^)&gt;∑R(τn)\\sum R(\\hat{\\tau_n})&gt;\\sum R(\\tau_n)∑R(τn​^​)&gt;∑R(τn​)，然后根据新的reward function去优化模型。 可以把IRL的训练类比GAN的训练，模型看作generator，reward function看成discriminator。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect12-Reinforcement-Learning/"},{"title":"李宏毅机器学习-lect13 Network Compression","text":"为什么需要压缩模型 Network Pruning Network Pruning的基本思想 Weight Pruning Neuron Pruning Why Pruning？ Knowledge Distillation Parameter Quantization Architecture Design Dynamic Computation 为什么需要压缩模型# 由于云端计算带来的延迟和隐私问题，有时我们需要把模型部署到运算资源有限的设备上（如手表、无人机、单片机等），那么此时需要对模型进行压缩，让模型用比较少的空间而保留原本的相似的性能。 Network Pruning# Network Pruning的基本思想# Network Pruning的方法是指删除掉模型的一些无关紧要的参数，然后再微调模型得到更小但效果接近的模型。每一次Pruning需要评估模型的各个参数的重要性（可以用绝对值，也可以用LLL中的方法衡量），然后移除不重要的参数，再进行微调。这个过程可以重复多次以获得足够小的模型。 Weight Pruning# Weight Pruning是指删除掉模型部分神经元上的一些权重，实现参数的减少，但是这种方法会让模型变成不规则的模型。这不仅难以实现，修剪后的模型还难以用GPU加速运算。 Neuron Pruning# Neuron Pruning的方法则是直接删除部分神经元，使得修建后的神经网络依然是规则的，这样比较容易实现，修剪的网络也便于用GPU加速。 Why Pruning？# 对于Pruning有效性的一种解释是“大乐透假说”（Lottery Ticket Hypothesis）。这个假说认为可以把一个大的模型看成很多小模型的组合，如果有某一个小模型的参数的初始化是良好的，那么相当于“中奖”，此时大的模型也能很好地优化。 这个假说有一些实验进行验证：先把一个大的模型随机初始化，然后进行训练，将Pruning后的模型拿出，分别对其进行随机初始化和与初始化大模型时相同的初始化。发现随机初始化的小模型难以训练，但是用和大模型初始化相同的初始化则可以使得小模型很好地被优化。 所以可以认为Pruning实际上是挑选出了“中奖”的小模型。 https://arxiv.org/abs/1810.05270中却报道了和大乐透假说相反的现象，下表中Scratch-E对应的一列是指把Pruning后的模型随机初始化的训练结果，而Scratch-B是在Scratch-E的基础上多训练几轮的结果，然后发现Scratch-B的结果可以优于Pruning后再微调的结果。 所以这篇文章认为大乐透假说可能只在部分条件下成立，比如learning rate比较小，或是模型通过weight pruning得到。 Knowledge Distillation# Knowledge Distillation，即知识蒸馏，这种方法是先训练一个比较大的teacher net，然后训练规模比较小的student net的产生和teacher net相近的输出。想法是直接让比较小的模型学习one hot vector的标准答案可能过于困难，所以只要求其输出接近teacher net，或许训练难度会比较低。 此外，为了进一步降低训练难度，有时还会将teacher net的softmax层输出中的yiy_iyi​改写为yi/Ty_i/Tyi​/T。这么做的意义是让teacher net中比较集中的输出变得比较平均，降低student net的训练难度。 Parameter Quantization# Parameter Quantization的想法是用比较少的位数来存储一个值。 第一种方法是Weight clustering，这个方法的想法是将模型中的weight进行聚类，将weight比较接近的部分归为一类，然后给每一类赋值（可以为这一类里所有weight的平均值）。然后可以用分类图+类与值的对应表的方式来储存模型中的所有参数。此外，还可以用Huffman Encoding或其他方法进一步减少存储参数所需空间。 第二种方法是将参数的位数减小，比如将64位浮点数改为8位，甚至整数，甚至只有1和-1两个值！ Architecture Design# Architecture Design是通过设计神经网络的结构来减少模型参数的。 以一般的CNN为例，如果输入、输出的feature map的channel数分别为2、4，卷积核大小为3x3，那么这个卷积层参数数为72。 但是如果我们将这个卷积层改为卷积核大小为3x3的Depth wise Convolution（channel数和输入feature map的channel数相同，每一个filter产生输出中的一个channel，这种卷积方式忽略了不同channel之间的关系），再让输出的feature map经过输出channel数为4的Pointwise Convolution（卷积核大小为1x1），最后输出的feature map大小是相同的，但是两个卷积层的参数和只有26。 对于输入、输出channel数为III、OOO，卷积核大小为k×kk\\times kk×k的卷积层，可以用参数数为k×k×Ik\\times k\\times Ik×k×I的Depthwise Convolution和参数数为I×OI\\times OI×O的Pointwise Convolution近似。这样可以将参数数压缩至1k×k\\frac{1}{k\\times k}k×k1​（OOO一般比较大，可以忽略）。 这种方法的思想类似于矩阵分解，即下图中的W=UVW=UVW=UV，但是这需要WWW的秩是小于KKK的，而对于一般的WWW没有这个限制。 Dynamic Computation# 我们还可以让模型根据实际情况动态地调整所用的模型的规模，就像可以有的手机可以通过电量自动调整屏幕亮度。 比如我们可以让模型动态地调整模型地深度，以图像识别为例，这种方法在多个卷积层后都接入输出层，我们可以通过硬件的具体情况（如电量）调整所用的特征提取部分的深度。训练时我们只需要将所有输出层的loss相加得到LLL，然后优化模型使得LLL最低即可。 类似地，我们也可以调整使用的神经网络的宽度。同样地，训练时我们只需要将所有宽度的模型（并不是训练多个模型，只是有的模型只用完整模型的部分宽度）得到的loss相加得到LLL，然后优化模型使得LLL最低即可。 更进一步，我们甚至让模型根据输入的复杂程度来改变模型的深度。比如比较容易分类的图片，我们可以只用模型中的一小部分，对于复杂的图片，则用模型中比较多的layer甚至全部layer。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect13-Network-Compression/"},{"title":"李宏毅机器学习-lect14 Life Long Learning","text":"对于AI的想象 灾难性遗忘 Life-Long v.s. Transfer Evaluation LLL实现方式 Selective Synaptic Plasticity Progressive Neural Networks Memory Reply LLL的不同场景 对于AI的想象# 对于AI，可能会想象今天教AI视觉，明天教AI语言……，在不断增加新的任务后，最终可以炼成无所不能的“天网”。这一过程称为Life Long Learning（LLL）。 灾难性遗忘# 但是实际上，AI在学习后面的任务后，可能会“遗忘”以前学会的任务。哪怕在最简单的手写数字识别任务上，先教会AI分辨带有噪声的手写数字，然后再教AI分辨一般的数字，都可以让AI在分辨含噪数字图片任务上的表现变差。 AI学习了新的任务而在旧的任务上表现变差的现象，称为灾难性遗忘。 但是实际上，如果AI一次性学习很多任务，可以发现AI在较多任务上都能有不错的表现，比如下图右侧显示的：让AI同时学习20各简单的NLP任务，在多个任务上都能有较好的表现，但是如果让AI依次学习各个任务，就会出现灾难性遗忘。 但是如果每次训练都让AI同时学习所有任务，则需要非常大量的存储空间用于存储多个任务的数据，同时训练过程会变得非常漫长。所以同时训练多个任务在某些场景下可能是不现实的。因此我们需要让AI依次学习不同的任务，还要避免灾难性遗忘。 Life-Long v.s. Transfer# Life Long Learning和Transfer Learning都涉及到旧任务和新任务的学习，但是二者关注点不同 Transfer Learning更关注学习旧任务后模型能否很好地学习新任务 Life Long Learning更关注学习新任务后是否遗忘旧任务 Evaluation# 评估Life Long Learning的效果（克服灾难性遗忘程度如何），一般可以哪里列出下图中的表格，第一行代表随机初始化模型在各个任务上的表现，第1-T行代表在训练模型做第1-T个任务后模型在第1-T个任务上的表现。 一种评估的方法是计算Accuracy：A=1T∑i=1TRT,iA=\\frac{1}{T}\\sum_{i=1}^T R_{T,i}A=T1​∑i=1T​RT,i​，这衡量的是在学习所有的任务后模型在所有任务上的表现。另一种评估的方法是计算\"Backward Transfer\"：B=1T−1∑i=1T−1(RT,i−Ri,i)B=\\frac{1}{T-1}\\sum_{i=1}^{T-1}(R_{T,i}-R_{i,i})B=T−11​∑i=1T−1​(RT,i​−Ri,i​)，这衡量的时第1至T-1个任务在T个任务都完成训练后的平均遗忘程度，一般来说这个值是负数。 LLL实现方式# Selective Synaptic Plasticity# 这种方法是一种Regularization based Approach的方法，在讨论这个方法之前，先了解以下为什么会产生灾难性遗忘。 在梯度下降时，我们会对模型的参数θ\\thetaθ求导，得到下降方向，但是由于两个任务的数据不同，求得的导数（下降方向）是不同的，直观来看就是error surface不同。可能对于task 2来说θ∗\\theta^*θ∗是其error surface的minima，但是对于task 1而言θ∗\\theta^*θ∗并不在error surface上很低的位置。 所以我们在让模型学习新的任务时，需要对loss function加上一些约束： L′(θ)=L(θ)+λ∑ibi(θi−θib)2 L'({\\theta})=L(\\theta)+\\lambda\\sum_{i}b_i(\\theta_i-\\theta_i^b)^2 L′(θ)=L(θ)+λi∑​bi​(θi​−θib​)2 上式中L(θ)L(\\theta)L(θ)代表原来的loss function，θib\\theta_i^bθib​是上一个任务训练后的参数θb\\theta^bθb的第i个元素，bib_ibi​是对θ\\thetaθ中每个元素的约束强度。一般而言，如果∂L∂θi\\frac{\\partial L}{\\partial \\theta_i}∂θi​∂L​越小，代表θi\\theta_iθi​对于loss的影响很小，这个参数可能是可以调整比较多的，所以bib_ibi​可以比较小。反之bib_ibi​的值应该比较大，给予这个参数比较强的约束。 如果bi=0b_i=0bi​=0，则就是无约束的情况，会导致灾难性遗忘。而如果bi=1b_i=1bi​=1，即直接用L2L_2L2​范数进行约束，那么可能会让模型的参数难以调整，从而使得模型难以学习新的任务。 还有一种约束模型优化方向的方法是https://arxiv.org/abs/1706.08840中提到的Gradient Episodic Memory (GEM)，这种方法的思想有点类似momentum，会在下个任务学习中，用一点之前的任务的数据，来计算出之前的优化方向，然后约束新的优化方向和旧的优化方向之间的夹角大小。 Progressive Neural Networks# Progressive Neural Networks是指对于相似的任务，可以共用网络的某些部分，只需要在最后的输出层有所不同即可，比如多个分类任务可以公用中间的特征提取部分，只需要不同的FC层即可。但是这样的方法可能会让模型十分臃肿，也会需要比较多的空间。 Memory Reply# Memory Reply是指在用task 1的数据训练模型做task 1的同时训练一个能生成task 1数据的生成器，然后就可以删除task 1的数据，在训练模型做task 2时，用task 2的数据和生成的task 1数据同时训练，然后再训练一个能生成task 1和task 2训练数据的生成器。后续依此类推。这种方法可以逼近Multi-task learning的训练效果，但也有人认为这不算一种LLL的方法。 LLL的不同场景# https://arxiv.org/abs/1904.07734中提到了LLL的三种不同场景（如Table 1所示） Table 1的描述可能比较抽象。具体对应到手写数字识别中，假设今天把十个数字两两分开，形成五个task，三种场景的具体示例参见Table 2。","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect14-Life-Long-Learning/"},{"title":"李宏毅机器学习-lect15 Meta Learning","text":"什么是Meta Learning？ 为什么需要Meta Learning？ Meta Learning的实现 定义function 确定loss 优化 Framework of Meta Learning ML v.s. Meta 不同 Goal Training Data Training and Testing Loss 相同 Meta Learning可以学到什么？ Initialization Optimizer Network Architecture Search (NAS) Data Augmentation Sample Reweighting Beyond Gradient Descent Applications 什么是Meta Learning？# 在之前的机器学习任务中，我们训练的都是下图中f∗f^*f∗的部分，即完成某项任务的模型，在训练之前我们需要定义一系列的超参数（学习率、初始化方式……）。而Meta Learning（元学习），则是去让模型学习自己去找出合适的超参数。 为什么需要Meta Learning？# Meta Learning的实现# 定义function# 确定loss# 优化# Framework of Meta Learning# ML v.s. Meta# 不同# Goal# Training Data# Training and Testing# Loss# 相同# Meta Learning可以学到什么？# Initialization# Optimizer# Network Architecture Search (NAS)# Data Augmentation# Sample Reweighting# Beyond Gradient Descent# Applications#","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect15-Meta-Learning/"},{"title":"李宏毅机器学习-各式各样的注意力机制","text":"Transformer的局限 Human Knowledge Local Attention / Truncated Attention Stride Attention Global Attention Clustering Learnable Patterns Representative key k,q first → v,k first New framework 目前里面提到的很多东西我自己也不太懂，不过先记下来，用到的时候也知道去哪找。 注： 横轴：运行速度 纵轴：表现 圆圈大小：内存占用 Transformer的局限#Transformer的一大局限是其运算效率，对于有N个vector的序列，由于每个query都需要和key作点积，那么就需要做次点积，如果序列非常长，那么运算成本可能是无法承受的。例如图像任务中，当图片中的每个pixel被看作向量，256x256大小的图片在一个self-attention层上的点积次数是。 Human Knowledge#Local Attention / Truncated Attention#在一些问题中，一个vector只需要把注意力放在临近的vector上，那么就可以使用Local Attention / Truncated Attention，只计算attention matrix中对角线临近区域的部分，其他设为0。这样的操作其实和CNN是类似的。 Stride Attention#如下图所示，每一个vector只关注与其一定间隔的向量，这个间隔是一个超参数。 Global Attention#Global attention是取出special token，然后用special token作为query、key和所有token计算attention。 实际上我们有时会采用多种注意力机制进行结合，如下图所示。 Clustering#Clustering方法是指把query和key进行聚类，然后只计算相近的query和key之间的attention，而差距较大的query和key之间的attention score被设置为0。 Learnable Patterns#在之前的方法中，我们还是依赖人对于问题的理解去判定哪些attention score是需要计算的，但是其实可以让模型自己学习出哪些attention score是需要计算的。 Representative key#因为attention map通常是低秩的，所以可以用有代表性的向量代表V、K（低秩近似），以减少运算量。 k,q first → v,k first#先计算value和key的积可以使得运算量降低，从而加速transformer运算。 对于不同的query，value和key的积是相同的，所以可以把先算出来（有点像一个模板），从而减少重复运算。 New framework#","link":"/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"快照压缩成像论文阅读记录","text":"综合 成像系统 算法 综述 End2End Deep unfolding Plug-and-Play 开个文档push自己读文章…… 综合# Snapshot Compressive Imaging: Theory, Algorithms, and Applications 首次阅读时间：2023.2.9-2023.2.10 主要内容：系统性回顾了2021年之前的快照压缩成像的硬件系统、数学模型、理论、算法 详细笔记：TBA 成像系统# To Be Add 算法# 综述# Recent advances of deep learning for spectral snapshot compressive imaging 首次阅读时间：2023.2.11 主要内容：回顾了2023年之前的光谱快照压缩成像算法（基于deep learning） 重点 深度先验+PnP是值得关注的方向 Transformer在光谱快照压缩成像中开始发挥其优势 End2End# Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction 首次阅读时间：2023.2.14 创新点： 首次将Transformer用于光谱SCI 光谱注意力机制（S-MSA，将各波长的二维图拉伸得到的向量看作token） 掩膜引导机制（使得S-MSA模块关注具有光谱图像中高保真度的区域） Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction 首次阅读时间：2023.2.27 创新点： SASM（光谱感知筛选机制，a图）：让模型关注信息丰富的区域（轻微降低性能但大大提高计算效率） SAH-MSA（光谱聚合散列自注意力机制，c图）：通过hashing，把每个patch中强相关的token归为一组，在组内进行self-attention。 Deep unfolding# Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging 首次阅读时间：2023.3.1 创新点： 第一个基于Transformer的深度展开方法-DAUHST（2022.5） 提出DAUF，从压缩图像和物理掩码中估计参数，然后使用这些参数来控制每次迭代，开始从退化模型指导迭代 提出Half-Shuffle Transformer (HST)，它可以同时捕获本地内容和远程依赖项，克服了CNN或local Transformer（捕获远程依赖有局限）以及global Transformer的缺陷（计算成本高） Plug-and-Play# Deep Image Prior 首次阅读时间：2023.2.11 创新点：用未经训练的网络作为先验，可用于去噪、超分辨等一系列任务 Snapshot temporal compressive microscopy using an iterative algorithm with untrained neural networks 首次阅读时间：2023.2.12 创新点：GAP-TV + deep image prior混合优化，用于视频SCI。也是首次将DIP用于SCI（2021年2月前没有将DIP用于SCI）。 Self-Supervised Neural Networks for Spectral Snapshot Compressive Imaging 首次阅读时间：2023.2.12 创新点：提出PnP-DIP/PnP-DIP-HSI算法，使用ADMM（+HSI先验）+DIP混合优化，用于光谱SCI。 Coded aperture compressive temporal imaging using complementary codes and untrained neural networks for high-quality reconstruction 首次阅读时间：2023.2.13 创新点：互补编码+UNN解码，用于视频SCI。互补编码和UNN也许是“天生一对”，那么或许也会有有别的编码方式和别的算法是“天作之合” SwinIR: Image Restoration Using Swin Transformer 首次阅读时间：2023.2.28 创新点：用Swin Transformer进行图像修复、超分辨等。流程：浅层信息提取+深层信息提取+重建","link":"/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"},{"title":"《Introduction to Programming with C++》读书笔记","text":"Contents of this book (Chapter 1-16) Chapter 2 const limits time(0) static_cast &lt;type&gt;(var) Chapter 3 bool rand and srand Chapter 4 Mathematical Functions Character Functions string string 基础 string 拼接 比较大小：按ASCII码依次比较 string读入 cout 进阶 setprecision(n) fixed showpoint setw(width) left and right simple file input and output write file read file Chapter 6 overloading functions function prototype default arguments constant reference parameters Chapter 7 Passing arrays to functions Preventing changes of array arguments in functions return an array Chapter 8 Passing two-dimensional arrays to functions Chapter 9: Object and Class Defining Classes and Creating Objects code UML notation Constructors Peculiarities Constructor initializer list Construction and Using Objects basics object size Separating Class Definition from implementation Inline Functions in Classes Data Field Encapsulation Chapter 10 Object-Oriented Thinking Passing Objects to Functions Array of Objects Instance and Static Members UML graph Using of static members/functions Constant Member Functions Object Composition Aggregation and composition UML graph Thinking in Objects Class Design Guidelines Chapter 11 Pointers and Dynamic memory management Using const with Pointers Passing Pointer Arguments in a Function Call Returning a Pointer from Functions Useful Array Functions Dynamic Persistent Memory Allocation Allocation Delete The this Pointer Destructors Copy Constructors Shallow Copy Deep Copy Chapter 15 Inheritance and Polymorphism Base Classes and Derived Classes UML graph Syntax Generic Programming Constructors and Destructors Calling Base Class Constructors Constructor and Destructor Chaining Redefining Functions Polymorphism Virtual Functions and Dynamic Binding The protected keyword Abstract Classes and Pure Virtual Functions Casting: static_cast versus dynamic_cast Example: GeometricObject, Circle, Rectangle Chapter 12 Templates, Vectors, Stacks Templates Basics Class Templates vector Class Chapter 14 Operator Overloading The Rational Class for this chapter Operator functions Overloading the Subscript Operator Overloading Augmented Assignment Operator Overloading the Unary Operator Overloading the ++ and -- Operator Overloading the Stream Operator friend Functions and friend Classes Overloading &lt;&lt; and &gt;&gt; Automatic Type Conversions Nonmember Functions for Overloading Operators Overloading the = Operators The Rational Class with Overloaded Function Operators Note Points for Operator Overloading Chapter 13 File Input and Output Text I/O Writing Data to File Reading Data from File Testing File Existence Testing End of File Letting the User Enter a File name Formatting Output getline, get and put fstream and File Open Modes Testing Stream States Binary I/O The write Function The read Function Random Access File Updating Files Chapter 16 Exception Handling Overview Advantages of Excpetion-Handling Exception Classes Custom Exception Classes Multiple Catches Exception Propagation Rethrowing Exceptions Exception Specification When to use Exceptions 💡 本文档记录了笔者2022年暑假啃《Introduction to Programming with C++》这本书自学C++基础的笔记 Contents of this book (Chapter 1-16)# Chapter 2# const# const double PI = 3.14159 // PI值不变，常量命名为全大写 limits# #include&lt;limits&gt; 内有各种变量类型的上限、下限 e.g INT_MAX = 2147483647 time(0)# 需要 #include&lt;ctime&gt; 显示当前时间距离1970.1.1 00:00:00 有多少秒 显示当前时间 static_cast &lt;type&gt;(var)# static_cast demo 123double x = 3.4;y = static_cast&lt;int&gt;(x);cout &lt;&lt; \"x=\" &lt;&lt; x &lt;&lt; \"y=\" &lt;&lt; y &lt;&lt; endl; Chapter 3# bool# bool demo 1234bool flag = true;bool b1 = -1.5;bool b2 = 0;bool b3 = 1.5; rand and srand# 123456789#include&lt;cstdlib&gt;int main(){ srand(seed); //更改rand的种子 rand(); // 产生一个随机数 return 0;} Chapter 4# Mathematical Functions# 12345678910111213141516171819#include &lt;cmath&gt;int main(){ double x, y; double a, b; x = 10.5; a = 2.1; b = 2.3; y = log(x); //y=ln(x) y = log10(x); //y=lg(x) y = ceil(x); // 向inf取整 y = floor(x); // 向-inf取整 y = max(a,b); y = min(a,b); return 0;} Character Functions# 12345678910111213141516171819#include &lt;cctype&gt;#include &lt;iostream&gt;int main(){ char ch = 'A'; cout &lt;&lt; isdigit(ch) &lt;&lt; endl; cout &lt;&lt; isalpha(ch) &lt;&lt; endl; cout &lt;&lt; isalnum(ch) &lt;&lt; endl; cout &lt;&lt; islower(ch) &lt;&lt; endl; cout &lt;&lt; isupper(ch) &lt;&lt; endl; cout &lt;&lt; isspace(ch) &lt;&lt; endl; cout &lt;&lt; tolower(ch) &lt;&lt; endl; //返回小写字母，但是是数值，得用static_cast&lt;char&gt;转换 cout &lt;&lt; toupper(ch) &lt;&lt; endl; //返回大写字母，但是是数值，得用static_cast&lt;char&gt;转换 cout &lt;&lt; static_cast&lt;char&gt;(tolower(ch)) &lt;&lt; endl; cout &lt;&lt; static_cast&lt;char&gt;(toupper(ch)) &lt;&lt; endl; return 0;} string# string 基础# 12345#include &lt;string&gt;string str = \"ABCD\";str.length();str.at(0);str[0]; string 拼接# 12string s3 = s2 + s1;s = s += \"ABC\"; 比较大小：按ASCII码依次比较# string读入# 123string str;cin &gt;&gt; str; //遇到空白会停止读取getline(cin, str, end); //直到end出现才停止，默认值为\"\\n\" cout 进阶# **#include &lt;iomanip&gt;** setprecision(n)# 12double number = 12.34567;cout &lt;&lt; setprecision(3) &lt;&lt; number &lt;&lt; endl; // 12.3 总共保留三位 The setprecision manipulator remains in effect until the precistion is changed! fixed# 123cout &lt;&lt; 232123434.357; //2.32123e+08cout &lt;&lt; fixed &lt;&lt; 232123434.357; // 232123434.357000 默认保留小数点后六位cout &lt;&lt; fixed &lt;&lt; setprecision(2) &lt;&lt; 345.123; //fixed 后的setprecision(2)指保留小数点后两位 showpoint# 1234cout &lt;&lt; setprecision(6);cout &lt;&lt; 1.23 &lt;&lt; endl; // 1.23cout &lt;&lt; showpoint &lt;&lt; 1.23 &lt;&lt; endl; // 1.23000cout &lt;&lt; showpoint &lt;&lt; 123 &lt;&lt; endl; // 123.000 setw(width)# 123456cout &lt;&lt; setw(8) &lt;&lt; \"C++\" &lt;&lt; setw(6) &lt;&lt; 101 &lt;&lt; endl;// ▢▢▢▢▢C++▢▢▢101cout &lt;&lt; setw(8) &lt;&lt; \"C++\" &lt;&lt; 101 &lt;&lt; endl; // setw(8)只控制\"C++\"输出，与101无关// ▢▢▢▢▢C++101cout &lt;&lt; setw(8) &lt;&lt; \"Programming\" &lt;&lt; endl; // 长度大于8// Programming left and right# 12345678cout &lt;&lt; setw(8) &lt;&lt; \"C++\" &lt;&lt; setw(6) &lt;&lt; 101 &lt;&lt; endl; // 默认右对齐// ▢▢▢▢▢C++▢▢▢101cout &lt;&lt; right;cout &lt;&lt; setw(8) &lt;&lt; 1.23 &lt;&lt; endl;// ▢▢▢▢1.23cout &lt;&lt; left;cout &lt;&lt; setw(8) &lt;&lt; 1.23 &lt;&lt; endl;// 1.23▢▢▢▢ simple file input and output# write file# 12345678910111213141516#include &lt;iostream&gt;#include &lt;fstream&gt;using namespace std;int main(){ ofstream output; // 实例化ofstream类对象 output.open(\"numbers.txt\");// 若不存在，创建；若存在，重写 output &lt;&lt; 95 &lt;&lt; \" \" &lt;&lt; 56; // 写文件 output.close(); // 关闭文件 return 0;} read file# 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;fstream&gt;using namespace std;int main(){ ifstream input; // 实例化对象 input.open(\"numbers.txt\"); // 打开文件，不存在会报错 int s1, s2; input &gt;&gt; s1; // 读文件 input &gt;&gt; s2; // 等效于 input &gt;&gt; s1 &gt;&gt; s2; input.close(); // 关闭文件 return 0;} Chapter 6# overloading functions# 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;cmath&gt;using namespace std;void fun(int x1, int x2);void fun(int x1, double x2);void fun(double x1, double x2);int main(){ // 根据输入参数的类型来确定调用哪个函数 fun(4, 5); fun(4, 5.4); fun(4.0, 5.4); return 0;}void fun(int x1, int x2){ cout &lt;&lt; \"0\" &lt;&lt; endl;}void fun(int x1, double x2){ cout &lt;&lt; \"1\" &lt;&lt; endl;}void fun(double x1, double x2){ cout &lt;&lt; \"2\" &lt;&lt; endl;} function prototype# 1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;void fun(int, int); // 声明时无序写出形参名void fun(double, double);int main(){ fun(1, 2); fun(1.0, 2.0); return 0;}void fun(int x1, int x2){ cout &lt;&lt; 0 &lt;&lt; endl;}void fun(double x1, double x2){ cout &lt;&lt; 1 &lt;&lt; endl;} default arguments# 1234567891011121314151617181920212223void fun0(int x = 1){ cout &lt;&lt; 0 &lt;&lt; endl;}void fun1(double x1 = 1, double x2 = 2){ cout &lt;&lt; 1 &lt;&lt; endl;}void fun2(double x1, double x2 = 1){ cout &lt;&lt; 2 &lt;&lt; endl;}fun0();fun0(2);fun1();fun1(3);//fun1( ,5);错误fun1(1, 2);fun2(3);fun2(2,4); constant reference parameters# 123456int max(const int&amp; num1, const int&amp; num2){ int result; result = num1 &gt; num2 ? num1 : num2; return result;} Chapter 7# Passing arrays to functions# 123456789101112131415161718192021222324#include &lt;iostream&gt;using namespace std;void m(int, int []);int main(){ int x = 1; int y[10]; y[0] = 1; m(x, y); cout &lt;&lt; x &lt;&lt; endl; cout &lt;&lt; y[0] &lt;&lt; endl; return 0;}void m(int number, int numbers[]){ number = 1000; numbers[0] = 555;} Preventing changes of array arguments in functions# 1234567891011121314151617181920212223242526void p(const int list[], int arraySize){ list[0] = 100;//这是错误的，list是常量}// 含有const数组形参函数嵌套调用错误示例void f1(int list[], int size){ // Doing something}void f2(const int list[], int size){ f1(list, size);//这是错误的，f1中形参list不是const}// 含有const数组形参函数嵌套调用正确示例void f1(const int list[], int size){ // Doing something}void f2(const int list[], int size){ f1(list, size);//这是正确的，f1中形参list是const} return an array# 12345678910// int[] fun(args);是错误的//正确:将数组地址传入，然后修改其数组元素void reverse(const int list[], int newList[], int size){ for(int i = 0, j = size - 1; i &lt; size; i++, j--) { newList[j] = list[i]; }} Chapter 8# Passing two-dimensional arrays to functions# 12345678910111213141516int sum(const int a[][COLUMN_SIZE], int rowSize) // 列数必须指定{ int total = 0; for(int i = 0; i &lt; rowSize; row++) { for(int j = 0; j &lt; COLUMN_SIZE; j++) { total += a[i][j]; } } return total;}int matrix[rowSize][COLUMN_SIZE];int y = sum(matrix, rowSize); // 调用 Chapter 9: Object and Class# Defining Classes and Creating Objects# code# 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;using namespace std;class Circle{public: // data field // if you don't use the public word, the visibility is private by default double radius; // constructors Circle() { radius = 1; } Circle(double newRadius) { radius = newRadius; } // function double getArea() { return radius * radius * 3.14159; }};int main(){ Circle circle1; Circle circle2(25); cout &lt;&lt; \"radius: \" &lt;&lt; circle1.radius &lt;&lt; \" area: \" &lt;&lt; circle1.getArea() &lt;&lt; endl; cout &lt;&lt; \"radius: \" &lt;&lt; circle2.radius &lt;&lt; \" area: \" &lt;&lt; circle2.getArea() &lt;&lt; endl; return 0;} When you define a custom class, capitalize the first letter of each word in a class name. UML notation# 12345678910// data fielddataFieldName: dataField Type// constructorClassName(parameterName: parameterType)// functionfunctionName(parameterName: parameterType): returnType// \"+\" means public Constructors# Peculiarities# Constructors must have the same name as the calss itself. Constructors do not have a return type–not even void! Constructors are invoked when an object is created. Constructors play the role of initializing objects. (A data field cannot be initialized when it is declared! like double radius = 5;) If we do not define a constructor, a no-arg constructor with an empty body is implicitly defined in the class. (Default constructor) Constructor initializer list# Data fields may be initialized in the constructor using an initializer list in the following syntax: 123456789101112ClassName(parameterList) : datafield1(value1), datafield2(value2){ // Additional statements if needed}// ExampleCircle::Circle() : radius(1){ } Using an initializer list is necessary to initialize object data fields that don’t have a no-arg constructor. Construction and Using Objects# basics# 1234ClassName ObjectName;objectName.dataField;objectName2 = objectName1; // deep copyobjectName.function(arguments); object size# Data are physically stored in an object, but functions are not. Since functions are shared by all objects of the same class, the compiler creates just one copy for sharing. 1234567891011121314151617181920212223242526#include &lt;iostream&gt;using namespace std;Class Circle{public: double radius; Circle() { radius = 1; } getArea() { return radius * radius * 3.14159; }};int main(){ Circle circle1; cout &lt;&lt; sizeof(circle1); return 0;} Separating Class Definition from implementation# Class Definition: list all the data fields, constructor prototypes, and function prototypes. Class implementation implements the constructors and functions. These are in two files, but both files should have the same name but different extension names(.h for data fields and .cpp for implementations). 1234567891011121314#ifndef CIRCLE_H#define CIRCLE_Hclass Circle{public: double radius; Circle(); Circle(double); double getArea();};#endif 12345678910111213141516#include \"Circle.h\"Circle::Circle() //\"Circle::\" tells the functions are defined in Circle class{ radius = 1;}Circle::Circle(double newRadius){ radius = newRadius;}double Circle::getArea(){ return radius * radius * 3.14159;} 1234567891011121314#include &lt;iostream&gt;#include \"Circle.h\"using namespace std;int main(){ Circle circle1; Circle circle2(5.0); cout &lt;&lt; circle1.radius &lt;&lt; circle1.getArea() &lt;&lt; endl; cout &lt;&lt; circle2.radius &lt;&lt; circle2.getArea() &lt;&lt; endl; return 0;} Inline Functions in Classes# short functions are good candidates for inline functions. 12345678910111213141516class A{public: A() { //Do something } double f1();}inline double A::f1(){ //Do something //return a number} Data Field Encapsulation# Making data fields private protects data and makes the class easy to maintain. Data may be tampered with. Public data may makes the class difficult to maintain and vulnerable to bugs. 123456789101112131415161718class Circle{public: // public first Circle(); Circle(double); double getArea(); double getRadius(); // enable user access to radius void setRadius(double); // enable user to set radius /* signature: returnType getPropertyName() for most type bool isPropertyName() for bool return type void setPropertyName(dataType propertyValue) */ private: double radius;} Chapter 10 Object-Oriented Thinking# Passing Objects to Functions# 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include \"Circle.h\"using namespace std;void passObjByValue(Circle c){ c.setRadius(10);}void passObjByReference(Circle&amp; c){ c.setRadius(10);}int main(){ Circle circle1(5.0); Circle circle2(5.0); passObjByValue(circle1); passObjByReference(circle2); cout &lt;&lt; circle1.getRadius() &lt;&lt; \" \" &lt;&lt; circle1.getArea() &lt;&lt; endl; cout &lt;&lt; circle2.getRadius() &lt;&lt; \" \" &lt;&lt; circle2.getArea() &lt;&lt; endl; return 0;} Passing by reference is preferred, because it takes time and additional memory space to pass by value. Array of Objects# 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include \"Circle.h\"using namespace std;void setArrayRadius(Circle[], int);void printArray(Circle[], int);int main(){ const int SIZE = 10; Circle circleArray[SIZE]; setArrayRadius(circleArray, SIZE); printArray(circleArray, SIZE); return 0;}void setArrayRadius(Circle circleArray[], int size){ for(int i = 0; i &lt; size ; i++) { circleArray[i].setRadius(i+1); }}void printArray(Circle circleArray[], int size){ for(int i = 0; i &lt; size; i++) { cout &lt;&lt; circleArray[i].getRadius() &lt;&lt; \" \" &lt;&lt; circleArray[i].getArea() &lt;&lt; endl; }} Instance and Static Members# A static variable is shared by all objects of the class A static function cannot access instance members of the class UML graph# Using of static members/functions# 12345678910111213141516171819#ifndef CIRCLE_H#define CIRCLE_Hclass Circle{public: Circle(); Circle(double); double getRadius(); void setRadius(double); double getArea(); static int getNumberOfObjects(); // using static functions to get static members private: double radius; static int numberOfObjects;};#endif 1234567891011121314151617181920212223242526272829303132333435#include \"Circle.h\"int Circle::numberOfObjects = 0; // define static members in cpp filesCircle::Circle() //\"Circle::\" tells the functions are defined in Circle class{ radius = 1; numberOfObjects++; // the static members can be accessed in constructors}Circle::Circle(double newRadius){ radius = newRadius; numberOfObjects++;}double Circle::getRadius(){ return radius;}void Circle::setRadius(double newRadius){ radius = newRadius &gt;=0 ? newRadius : -newRadius;}double Circle::getArea(){ return radius * radius * 3.14159;}int Circle::getNumberOfObjects(){ return numberOfObjects;} 123456789101112131415161718192021222324#include &lt;iostream&gt;#include \"Circle.h\"using namespace std;int main(){ cout &lt;&lt; Circle::getNumberOfObjects() &lt;&lt; endl; /* static members can be accessed before the object been created. static functions can be invoked before the object been created. */ Circle circle1; cout &lt;&lt; circle1.getNumberOfObjects() &lt;&lt; endl; Circle circle2(4.0); cout &lt;&lt; circle2.getNumberOfObjects() &lt;&lt; endl; /* static functions can be invoked by ClassName::function() or ObjectName.function() */ return 0;} If a variable or a function is not dependent on a specific instance of the class, it should be a static variable or static function Using ClassName::functionName(args) to invoke a static function and ClassName::staticVariable to access static variables improves readability. Constant Member Functions# A const function should not change the value of any data fields in the object. 12345678910111213141516171819#ifndef CIRCLE_H#define CIRCLE_Hclass Circle{public: Circle(); Circle(double); double getRadius() const; void setRadius(double); double getArea() const; static int getNumberOfObjects(); private: double radius; static int numberOfObjects;};#endif 1234567891011121314151617181920212223242526272829303132333435#include \"Circle.h\"int Circle::numberOfObjects = 0;Circle::Circle() //\"Circle::\" tells the functions are defined in Circle class{ radius = 1; numberOfObjects++;}Circle::Circle(double newRadius){ radius = newRadius; numberOfObjects++;}double Circle::getRadius() const{ return radius;}void Circle::setRadius(double newRadius){ radius = newRadius &gt;=0 ? newRadius : -newRadius;}double Circle::getArea() const{ return radius * radius * 3.14159;}int Circle::getNumberOfObjects(){ return numberOfObjects;} 123456789101112131415161718192021222324#include &lt;iostream&gt;#include \"Circle.h\"using namespace std;void printCircle(const Circle&amp; c){ cout &lt;&lt; c.getRadius() &lt;&lt; \" \" &lt;&lt; c.getArea() &lt;&lt; endl;}/*A const member function should not change the value of any data fields in the object,so instance get function should always be defined as a constant member function.If a function does not change the object being passed like printCircle, you should define the parameters constant using const keyword, and member functions being invoked in the function should be const member functions. */int main(){ Circle circle1; printCircle(circle1); return 0;} Object Composition# An object can contain another object. The relationship between the two is called composition. Aggregation and composition# The owner object is called an aggregating object and its class an aggregating class. The subject object is called an agggregated object and its class an aggregated class. An object may be owned by several other aggregating objects. If an object is exclusively owned by an aggregating object, the relationship between the object and its aggregation object is referred to as composition. UML graph# Filled diamond is attached to an aggregating class to denote the composition relationship. An empty diamond is attached to an aggregating class to denote the aggregation relationship. 1234567891011121314151617class Name{ // something}class Address{ // something}class Student{private: Name name; Adddress address; // something} **Aggregation may exist between objects of the same class. ** 12345class Person{private: Person teammates[3];} Thinking in Objects# The procedural paradigm focuses on designing functions. The object oriented paradigm couples data and functions together into objects. Software design using the object-oriented paradigm focuses on objects and operations on objects. Class Design Guidelines# Cohesion A class describe a single entity, all the class operations should logically fit together to support a coherent purpose. Consistency: Follow standard programming style and naming conventions. Choose informative names for classes, data fields, and functions. Placing data declaration after the functions and place constructors before functions. Choose the same names for similar operations using function overloading. Consistently provide a public no-arg constructor. Encapsulation Using private modifier to hide data. Clarity A class should have a clear contract that is easy to explain and understand. Completeness A class should provide a variety of ways for customization through properties and functions for wide range of applications. Instance vs. Static Chapter 11 Pointers and Dynamic memory management# Using const with Pointers# A constant pointer points to a constant memory location, but the actual value in the memory location can be changed. 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;int main(){ double radius = 5; double length = 5; double* const p1 = &amp;radius; // constant pointer const double* p2 = &amp;length; // constant data radius = 4; length = 6; // the value can be modify *p1 = 8; // p1 = p2 is wrong cout &lt;&lt; *p1 &lt;&lt; endl; cout &lt;&lt; *p2 &lt;&lt; endl; p2 = p1; // *p2 = *p1 is wrong cout &lt;&lt; *p2 &lt;&lt; endl; cout &lt;&lt; \"ok\" &lt;&lt; endl; return 0;} Passing Pointer Arguments in a Function Call# A pointer argument can be passed by value or by reference 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;iostream&gt;using namespace std;void swap1(int, int);void swap2(int&amp;, int&amp;);void swap3(int*, int*);void swap4(int* &amp;, int* &amp;);int main(){ int num1 = 1; int num2 = 2; int num3 = 3; int num4 = 4; int* p1 = &amp;num3; int* p2 = &amp;num4; swap1(num1, num2); cout &lt;&lt; num1 &lt;&lt; \" \" &lt;&lt; num2 &lt;&lt; endl; swap2(num1, num2); cout &lt;&lt; num1 &lt;&lt; \" \" &lt;&lt; num2 &lt;&lt; endl; swap3(p1, p2); cout &lt;&lt; *p1 &lt;&lt; \" \" &lt;&lt; *p2 &lt;&lt; endl; cout &lt;&lt; p1 &lt;&lt; \" \" &lt;&lt; p2 &lt;&lt; endl; *p1 = 3; *p2 = 4; swap4(p1, p2); cout &lt;&lt; *p1 &lt;&lt; \" \" &lt;&lt; *p2 &lt;&lt; endl; cout &lt;&lt; p1 &lt;&lt; \" \" &lt;&lt; p2 &lt;&lt; endl; return 0;}void swap1(int num1, int num2) // pass by value{ int t = num1; num1 = num2; num2 = t;}void swap2(int&amp; num1, int&amp; num2) // pass by reference{ int t = num1; num1 = num2; num2 = t;}void swap3(int* p1, int* p2) // pass two pointers by value{ int t = *p1; *p1 = *p2; *p2 = t;}void swap4(int* &amp;p1, int* &amp;p2) // pass two pointers by reference{ int *pt = p1; p1 = p2; p2 = pt;} Returning a Pointer from Functions# 1234567891011int *reverse(int* list, int size){ for(int i = 0, j = size - 1; i &lt; j; i++, j--) { int temp = list[j]; list[j] = list[i]; list[i] = temp; } return list;} Useful Array Functions# 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;void printArray(const int* list, int size){ for(int i = 0; i &lt; size; i++) { cout &lt;&lt; *(list + i) &lt;&lt; \" \"; } cout &lt;&lt; endl;}int main(){ const int SIZE = 6; int list[SIZE] = {1, 2, 3, 4, 5, 6}; random_shuffle(list, list + SIZE); printArray(list, SIZE); sort(list, list + SIZE); printArray(list, SIZE); int* pmin = NULL; int* pmax = NULL; int* p = NULL; pmin = min_element(list, list + SIZE); cout &lt;&lt; *pmin &lt;&lt; \" \" &lt;&lt; pmin - list &lt;&lt; endl; pmax = max_element(list, list + SIZE); cout &lt;&lt; *pmax &lt;&lt; \" \" &lt;&lt; pmax - list &lt;&lt; endl; int key1 = 4; int key2 = 10; p = find(list, list + SIZE, key1); cout &lt;&lt; p - list &lt;&lt; endl; p = find(list, list + SIZE, key2); cout &lt;&lt; p - list &lt;&lt; endl; return 0;} Dynamic Persistent Memory Allocation# A new operator can be used to create persistent memory at runtime for primitive type values, arrays, and objects. C++ allocates local variables in the stack, but the memory allocated by the new operator is in an area of memory called heap. The heap memory remains available until you explicitly free it or the program terminates. Allocation# 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;using namespace std;int* reverse1(const int* list, const int SIZE) // wrong reverse{ int result[SIZE]; for (int i = 0, j = size - 1; i &lt; size; i++, j--) { result[j] = list[i]; } return result; /* example11-8.cpp:15:9: warning: address of local variable 'result' returned [-Wreturn-local-addr] */}int* reverse2(const int* list, const int SIZE) // right reverse{ int* list = new int[SIZE]; for (int i = 0, j = size - 1; i &lt; size; i++, j--) { result[j] = list[i]; } return result;}int main(){ const int SIZE = 6; int list[SIZE] = {1, 2, 3, 4, 5, 6}; int *list1 = reverse1(list, SIZE); int *list2 = reverse2(list, SIZE); printArray()} **int* p = new int(4);**means allocate memory space for an int variable initialized to 4 **int* p = new int[size];**means allocate dynamic array For creating dynamic objects, you can use **ClassName* pObject = new ClassName();** or **ClassName* pObject = new ClassName;** for no-arg constructor. **ClassName* pObject = new ClassName(arguments);** for creating an object using the constructor with arguments. 1234string* p = new string(\"aaaaaaa\");cout &lt;&lt; (*p).substr(0, 3) &lt;&lt; endl;cout &lt;&lt; p-&gt;substr(0, 4) &lt;&lt; endl;cout &lt;&lt; p-&gt;length() &lt;&lt; endl; Delete# **To explicitly free the memory creater by the new operator, use the delete operator for the pointer. Like ****delete p;****or ****delete [] list;** Use the **delete** keyword only with the pointer that points to the memory created by the **new** operator. After the memory pointed by a pointer is freed, the value of the pointer becomes undefined. Do not apply the dereference operator * on dangling pointer. Do not reassign a pointer before deleting the memory to which it points. It may cause memory leak. 1234567int* p = new int;*p = 45;p = new int;/*The original memory space that holds value 45 is not accessible.This memory can't be accessed and can't be deleted. This is memory leak.*/ The this Pointer# The this pointer points to the calling object itself 123456789Circle::Circle(double radius){ this-&gt;radius = radius;}void Circle::setRadius(double radius){ this-&gt;radius = radius &gt;= 0 ? radius : -radius;} Destructors# Every class has a destructor, which is called automatically when an object is deleted. Every class has a default destructor if the destructor is not explicitly defined. Destructors are named the same as class with a destructor defined, but put a tilde character(~) in the front Destructors have no return type and no arguments. 123456~Circle();Circle::~Circle(){ numberOfObjects--;} Copy Constructors# Every class has a copy constructor, which is used to copy objects. **ClassName(const ClassName&amp;)** A default copy constructors is provided for each class implicitly, if it is not defined explicitly. The default copy constructir simply copied each data field in one object to its counterpart in the other object. Shallow Copy# 123456789101112131415161718192021222324252627#ifndef COURSE_H#define COURSE_H#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;algorithm&gt;using namespace std;class Course{public: Course(); Course(const string&amp; CourseName, int capacity); ~Course(); string getCourseName() const; void addStudentName(const string&amp; name); void dropStudent(const string&amp; name); string* getStudents() const; int getNumberOfStudents() const;private: string CourseName; string* students; int numberOfStudents; int capacity;};#endif 123456789101112131415161718#include \"Course.h\"int main(){ Course course1(\"C++\", 10); Course course2(course1); course1.addStudentName(\"San Zhang\"); course2.addStudentName(\"Si Li\"); string* p1 = course1.getStudents(); string* p2 = course2.getStudents(); cout &lt;&lt; *p1 &lt;&lt; endl; cout &lt;&lt; *p2 &lt;&lt; endl; return 0;} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include \"Course.h\"Course::Course(){ CourseName = \"undefined\"; numberOfStudents = 0; capacity = 0;}Course::Course(const string&amp; CourseName, int capacity){ this-&gt;CourseName = CourseName; this-&gt;capacity = capacity; numberOfStudents = 0; students = new string[capacity];}Course::~Course(){ delete[] students;}string Course::getCourseName() const{ return CourseName;}void Course::addStudentName(const string&amp; name){ if (numberOfStudents &lt; capacity) { students[numberOfStudents++] = name; } else { cout &lt;&lt; \"Can not add more students!\" &lt;&lt; endl; }}void Course::dropStudent(const string&amp; name){ if (name.length() == 0) { cout &lt;&lt; \"The lenth of name must greater than zero!\" &lt;&lt; endl; return; } string* p = find(students, students + capacity, name); int index = p - students; if (index &gt;= 0 &amp;&amp; index &lt; capacity) { for (int i = index; i &lt; numberOfStudents - 1; i++) { students[i] = students[i + 1]; } students[numberOfStudents - 1].clear(); numberOfStudents--; } else { cout &lt;&lt; name &lt;&lt; \" do not in the list of students in \" &lt;&lt; CourseName &lt;&lt; \"!\" &lt;&lt;endl; }}string* Course::getStudents() const{ return students;}int Course::getNumberOfStudents() const{ return numberOfStudents;} The program will display “Si Li” twice and it will cause error before the space for students in course1 is freed (students of course2 have no space!) Deep Copy# 12345678910111213Course(const Course&amp;);Course::Course(const Course&amp; course){ CourseName = course.CourseName; numberOfStudents = course.numberOfStudents; capacity = course.capacity; students = new string[capacity]; for (int i = 0; i &lt; numberOfStudents; i++) { students[i] = course.students[i]; }} Chapter 15 Inheritance and Polymorphism# Base Classes and Derived Classes# **Inheritance enables you to define a general class (i.e., a base class) and later extend it to more specialized classes (i.e., derived classes). ** UML graph# Syntax# **class derivedClassName : public baseClassName** All the public member in bass class are inherited in derived class But the private data fields can not be accessed in the derived class. You can only use get or set functions to read or modify them. A derived class and its base class must have the is-a relationship. Not all “is-a” relationships should be modeled using inheritance. For class A to extend class B, A should contain more detailed information than B. C++ allows you to derive a derived class from several classes. This capability is known as multiple inheritance. Generic Programming# An object of a derived class can be passed wherever an object of a base type parameter is required. Thus a function can be used generically for a wide range of object arguments. This is known as generic programming. 123456789101112void display(BaseClass &amp;obj){ // something }BaseClass obj1;DerivedClass1 obj2;DerivedClass2 obj3;display(obj1);display(obj2);display(obj3); Constructors and Destructors# Calling Base Class Constructors# 123456789101112131415161718192021// function 1// invoke no-arg constructor of the base classDerivedClass(parameterList) : BaseClass() //bracket after BaseClass can be omitted{ // something}// function 2// invoke constructor with specific arguments of the base classDerivedClass(parameterList) : BaseClass(argumentList){ setData1(data1); // something}// function 3// equals to function 2DerivedClass(parameterList) : BaseClass(argumentList), data1(data1){ // something} Constructor and Destructor Chaining# **The constructor of a derived class first calls its base class’s constructor before it executes its own code. The destructor of a derived class executes its own code then automatically calls its base class’s destructor. ** **Cautions: ** If a class is designed to be extended, it is better to provide a no-arg constructor to avoid programming errors. 123456789101112131415class Fruit{public: Fruit(int id) { }};class Apple: public Fruit // no-arg constructor has not be defined in Fruit Class!{public: Apple() { }}; If the base class has a customized copy constructor and assignment operator, you should customize these in the derived classes to ensure that the data fields in the base class are properly copied. 12345678910111213Child::Child(const Child&amp; object): Parent(object){ // Write the code for copying data fields in Child}/*The code for the assignment operator in Child would typically look like this:Child&amp; Child::operator=(const Child&amp; object)*/{ Parent::operator(object); // Write the code for copying data fields in Child} Redefining Functions# A function defined in the base class can be redefined in the derived classes. To redefine a base class’s function in the derived class, you need to add the function’s prototype in the derived class’s header file, and provide a new implementation for the function in the derived class’s implementation file. Polymorphism# Polymorphism means that a variable of a supertype can refer to a subtype object. For example, every circle is a geometric object, but not every geometric object is a circle. Therefore, you can always pass an instance of a derived class to a parameter of its base class type. Virtual Functions and Dynamic Binding# A function can be implemented in several classes along the inheritance chain. Virtual functions enable the system to decide which function is invoked at runtime based on the actual type of the object. The capability of determining which function to invoke at runtime is known as dynamic binding. 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include \"GeometricObject.h\"#include \"DerivedCircle.h\"#include \"DerivedRectangle.h\" using namespace std;void displayGeometricObject(const GeometricObject&amp; g){ cout &lt;&lt; g.toString() &lt;&lt; endl;}int main(){ GeometricObject geometricObject; displayGeometricObject(geometricObject); Circle circle(5); displayGeometricObject(circle); Rectangle rectangle(4, 6); displayGeometricObject(rectangle); return 0;}/*if toString function is defined in GeometricObject.h like:string toString() const;output: Geometric Object Geometric Object Geometric Object if toString function is defined in GeometricObject.h like:virtual string toString() const;output: Geometric Object Circle Object Rectangle Object*/ **To enable dynamic binding for a function, you need to do two things: ** The function must be defined virtual in the base class. The variable that references the object must be passed by reference or passed as a pointer in the virtual function Note Static Binding: The declared type of the variable decides which function to match at compile time. The compiler finds a matching function according to parameter type, number of parameters, and order of the parameters at compile time. Dynamic Binding: Dynamically binds the implementation of the function at runtime, decided by the actual class of the object referenced by the variable. If a function defined in a base class needs to be redefined in its derived classes, you should define it virtual to avoid confusions and mistakes. On the other hand, if a function will not be redefined, it is more efficient not to declare it virtual, because more time and system resource are required to bind virtual functions dynamically at runtime. The protected keyword# Keyword Who can access? public inside the class/ friend functions/ friend classes protected inside the class/ friend functions/ friend classes/ derived classes private inside the class/ any other class Abstract Classes and Pure Virtual Functions# **An abstract class cannot be used to create objects. An abstract class can contain abstract functions, which are implemented in concrete derived classes. ** For example, **getArea** function can not be implemented in GeometricObject class, but can be implemented in the Circle class or Rectangle class. So we can define **getArea** in GeometricObject class as an abstract funcions(Pure Virtual Functions). Then GeometricObject class is a abstract class which cannot be used to create objects. syntax: **virtual type functionName(parameterList) = 0;** The “= 0” notation indicates that this function is a pure virtual function. A pure virtual function does not have a body or implementation in the base class. Casting: static_cast versus dynamic_cast# The dynamic_cast operator can be used to cast an object to its actual type at runtime. dynamic_cast can be performed only on the pointer or the reference of a polymorphic type; i.e., the type contains a virtual function. dynamic_cast can be used to check whether casting is performed successfully at runtime. static_cast is performed at compile time. 12345678// upcastingBaseClass* p = new DerivedClass; // can be performed implicitly.// downcastingDerivedClass* p1;p1 = static_cast&lt;DerivedClass*&gt;(p);// orp1 = dynamic_cast&lt;DerivedClass*&gt;(p); Example: GeometricObject, Circle, Rectangle# 12345678910111213141516171819202122232425#ifndef GEOMETRICOBJECT_H#define GEOMETRICOBJECT_H#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;class GeometricObject{public: GeometricObject(); GeometricObject(const string&amp; color, bool filled); virtual ~GeometricObject(); string getColor() const; void setColor(const string&amp; color); bool isFilled() const; void setFilled(bool filled); virtual string toString() const; //virtual double getArea() const = 0; //virtual double getPerimeter() const = 0;private: string color; bool filled;};#endif 12345678910111213141516171819202122232425262728293031323334353637383940414243#include \"GeometricObject.h\"GeometricObject::GeometricObject(){ color = \"white\"; filled = false;}GeometricObject::GeometricObject(const string&amp; color, bool filled){ this-&gt;color = color; this-&gt;filled = filled;}GeometricObject::~GeometricObject(){ cout &lt;&lt; \"Geometric Object destroyed!\" &lt;&lt; endl;}string GeometricObject::getColor() const{ return color;}void GeometricObject::setColor(const string&amp; color){ this-&gt;color = color;}bool GeometricObject::isFilled() const{ return filled;}void GeometricObject::setFilled(bool filled){ this-&gt;filled = filled;}string GeometricObject::toString() const{ return \"Geometric Object\";} 1234567891011121314151617181920212223#ifndef CIRCLE_H#define CIRCLE_H#include \"GeometricObject.h\"class Circle : public GeometricObject{public: Circle(); Circle(double radius); Circle(double radius, const string&amp; color, bool filled); ~Circle(); double getRadius() const; void setRadius(double); double getArea() const; double getPerimeter() const; double getDiameter() const; string toString() const;private: double radius;};#endif 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include \"Circle.h\"Circle::Circle(){ radius = 1;}Circle::Circle(double radius){ setRadius(radius);}Circle::Circle(double radius, const string&amp; color, bool filled){ setRadius(radius); setColor(color); // this-&gt;color = color; Illgal since color is private in the base Class setFilled(filled);}Circle::~Circle(){ cout &lt;&lt; \"Circle Object destroyed!\" &lt;&lt; endl;}double Circle::getRadius() const{ return radius;}void Circle::setRadius(double radius){ this-&gt;radius = radius &gt;= 0 ? radius : -radius;}double Circle::getArea() const{ return radius * radius * 3.14159;}double Circle::getPerimeter() const{ return 2 * 3.14159 * radius;}double Circle::getDiameter() const{ return 2 * radius;}string Circle::toString() const{ return \"Circle Object\";} 12345678910111213141516171819202122232425#ifndef RECTANGLE_H#define RECTANGLE_H#include \"GeometricObject.h\"class Rectangle : public GeometricObject{public: Rectangle(); Rectangle(double width, double height); Rectangle(double width, double height, const string&amp; color, bool filled); ~Rectangle(); double getWidth() const; void setWidth(double); double getHeight() const; void setHeight(double); double getArea() const; double getPerimeter() const; string toString() const;private: double width; double height;};#endif 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include \"Rectangle.h\"Rectangle::Rectangle(){ width = 1; height = 1;}Rectangle::Rectangle(double width, double height){ setWidth(width); setHeight(height);}Rectangle::Rectangle(double width, double height, const string&amp; color, bool filled) : GeometricObject(color, filled){ setWidth(width); setHeight(height);}Rectangle::~Rectangle(){ cout &lt;&lt; \"Rectangle Object destroyed!\" &lt;&lt; endl;}double Rectangle::getWidth() const{ return width;}void Rectangle::setWidth(double width){ this-&gt;width = width &gt;= 0 ? width : -width;}double Rectangle::getHeight() const{ return height;}void Rectangle::setHeight(double height){ this-&gt;height = height &gt;= 0 ? height : -height;}double Rectangle::getArea() const{ return width * height;}double Rectangle::getPerimeter() const{ return 2 * (width + height);}string Rectangle::toString() const{ return \"Rectangle Object\";} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;iostream&gt;#include &lt;typeinfo&gt;#include \"GeometricObject.h\"#include \"Circle.h\"#include \"Rectangle.h\"using namespace std;void displayType1(const GeometricObject obj){ cout &lt;&lt; obj.toString() &lt;&lt; endl;}void displayType2(const GeometricObject&amp; obj){ cout &lt;&lt; obj.toString() &lt;&lt; endl;}void displayType3(const GeometricObject* obj){ cout &lt;&lt; obj-&gt;toString() &lt;&lt; endl;}void displayObj(GeometricObject &amp;obj){ GeometricObject* p = &amp;obj; Circle* p1 = dynamic_cast&lt;Circle*&gt;(p); Rectangle* p2 = dynamic_cast&lt;Rectangle*&gt;(p); cout &lt;&lt; typeid(*p).name() &lt;&lt; endl; if (p1 != NULL) { cout &lt;&lt; \"radius: \" &lt;&lt; p1-&gt;getRadius() &lt;&lt; endl; } if (p2 != NULL) { cout &lt;&lt; \"width: \" &lt;&lt; p2-&gt;getWidth() &lt;&lt; endl; cout &lt;&lt; \"height: \" &lt;&lt; p2-&gt;getHeight() &lt;&lt; endl; }}int main(){ GeometricObject obj; obj.setColor(\"red\"); obj.setFilled(true); cout &lt;&lt; obj.toString() &lt;&lt; endl &lt;&lt; \"color: \" &lt;&lt; obj.getColor() &lt;&lt; endl &lt;&lt; \"filled: \" &lt;&lt; obj.isFilled() &lt;&lt; endl; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; Circle circle(5); cout &lt;&lt; circle.toString() &lt;&lt; endl &lt;&lt; \"radius: \" &lt;&lt; circle.getRadius() &lt;&lt; endl &lt;&lt; \"area:\" &lt;&lt; circle.getArea() &lt;&lt; endl &lt;&lt; \"Perimeter: \" &lt;&lt; circle.getPerimeter() &lt;&lt; endl &lt;&lt; \"color: \" &lt;&lt; circle.getColor() &lt;&lt; endl &lt;&lt; \"filled: \" &lt;&lt; circle.isFilled() &lt;&lt; endl; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; Rectangle rectangle(2, 3, \"blue\", true); cout &lt;&lt; rectangle.toString() &lt;&lt; endl &lt;&lt; \"width: \" &lt;&lt; rectangle.getWidth() &lt;&lt; endl &lt;&lt; \"height: \" &lt;&lt; rectangle.getHeight() &lt;&lt; endl &lt;&lt; \"area:\" &lt;&lt; rectangle.getArea() &lt;&lt; endl &lt;&lt; \"Perimeter: \" &lt;&lt; rectangle.getPerimeter() &lt;&lt; endl &lt;&lt; \"color: \" &lt;&lt; rectangle.getColor() &lt;&lt; endl &lt;&lt; \"filled: \" &lt;&lt; rectangle.isFilled() &lt;&lt; endl; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; displayType1(obj); displayType1(circle); displayType1(rectangle); displayType2(obj); displayType2(circle); displayType2(rectangle); displayType3(&amp;obj); displayType3(&amp;circle); displayType3(&amp;rectangle); cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; cout &lt;&lt; \"Area: \" &lt;&lt; circle.getArea() &lt;&lt; endl; cout &lt;&lt; \"Perimeter: \" &lt;&lt; circle.getPerimeter() &lt;&lt; endl; cout &lt;&lt; \"Area: \" &lt;&lt; rectangle.getArea() &lt;&lt; endl; cout &lt;&lt; \"Perimeter: \" &lt;&lt; rectangle.getPerimeter() &lt;&lt; endl; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; displayObj(circle); displayObj(rectangle); cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; GeometricObject* p = new Circle(5); delete p; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; return 0;} Chapter 12 Templates, Vectors, Stacks# Templates Basics# **Templates provide the capability to parameterize types in functions and classes. You can define functions or classes with generic types that can be substituted for concrete types by the compiler ** 1234567891011121314151617181920212223242526272829303132333435363738template &lt;typename T&gt;T maxValue(const T&amp; value1, const T&amp; value2){ return value1 &gt; value2 ? value1 : value2;}// equals to these four overloading functionint maxValue(int value1, int value2){ if (value1 &gt; value2) return value1; else return value2;}double maxValue(double value1, double value2){ if (value1 &gt; value2) return value1; else return value2;} char maxValue(char value1, char value2){ if (value1 &gt; value2) return value1; else return value2;} string maxValue(string value1, string value2){ if (value1 &gt; value2) return value1; else return value2;} Cautions: The generic maxValue function can be used to return a maximum of two values of any type, provided that: The two values have the same type The two values can be compared using the &gt; operator Occasionally, a template function may have more than one parameter. In this case, place the parameters together inside the brackets, separated by commas, such as **template&lt;typename T1, typename T2, typename T3&gt;** Class Templates# We use a improved stack class as example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#ifndef IMPROVEDSTACK_H#define IMPROVEDSTACK_Htemplate `&lt;typename T = int&gt;` // default type is intclass Stack{public: Stack(); Stack(int capacity); Stack(const Stack&amp;); ~Stack(); bool empty() const; T peek() const; void push(T value); T pop(); int getSize() const; int getCapacity() const;private: T* elements; int size; int capacity; void ensureCapacity();};// template `&lt;typename T&gt;` must be put before the implementationtemplate `&lt;typename T&gt;`Stack`&lt;T&gt;`::Stack() : size(0), capacity(16) // use Stack`&lt;T&gt;` rather than Stack{ elements = new T[capacity];}template `&lt;typename T&gt;`Stack`&lt;T&gt;`::Stack(int capacity) : size(0){ this-&gt;capacity = capacity; elements = new T[capacity];}template `&lt;typename T&gt;`Stack`&lt;T&gt;`::Stack(const Stack&amp; stack){ capacity = stack.capacity; size = stack.size; elements = new T[capacity]; for (int i = 0; i &lt; size; i++) { elements[i] = stack.elements[i]; }}template `&lt;typename T&gt;`Stack`&lt;T&gt;`::~Stack(){ delete[] elements;}template `&lt;typename T&gt;`bool Stack`&lt;T&gt;`::empty() const{ return size == 0;}template `&lt;typename T&gt;`T Stack`&lt;T&gt;`::peek() const{ return elements[size - 1];}template `&lt;typename T&gt;`void Stack`&lt;T&gt;`::push(T value){ ensureCapacity(); elements[size++] = value;}template `&lt;typename T&gt;`T Stack`&lt;T&gt;`::pop(){ return elements[--size];}template `&lt;typename T&gt;`int Stack`&lt;T&gt;`::getSize() const{ return size;}template `&lt;typename T&gt;`void Stack`&lt;T&gt;`::ensureCapacity(){ if (size &gt;= capacity) { T* old = elements; capacity = capacity * 2; elements = new T[capacity]; for (int i = 0; i &lt; size; i++) { elements[i] = old[i]; } delete[] old; }}template `&lt;typename T&gt;`int Stack`&lt;T&gt;`::getCapacity() const{ return capacity;}#endif // !IMPROVEDSTACK_H vector Class# C++ provides the vector class, which is more flexible than arrays. You can use a vector object just like an array, but a vector’s size can grow automatically if needed. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;using namespace std;template &lt;typename T&gt;T sum(vector&lt;vector&lt;T&gt; &gt; &amp;matrix) // a function to calculate the sum of a matrix{ T result = 0; for (unsigned i = 0; i &lt; matrix.size(); i++) { for (unsigned j = 0; j &lt; matrix[0].size(); j++) { result += matrix[i][j]; } } return result;}int main(){ vector &lt;int&gt; intVector; vector &lt;string&gt; stringVector(4); cout &lt;&lt; \"size of int vector: \" &lt;&lt; intVector.size() &lt;&lt; endl; cout &lt;&lt; \"size of string vector: \" &lt;&lt; stringVector.size() &lt;&lt; endl; cout &lt;&lt; \"capacity of int vector: \" &lt;&lt; intVector.capacity() &lt;&lt; endl; cout &lt;&lt; \"capacity of string vector: \" &lt;&lt; stringVector.capacity() &lt;&lt; endl; intVector.push_back(3); cout &lt;&lt; (intVector[0] == intVector.at(0) ? \"True\" : \"False\") &lt;&lt; endl; intVector.pop_back(); for (int i = 0; i &lt; 20; i++) { intVector.push_back(i + 1); cout &lt;&lt; \"size of int vector: \" &lt;&lt; intVector.size() &lt;&lt; endl; cout &lt;&lt; \"capacity of int vector: \" &lt;&lt; intVector.capacity() &lt;&lt; endl; } /*while (!intVector.empty()) { cout &lt;&lt; intVector[intVector.size()] &lt;&lt; endl; cout &lt;&lt; \"size of int vector: \" &lt;&lt; intVector.size() &lt;&lt; endl; cout &lt;&lt; \"capacity of int vector: \" &lt;&lt; intVector.capacity() &lt;&lt; endl; intVector.pop_back(); }*/ vector &lt;int&gt; v; v.swap(intVector); cout &lt;&lt; \"size of int vector: \" &lt;&lt; v.size() &lt;&lt; endl; cout &lt;&lt; \"capacity of int vector: \" &lt;&lt; v.capacity() &lt;&lt; endl; for (unsigned i = 0; i &lt; v.size(); i++) { cout &lt;&lt; v.at(i) &lt;&lt; \" \"; } cout &lt;&lt; endl; intVector.clear(); cout &lt;&lt; \"size of int vector: \" &lt;&lt; intVector.size() &lt;&lt; endl; cout &lt;&lt; \"capacity of int vector: \" &lt;&lt; intVector.capacity() &lt;&lt; endl; cout &lt;&lt; \"----------------------------\" &lt;&lt; endl; vector &lt;vector &lt;int&gt; &gt; matrix(4); // use vector to create a matrix for (unsigned i = 0; i &lt; matrix.size(); i++) { matrix[i] = vector&lt;int&gt;(3); } for (int i = 0; i &lt; matrix.size(); i++) { for (int j = 0; j &lt; matrix[0].size(); j++) { matrix[i][j] = (i + 1) * (j + 1); } } for (int i = 0; i &lt; matrix.size(); i++) { for (int j = 0; j &lt; matrix[0].size(); j++) { cout &lt;&lt; matrix[i][j] &lt;&lt; \" \"; } cout &lt;&lt; endl; } cout &lt;&lt; \"sum of matrix: \" &lt;&lt; sum(matrix) &lt;&lt; endl; return 0;} Chapter 14 Operator Overloading# The Rational Class for this chapter# **In this chapter, we will overload several operators for using Rational class more conveniently. ** And in this section, we implemented the Rational class without operator overloading first. 12345678910111213141516171819202122232425262728293031323334#ifndef RATIONAL_H #define RATIONAL_H#include &lt;cmath&gt;#include &lt;string&gt;#include &lt;sstream&gt;#include &lt;iostream&gt;using namespace std;class Rational{ public: Rational(); Rational(int numerator, int denominator); int getNumerator() const; int getDenominator() const; Rational add(const Rational&amp; secondRational) const; Rational subtract(const Rational&amp; secondRational) const; Rational multiply(const Rational&amp; secondRational) const; Rational divide(const Rational&amp; secondRational) const; int compareTo(const Rational&amp; secondRational) const; bool equals(const Rational&amp; secondRational) const; int intValue() const; double doubleValue() const; string toString() const; private: int numerator; int denominator; int gcd(int n, int d); };#endif // !RATIONAL_H 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#include \"Rational.h\"Rational::Rational(){ numerator = 0; denominator = 1;}Rational::Rational(int numerator, int denominator){ int factor = gcd(numerator, denominator); this-&gt;numerator = numerator / factor * (denominator &gt;= 0 ? 1 : -1); this-&gt;denominator = abs(denominator) / factor;}int Rational::getNumerator() const{ return numerator;}int Rational::getDenominator() const{ return denominator;}int Rational::gcd(int n, int d){ n = abs(n); d = abs(d); int factor = 1; for (int i = 1; i &lt;= d &amp;&amp; i &lt;= n; i++) { if (d % i == 0 &amp;&amp; n % i == 0) { factor = i; } } return factor;}Rational Rational::add(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator() + denominator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::subtract(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator() - denominator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::multiply(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::divide(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator(); d = denominator * secondRational.getNumerator(); return Rational(n, d);}int Rational::compareTo(const Rational&amp; secondRational) const{ double v1 = this-&gt;doubleValue(); double v2 = secondRational.doubleValue(); if (v1 &gt; v2) { return 1; } else if (v1 &lt; v2) { return -1; } else { return 0; }}bool Rational::equals(const Rational&amp; secondRational) const{ return compareTo(secondRational) == 0 ? true : false;}int Rational::intValue() const{ return numerator / denominator;}double Rational::doubleValue() const{ return 1.0* numerator / denominator;}string Rational::toString() const{ stringstream ss; ss &lt;&lt; numerator; if (denominator &gt; 1) ss &lt;&lt; \"/\" &lt;&lt; denominator; return ss.str();} Operator functions# If we want to compare two Rational objects, we can use the member function **compareTo**, but overloading **&lt;**, **&gt;**, **&lt;=**, **&gt;=** is more conveniently. 12345678// write in the defination of the Rational classbool operator&lt;(const Rational&amp; secondRational) const;// write in the Rational.cppbool Rational::operator&lt;(const Rational&amp; secondRational) const{ return compareTo(secondRational) &lt; 0 ? true : false} We can overload **+**, **-**, *****, **/** to make calculate Rational objects more conveniently. 123456Rational operator+(const Rational&amp; secondRational) const;Rational Rational::operator+(const Rational&amp; secondRational) const{ return add(secondRational);} Notes: C++ allows to overload the operators in Table 14.1, but does not allow to create new operators. We cannot change the operator precedence and associativity by overloading Most operator are binary operators. Some are unary. We cannot change the number of operands by overloading. Overloading the Subscript Operator# **The subscript operator **[]is commonly defined to access and modify a data field or an element in an object. 1234567891011121314151617181920212223242526// wrong overloading/*Rational r1(2, 3);cout &lt;&lt; r1[0] &lt;&lt; r1[1] &lt;&lt; endl; // okr1[0] = 5; // nope!*/int operator[](int index);int Rational::operator[](int index){ if(index == 0) return numerator; else return denominator;}// correct overloadingint&amp; operator[](int index);int&amp; Rational::operator[](int index){ if(index == 0) return numerator; else return denominator;} In C++, Lvalue (left value) refers to anything that can appear on the left side of the assignment operator (=) and Rvalue (right value) refers to anything that can appear ont the right side of the assignment operator. If we want to overload a Lvalue operator, we should use return-by-reference. Overloading Augmented Assignment Operator# **For augmented assignment operators (+=, -=, *=, **/= and %=) **are Lvalue operators. We can define the augmented assignment operators as functions to return a value by reference. ** 123456Rational&amp; operator+=(const Rational&amp; secondRational);Rational&amp; Rational::operator+=(const Rational&amp; secondRational){ *this = add(secondRational); return *this;} Overloading the Unary Operator# The unary + and – operators can be overloaded. Since the unary operator operates on the one operand that is the calling object itself, the unary function operator has no parameters. 123456Rational operator-();Rational Rational::operator-(){ return Rational(-numerator, denominator);} Overloading the ++ and -- Operator# The **++** and **--** operator may be prefix or postfix, and the methods of overloading prefix **++**/**--**** and postfix **++**/**--** are different. ** Prefix **++**/**--** is Lvalue operator, so it should return by reference. 1234567Rational&amp; operator++();Rational&amp; Rational::operator++(){ numerator += denominator; return *this;} Postfix **++**/**--** are defined with a special dummy parameter of the int type. So when we operator postfix **++**/**--** the function should have an int type dummy parameter. And Prefix **++**/**--** operator function do not have parameters. 12345678Rational operator++(int);Ratioanl Rational::operator++(int){ Rational temp(numerator, denominator); numerator += denominator; return temp;} Overloading the Stream Operator# friend Functions and friend Classes# **&lt;&lt;** and **&gt;&gt;** must be implemented as **friend**nonmember functions. So we learn **friend**function and **friend**** **class first. **We can define a **friend** function or a **friend** class to enable it to access private members in another class. ** 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;using namespace std;class Date{public: Date() { year = 2000; month = 1; day = 1; } friend class AccessDate; private: int year; int month; int day;}class AccessDate{public: static void p() { Date birthDate(); birthDate.year = 2001; cout &lt;&lt; birthDate.year &lt;&lt; endl; }}int main(){ AccessDate::p(); return 0;} 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;using namespace std;class Date{public: Date() { year = 2000; month = 1; day = 1; } friend void p() { Date date; date.year = 2022; cout &lt;&lt; date.year &lt;&lt; endl; } private: int year; int month; int day; }int main(){ p(); return 0;} Overloading &lt;&lt; and &gt;&gt;# The stream extraction (**&gt;&gt;**) and insertion (**&lt;&lt;**) operators can be overloaded for performing input and output operations. ** If we want to display a **Rational** object **r**, we will use: **cout &lt;&lt; r.toString() &lt;&lt; endl;** Wouldn’t it be nice to be able to display **r** like the following? **cout &lt;&lt; r &lt;&lt; endl;** But we cannot overload the **&lt;&lt;** operator as a member function for two operands **cout** **is an instance of **ostream**class, not **Rational** class. 12345678910111213141516class Rational{public // something friend ostream&amp; operator&lt;&lt;(ostream&amp; out, const Rational&amp; rational); private: // something}ostream&amp; operator&lt;&lt;(ostream&amp; out, const Rational&amp; rational){ out &lt;&lt; rational.toString(); // out &lt;&lt; rational.numerator &lt;&lt; \"/\" &lt;&lt; rational.denominator; return out;} Similarly, we can overload **&gt;&gt;** like the following 1234567891011121314151617181920class Rational{public // something friend istream&amp; operator&gt;&gt;(istream&amp; in, const Rational&amp; rational); private: // something}istream&amp; operator&gt;&gt;(istream&amp; in, const Rational&amp; rational){ cout &lt;&lt; \"Enter Numerator: \"; in &gt;&gt; rational.numerator; cout &lt;&lt; \"Enter Denominator: \"; in &gt;&gt; rational.denominator; return in;} Automatic Type Conversions# You can define functions to perform automatic conversion from an object to a primitive type value and vice versa. C++ can perform certain type conversion automatically. For example, in **4 + 5.5**, C++ convert 4 to 4.0 first. if we want to convert a **Rational**** **object to a double value. Here is the implementation. 123456operator double();Rational::operator double(){ return doubleValue();} And if we want to convert a **int** variable to **Rational** number. We may use the following code. 1234567Rational(int numerator);Rational::Rational(int numerator){ this-&gt;numerator = numerator; denominator = 1;} ** A class can define the conversion function to convert an object to a primitive type value or define a conversion constructor to convert a primitive type value to an object, but not both simultaneously in the class. If both are defined, the compiler will report an ambiguity error. Nonmember Functions for Overloading Operators# **If an operator can be overloaded as a nonmember function, define it as a nonmember function to enable implicit type conversions. ** In previous sections, we implemented **r1 + 4**, but we cannot calculate **4 + r1**, because the left operand is the calling object for the **+** operator and the left operand must be a **Rational** object. To solve this problem, we may use the following code. 123456789101112131415Rational(int numerator); // write in the class defination// This constructor enables the integer to be converted to a Rational objectRational::Rational(int numerator){ this-&gt;numerator = numerator; denominator = 1;}Rational operator+(const Ratioanl&amp; r1, const Rational&amp; r2); // write in the outside of the classRational operator+(const Ratioanl&amp; r1, const Rational&amp; r2){ return r1.add(r2);} Overloading the = Operators# You need to overload the **=** operator to perform a customized copy operation for an object. The behavior of the operator is the same as the copy constructor, both of these should be implemented explicitly if a data field in the class is a pointe that points to a dynamic generated array or object. 1234567891011121314151617181920212223class Course{public: // something const Course&amp; operator=(const Course&amp; course); private: string courseName; stirng* students; int capacity; int numerOfStudents;}const Course&amp; Course::operator=(const Course&amp; course){ courseaName = course.courseName; numberOfStudents = course.numberOfStudents; capacity = course.capacity; students = new string[capacity]; return *this;} The Rational Class with Overloaded Function Operators# 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#ifndef RATIONAL_H #define RATIONAL_H#include &lt;cmath&gt;#include &lt;string&gt;#include &lt;sstream&gt;#include &lt;iostream&gt;using namespace std;class Rational{public: Rational(); Rational(int numerator, int denominator); int getNumerator() const; int getDenominator() const; Rational add(const Rational&amp; secondRational) const; Rational subtract(const Rational&amp; secondRational) const; Rational multiply(const Rational&amp; secondRational) const; Rational divide(const Rational&amp; secondRational) const; int compareTo(const Rational&amp; secondRational) const; bool equals(const Rational&amp; secondRational) const; int intValue() const; double doubleValue() const; string toString() const; // overloaded operators // Lvalue operators int&amp; operator[](int index); Rational&amp; operator+=(const Rational&amp; secondRatioanl); Rational&amp; operator-=(const Rational&amp; secondRatioanl); Rational&amp; operator*=(const Rational&amp; secondRatioanl); Rational&amp; operator/=(const Rational&amp; secondRatioanl); // unary operators Rational operator+(); Rational operator-(); // prefix ++/-- Rational&amp; operator++(); Rational&amp; operator--(); // postfix ++/-- Rational operator++(int); Rational operator--(int); // stream operator friend ostream&amp; operator&lt;&lt;(ostream&amp; out, const Rational&amp; rational); friend istream&amp; operator&gt;&gt;(istream&amp; in, Rational&amp; rational); // convertion function Rational(int numerator);private: int numerator; int denominator; int gcd(int n, int d);};// nonmember operator functionsRational operator+(const Rational&amp; r1, const Rational&amp; r2);Rational operator-(const Rational&amp; r1, const Rational&amp; r2);Rational operator*(const Rational&amp; r1, const Rational&amp; r2);Rational operator/(const Rational&amp; r1, const Rational&amp; r2);bool operator&lt;(const Rational&amp; r1, const Rational&amp; r2);bool operator&gt;(const Rational&amp; r1, const Rational&amp; r2);bool operator&lt;=(const Rational&amp; r1, const Rational&amp; r2);bool operator&gt;=(const Rational&amp; r1, const Rational&amp; r2);bool operator==(const Rational&amp; r1, const Rational&amp; r2);bool operator!=(const Rational&amp; r1, const Rational&amp; r2);#endif // !RATIONAL_H 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280#include \"Rational.h\"Rational::Rational(){ numerator = 0; denominator = 1;}Rational::Rational(int numerator, int denominator){ int factor = gcd(numerator, denominator); this-&gt;numerator = numerator / factor * (denominator &gt;= 0 ? 1 : -1); this-&gt;denominator = abs(denominator) / factor;}int Rational::getNumerator() const{ return numerator;}int Rational::getDenominator() const{ return denominator;}int Rational::gcd(int n, int d){ n = abs(n); d = abs(d); int factor = 1; for (int i = 1; i &lt;= d &amp;&amp; i &lt;= n; i++) { if (d % i == 0 &amp;&amp; n % i == 0) { factor = i; } } return factor;}Rational Rational::add(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator() + denominator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::subtract(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator() - denominator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::multiply(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getNumerator(); d = denominator * secondRational.getDenominator(); return Rational(n, d);}Rational Rational::divide(const Rational&amp; secondRational) const{ int n; int d; n = numerator * secondRational.getDenominator(); d = denominator * secondRational.getNumerator(); return Rational(n, d);}int Rational::compareTo(const Rational&amp; secondRational) const{ double v1 = this-&gt;doubleValue(); double v2 = secondRational.doubleValue(); if (v1 &gt; v2) { return 1; } else if (v1 &lt; v2) { return -1; } else { return 0; }}bool Rational::equals(const Rational&amp; secondRational) const{ return compareTo(secondRational) == 0 ? true : false;}int Rational::intValue() const{ return numerator / denominator;}double Rational::doubleValue() const{ return 1.0* numerator / denominator;}string Rational::toString() const{ stringstream ss; ss &lt;&lt; numerator; if (denominator &gt; 1) ss &lt;&lt; \"/\" &lt;&lt; denominator; return ss.str();}int&amp; Rational::operator[](int index){ if (index == 0) { return numerator; } else { return denominator; }}Rational&amp; Rational::operator+=(const Rational&amp; secondRational){ *this = add(secondRational); return *this;}Rational&amp; Rational::operator-=(const Rational&amp; secondRational){ *this = subtract(secondRational); return *this;}Rational&amp; Rational::operator*=(const Rational&amp; secondRational){ *this = multiply(secondRational); return *this;}Rational&amp; Rational::operator/=(const Rational&amp; secondRational){ *this = divide(secondRational); return *this;}Rational Rational::operator+(){ return Rational(numerator, denominator);}Rational Rational::operator-(){ return Rational(-numerator, denominator);}Rational&amp; Rational::operator++(){ numerator += denominator; return *this;}Rational Rational::operator++(int){ Rational temp(numerator, denominator); numerator += denominator; return temp;}Rational&amp; Rational::operator--(){ numerator -= denominator; return *this;}Rational Rational::operator--(int){ Rational temp(numerator, denominator); numerator -= denominator; return temp;}ostream&amp; operator&lt;&lt;(ostream&amp; out, const Rational&amp; rational){ out &lt;&lt; rational.toString(); return out;}istream&amp; operator&gt;&gt;(istream&amp; in, Rational&amp; rational){ cout &lt;&lt; \"Enter Numerator: \"; in &gt;&gt; rational.numerator; cout &lt;&lt; \"Enter Denominator: \"; in &gt;&gt; rational.denominator; Rational r(rational.numerator, rational.denominator); rational.numerator = r.numerator; rational.denominator = r.denominator; return in;}Rational::Rational(int numerator){ this-&gt;numerator = numerator; denominator = 1;}Rational operator+(const Rational&amp; r1, const Rational&amp; r2){ return r1.add(r2);}Rational operator-(const Rational&amp; r1, const Rational&amp; r2){ return r1.subtract(r2);}Rational operator*(const Rational&amp; r1, const Rational&amp; r2){ return r1.multiply(r2);}Rational operator/(const Rational&amp; r1, const Rational&amp; r2){ return r1.divide(r2);}bool operator&lt;(const Rational&amp; r1, const Rational&amp; r2){ return r1.compareTo(r2) &lt; 0 ? true : false;}bool operator&gt;(const Rational&amp; r1, const Rational&amp; r2){ return r1.compareTo(r2) &gt; 0 ? true : false;}bool operator&lt;=(const Rational&amp; r1, const Rational&amp; r2){ return r1.compareTo(r2) &lt;= 0 ? true : false;}bool operator&gt;=(const Rational&amp; r1, const Rational&amp; r2){ return r1.compareTo(r2) &gt;= 0 ? true : false;}bool operator==(const Rational&amp; r1, const Rational&amp; r2){ return r1.equals(r2) == true ? true : false;}bool operator!=(const Rational&amp; r1, const Rational&amp; r2){ return r1.equals(r2) == false ? true : false;} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include \"Rational.h\"#include &lt;iostream&gt;using namespace std;int main(){ Rational r1(2, 3); Rational r2(4, 2); cout &lt;&lt; \"Test functions: \" &lt;&lt; endl; cout &lt;&lt; r1.toString() &lt;&lt; endl; cout &lt;&lt; r2.toString() &lt;&lt; endl; cout &lt;&lt; r1.intValue() &lt;&lt; endl; cout &lt;&lt; r2.intValue() &lt;&lt; endl; cout &lt;&lt; r1.doubleValue() &lt;&lt; endl; cout &lt;&lt; r2.doubleValue() &lt;&lt; endl; cout &lt;&lt; (r1.add(r2)).toString() &lt;&lt; endl; cout &lt;&lt; (r1.subtract(r2)).toString() &lt;&lt; endl; cout &lt;&lt; (r1.multiply(r2)).toString() &lt;&lt; endl; cout &lt;&lt; (r1.divide(r2)).toString() &lt;&lt; endl; cout &lt;&lt; r1.compareTo(r2) &lt;&lt; endl; cout &lt;&lt; r1.equals(r2) &lt;&lt; endl; cout &lt;&lt; \"-------------------------------\" &lt;&lt; endl; cout &lt;&lt; \"Test operator functions\" &lt;&lt; endl; Rational r3; cin &gt;&gt; r3; cout &lt;&lt; r3 &lt;&lt; endl; cout &lt;&lt; r1 + 0.5 &lt;&lt; endl; cout &lt;&lt; 2 - r1 &lt;&lt; endl; cout &lt;&lt; 1 + r1 &lt;&lt; endl; cout &lt;&lt; r1 * 3 &lt;&lt; endl; cout &lt;&lt; 3 / r1 &lt;&lt; endl; cout &lt;&lt; ((3 &gt; r1) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; ((10 &lt; r1) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; ((2 &gt;= r2) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; ((3 &lt;= r2) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; ((2 == r2) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; ((3 != r2) ? \"true\" : \"false\") &lt;&lt; endl; cout &lt;&lt; r1[0] &lt;&lt; \" \" &lt;&lt; r1[1] &lt;&lt; endl; cout &lt;&lt; r2[0] &lt;&lt; \" \" &lt;&lt; r2[1] &lt;&lt; endl; cout &lt;&lt; (r1 += r2) &lt;&lt; endl; cout &lt;&lt; r1 &lt;&lt; endl; cout &lt;&lt; (r1 -= r2) &lt;&lt; endl; cout &lt;&lt; r1 &lt;&lt; endl; cout &lt;&lt; (r1 *= r2) &lt;&lt; endl; cout &lt;&lt; r1 &lt;&lt; endl; cout &lt;&lt; (r1 /= r2) &lt;&lt; endl; cout &lt;&lt; r1 &lt;&lt; endl; cout &lt;&lt; (-r1).toString() &lt;&lt; endl; cout &lt;&lt; (r1++).toString() &lt;&lt; endl; cout &lt;&lt; r1.toString() &lt;&lt; endl; cout &lt;&lt; (++r1).toString() &lt;&lt; endl; cout &lt;&lt; r1.toString() &lt;&lt; endl; cout &lt;&lt; (r1--).toString() &lt;&lt; endl; cout &lt;&lt; r1.toString() &lt;&lt; endl; cout &lt;&lt; (--r1).toString() &lt;&lt; endl; cout &lt;&lt; r1.toString() &lt;&lt; endl; return 0;} Note Points for Operator Overloading# **The preceding sections introduced how to overload function operators. The following points are worth noting: ** **Conversion functions: ** Conversion funcitons from a class type to a primitive type or from a primitive type to a class type cannot both be defined in the same class. Doing so would cause ambiguity errors, because the compiler cannot decide which conversion to perform. Often converting from a primitive type to a class type is more useful. So, we will define our Rational class to support automatic conversion from a primitive type to the Rational type. **Member or nonmember functions: ** Most operators can be overloaded either as member or nonmember functions. However, the **=****, **[]**, **-&gt;**, and ****()** operators must be overloaded as member functions and &lt;&lt; and &gt;&gt; operators must be overloaded as nonmember functions. If an operator (i.e., **+****, **-**, *****, **/**, **%**, **&lt;**, **&lt;=**, **==**, **!=**, **&gt;**, and ****&gt;=**) can be implemented either as a member or nonmember function, it is better to overload it as a nonmember function to enable automatic type conversion with symmetric operands. Lvalue: If you want the returned object to be used as an Lvalue (i.e., used on the left-hand side of the assignment statement), you need to define the function to return a reference. The augmented assignment operators **+=**, **-=**, ***=**, **/=**, and **%=**, the prefix **++** and **--**operators, the subscript operator **[]**, and the assignment operators **=** are Lvalue operators. Chapter 13 File Input and Output# C++ provides the **ifstream**, **ofstream**, and **fstream** classes for processing and manipulating files. These classes are defined in the **&lt;fstream&gt;** header file. Text I/O# Writing Data to File# The **ofstream**** **class can be used to write primitive data-type values, arrays, strings, and objects to a text file. 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;fstream&gt;using namespace std;int main(){ // Create a file ofstream output; output.open(\"score.txt\"); // equals to // ofstream output(\"score.txt\"); output &lt;&lt; \"Zhang San\" &lt;&lt; \" \" &lt;&lt; 90 &lt;&lt; endl; output &lt;&lt; \"Lisi\" &lt;&lt; \" \" &lt;&lt; 87 &lt;&lt; endl; // close the file output.close() return 0;} If a file already exists, its contents will be destroyed without warning! The directory separator should be “\\” e.g **output.open(\"D:\\\\example.txt\");** It is better to use a relative file name rather than absolute file name. Reading Data from File# The **ifstream**** **class can be used to read data from a text file. 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ ifstream input(\"score.txt\"); string name; int score; input &gt;&gt; name &gt;&gt; score; cout &lt;&lt; name &lt;&lt; score &lt;&lt; endl; input &gt;&gt; name &gt;&gt; score; cout &lt;&lt; name &lt;&lt; score &lt;&lt; endl; input.close() return 0;} To read data correctly, we need to know exactly how data are stored. Testing File Existence# If the file does not exist when reading a file, the program will run and produce incorrect results. We can invoke the **fail()**** function immediately after invoking the **open** function. If **fail()** returns **true**, **it indicates that the file does not exist. 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ ifstream in(\"score.txt\"); if (input.fail()) { cout &lt;&lt; \"File does not exist!\" &lt;&lt; endl; } else { in.close(); cout &lt;&lt; \"File exists.\" &lt;&lt; endl; } return 0;} Testing End of File# We can invoke the **eof()**** **function **on the input object **to detect the end of file. 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ ifstream input(\"score.txt\") double sum = 0; double score; int count = 0; while(!input.eof()) { input &gt;&gt; score; if(input.eof()) break; sum += score; count++; } // equals to /* while(input &gt;&gt; score) { sum += score; count++; } */ cout &lt;&lt; sum / count &lt;&lt; endl; input.close(); return 0;} The statement **input &gt;&gt; score**** **is actually invoke an operator function. The function returns an object if a number is read, otherwise it returns NULL Letting the User Enter a File name# The file name passed to the input and output stream constructor or the **open** function must be a C-string in the standard C++. So we must use **c_str()**** function in the **string** class is invoked to return a C-string from **string** **object. 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ string filename; cout &lt;&lt; \"Enter a file name: \"; cin &gt;&gt; filename; ofstream out(filename.c_str()); out &lt;&lt; \"Hello World!\" &lt;&lt; endl; out.close(); return 0;} Formatting Output# The stream manipulators can be used to format console output as well as file output. We can include **&lt;iomanip&gt;**** ** to manipulate output in file. 123456789101112131415#incldue &lt;iostream&gt;#inlcude &lt;iomanip&gt;#include &lt;fstream&gt;using namespace std;int main(){ ofstream output; output &lt;&lt; setw(6) &lt;&lt; \"John\" &lt;&lt; setw(2) &lt;&lt; \"T\" &lt;&lt; setw(4) &lt;&lt; 87 &lt;&lt; endl; output.close(); return 0;} getline, get and put# The **getline** function can be used to read a string that includes whitespace characters and the **get**/**put**** function can be used to read and write a single character.** Syntax **getline(ifstream&amp; input, string s, char delimitChar);** The function stops reading characters when the delimiter character or **eof**** is encountered. The **delimitChar** has a default value **\"\\n\"**. The **getline** **function is defined in the **iostream**. **get();**** **or **get(char&amp; ch);** The first version returns a character from the input. The second version passes a character reference argument, reads a character from the input, and stores it in ch. This function also returns the reference to the input object being used. **put(char ch)** fstream and File Open Modes# We can use **fstream** to create a file object for both input and output. It is convenient to use **fstream** if a program needs to use the same stream object for both input and output. To open an **fstream** file, you have to specify a file open mode to tell C++ how the file will be used. Some of the file modes also can be used with **ifstream **and **ofstream** objects to open a file. For example, you may use the **ios:app** mode to open a file with an **ofstream** object so that you can append data to the file. However, for consistency and simplicity, it is better to use the file modes with the **fstream** objects. Several modes can be combined using the **|** operator. This is a bitwise inclusive-OR operator. For example, to open an output file named city.txt for appending data, we can use the following statement: **stream.open(\"city.txt\", ios::out | ios::app);**. Testing Stream States# **The functions **eof()**, **fail()**, **good()**, and **bad()** can be used to test the states of stream operations. ** Binary I/O# The **ios::binary** mode can be used to open a file for binary input and output. Computers do not differentiate binary files and text files. All files are stored in binary format, and thus all files are essentially binary files. Text I/O is built upon binary I/O to provide a level of abstraction for character encoding and decoding. The write Function# syntax: **streamObject.write(const char* s, int size);**** **So we should pass standard C-string into **write** function. 1234567891011121314#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ fstream binaryio(\"data.dat\", ios::out | ios::binary); string s = \"Hello\"; binaryio.write(s.c_str(), s.size()); binaryio.close(); return 0;} Often we need to write data other than characters. We can use **reinterpret_cast**** **operator to cast any pointer type of unrelated classes. It simply performs a binary copy of the value from one type to the other without altering the data. **syntax: ***reinterpret_cast&lt;dataType*&gt;(address);** And in this case, the dataType is **char*** 1234567891011121314#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ fstream binaryio(\"data.dat\", ios::out | ios::binary); int num = 199; binaryio.write(reinterpret_cast&lt;char*&gt;(&amp;num), sizeof(num)); binaryio.close(); return 0;} The read Function# syntax: ** **streamObject.read(char* address, int size);** The **size** **parameter indicates the maximum number of bytes read. The actual number of bytes read can be obtained from a member function **gcount**. We can also use **reinterpret_cast**** *to cast bytes into types not char 123456789101112131415#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main(){ fstream binaryio(\"data.dat\", ios::in | ios::binary); int num; binaryio.read(reinterpret_cast&lt;char*&gt;(&amp;num), sizeof(num)); cout &lt;&lt; num &lt;&lt; endl; binaryio.close(); return 0;} Random Access File# **The functions **seekg()** and **seekp()** can be used to move file pointer to any position in a random-access file for input and output. ** The **seekp(\"seek put\")** function is for the output stream, and the **seekg(\"seek get\")**function is for the input stream. Each function has two versions with one argument or two arguments. With one argument, the argument is the absolut location. **input.seekg(0);** **output.seekp(0);** With two arguments, the first argument is a long integer that indicates an offset, and the second argument, known as the seek base, specifies where to calculate the offset from. The following are three possible seek base arguments. Examples： We can also use **tellp**** and **tellg** **functions to return the position of the file pointer in the file. Updating Files# We can update a binary file by opening a file using the mode **ios::in | ios:out | ios::binary**. Often we need to update the contents of the file. You can open a file for both input and output. For example: **binaryio.open(\"data.dat\"), ios::in | ios::out | ios::binary);** 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;fstream&gt;#include \"Student.h\" // Define the Student Classusing namespace std;void display(const Student&amp; student){ // details omitted}int main(){ fstream binaryio; binaryio.open(\"student.dat\", ios::in | ios::out | ios::binary); Student stu1; binaryio.seekg(sizeof(Student)); binaryio.read(reinterpret_cast&lt;char*&gt;(&amp;stu1), sizeof(Student)); // initial name is Zhang San display(stu1); stu1.setName(\"Li si\"); binaryio.seekp(sizeof(Student)); binaryio.write(reinterpret_cast&lt;char*&gt;(&amp;stu1), sizeof(Student)); Student stu2; binaryio.seekg(sizeof(Student)); binaryio.read(reinterpret_cast&lt;char*&gt;(&amp;stu2)); display(stu2); binaryio.close(); return 0;} Chapter 16 Exception Handling# Overview# An exception is thrown using a **throw** statement and caught in a **try-catch** block. 123456789101112131415161718192021222324#include &lt;iostream&gt;using namespace std;int main(){ cout &lt;&lt; \"Enter two integers: \"; int n1, n2; cin &gt;&gt; n1 &gt;&gt; n2; try { if (n2 == 0) throw n1; cout &lt;&lt; n1 &lt;&lt; \" / \" &lt;&lt; n2 &lt;&lt; \" = \" &lt;&lt; n1 / n2 &lt;&lt; endl; } catch(int ex) { cout &lt;&lt; n1 &lt;&lt; \" cannot divided by zero\" &lt;&lt; endl; } return 0;} The **try** block contains the code that is executed in normal circumstances. The **catch**** block contains the code that is executed when **n2** is zero. When **n2** **is zero, the program throws an exception by executing **throw n1**. However, after the catch block has been executed, the program control does not return to the throw statement; instead, it executes the next statement after the catch block. If you are not interested in the contents of an exception object, the catch block parameter may be omitted. 123456789try{ // ...}catch(int){ cout &lt;&lt; \"Error occurred\" &lt;&lt; endl;} Advantages of Excpetion-Handling# Exception handling enables the caller of the function to process the exception thrown from a function. Without this capability, a function must handle the exception or terminate the program. Often, the called function does not know what to do in case of error. This is typically the case for the library function. The library function can detect the error, but only the caller knows what needs to be done when an error occurs. The fundamental idea of exception handling is to separate error detection (done in a called function) from error handling (done in the calling function). Exception Classes# You can use C++ standard exception classes to create exception objects and throw exceptions. The catch block parameter in the preceding examples is the int type. A class type is often more useful. The root class in this hierarchy is **exception****（defined in header &lt;exception&gt;）. It contains the virtual function **what()** **that returns an exception object’s error message. The **runtime_error **class (defined in header **&lt;exception&gt;**) is a base class for several standard exception classes that describes runtime errors. The **logic_error **class (defined in header **&lt;stdexcept&gt;** ) is a base class for several standard exception classes that describes logic errors. 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;stdexcept&gt;using namespace std;int quotient(int n1, int n2){ if (n2 == 0) { throw runtime_error(\"Divisor cannot be zero!\"); } return n1 / n2;}int main(){ cout &lt;&lt; \"Enter two integers: \"; int n1, n2; cin &gt;&gt; n1 &gt;&gt; n2; int result = 1; try { result = quotient(n1, n2); cout &lt;&lt; n1 &lt;&lt; \" / \" &lt;&lt; n2 &lt;&lt; \" = \" &lt;&lt; result &lt;&lt; endl; } catch(runtime_error &amp;ex) { cout &lt;&lt; ex.what() &lt;&lt; endl; cout &lt;&lt; result &lt;&lt; endl; // if result is defined in try block, it is best not to visit result! } return 0;} 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;stdexcept&gt;using namespace std;double getArea(double radius){ if(radius &lt; 0) { throw invalid_argument(\"Radius cannot be negative\"); } return radius * radius * 3.14159;}int main(){ cout &lt;&lt; \"Enter radius: \"; double radius; cin &gt;&gt; radius; try { double result = getArea(radius); cout &lt;&lt; \"The area is \" &lt;&lt; result &lt;&lt; endl; } catch(exception&amp; ex) { cout &lt;&lt; ex.what() &lt;&lt; endl; } return 0;} Custom Exception Classes# You can define custom exception classes to model exceptions that cannot be adequately represented using C++ standard exception classes. If we run into a problem that cannot be adequately described by the standard exception classes, we can create our own exception class. This class is just like any C++ class, but often it is desirable to **derive it from **exception** or a derived class of ****exception** so we can use the **what()** funtion. The following is an example defining exception class **TriangleException** for the **Triangle** class. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#ifndef TRIANGLE_H#define TRIANGLE_H#include &lt;cmath&gt;#include \"TriangleException.hpp\"class Triangle{public: Triangle() { side1 = side2 = side3 = 1; } Triangle(double side1, double side2, double side3) { if (!isValid(side1, side2, side3)) { throw TriangleException(side1, side2, side3); } this-&gt;side1 = side1; this-&gt;side2 = side2; this-&gt;side3 = side3; } inline double getSide1() const { return side1; } inline double getSide2() const { return side2; } inline double getSide3() const { return side3; } inline void setSide1(double side1) { if (!isValid(side1, side2, side3)) { throw TriangleException(side1, side2, side3); } this-&gt;side1 = side1; } inline void setSide2(double side2) { if (!isValid(side1, side2, side3)) { throw TriangleException(side1, side2, side3); } this-&gt;side2 = side2; } inline void setSide3(double side3) { if (!isValid(side1, side2, side3)) { throw TriangleException(side1, side2, side3); } this-&gt;side3 = side3; } inline double getArea() const { double p = (side1 + side2 + side3) / 2; return sqrt(p * (p - side1) * (p - side2) * (p - side3)); } inline double getPerimeter() const { return (side1 + side2 + side3) / 2; }private: double side1, side2, side3; inline bool isValid(double side1, double side2, double side3) { return ((side1 + side2) &gt; side3) &amp;&amp; ((side1 + side3) &gt; side2) &amp;&amp; ((side3 + side2) &gt; side1); }};#endif // !TRIANGLE_H 12345678910111213141516171819202122232425262728293031323334353637#ifndef TRIANGLEEXCEPTION_H#define TRIANGLEEXCEPTION_H#include &lt;stdexcept&gt;using namespace std;class TriangleException : public logic_error // derived from logic_error class{public: TriangleException(double side1, double side2, double side3) : logic_error(\"Invalid Triangle\") { this-&gt;side1 = side1; this-&gt;side2 = side2; this-&gt;side3 = side3; } inline double getSide1() const { return side1; } inline double getSide2() const { return side2; } inline double getSide3() const { return side3; }private: double side1, side2, side3;};#endif // !TRIANGLEEXCEPTION_H 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include \"Triangle.hpp\"#include \"TriangleException.hpp\"using namespace std;int main(){ cout &lt;&lt; \"Enter three sides of a triangle: \"; double s1, s2, s3; cin &gt;&gt; s1 &gt;&gt; s2 &gt;&gt; s3; try { Triangle t(s1, s2, s3); cout &lt;&lt; t.getArea() &lt;&lt; endl; } catch (TriangleException&amp; ex) { cout &lt;&lt; ex.what() &lt;&lt; endl; cout &lt;&lt; \"Three sides of the invalid triangle are: \" &lt;&lt; ex.getSide1() &lt;&lt; \" \" &lt;&lt; ex.getSide2() &lt;&lt; \" \" &lt;&lt; ex.getSide3() &lt;&lt; endl; } return 0;} A custom exception class is just like a regular class. Extending from a base class is not necessary, but it is a good practice to extend from the standard exception or a derived class of exception so your custom exception class can use the functions from the standard classes. Multiple Catches# A try-catch block may contain multiple catch clauses to deal with different exceptions thrown in the try clause. One **catch**** block can catch only one type of exception. C++ allows we to add mutiple **cath** block after a **try** **block in order to catch multiple types of exceptions. Note Various exception classes can be derived from a common base class. If a catch block catches exception objects of a base class, it can catch all the exception objects of the derived classes of that base class. The order in which exceptions are specified in **catch**** blocks is important. A **catch** block for a base class type should appear after a **catch** **block for a derived class type. Otherwise, the exception of a derived class is always caught by the catch block for the base class! 123456789101112131415161718192021222324252627// wrong ordertry{ // something}catch(logic_error&amp; ex){ //}catch(TriangleException&amp; ex){ //}// correct ordertry{ // something}catch(TriangleException&amp; ex){ //}catch(logic_error&amp; ex){ //} You may use an ellipsis **(...)** as the parameter of **catch****, **which will catch any exception no matter what the type of the exception that was thrown. This can be used as a default handler that catches all exceptions not caught by other handlers if it is specified last. 12345678910111213141516try{ // Execute some code here}catch (Exception1&amp; ex1) { cout &lt;&lt; \"Handle Exception1\" &lt;&lt; endl;}catch (Exception2&amp; ex2) { cout &lt;&lt; \"Handle Exception2\" &lt;&lt; endl;}catch (...){ cout &lt;&lt; \"Handle all other exceptions\" &lt;&lt; endl;} Exception Propagation# An exception is thrown through a chain of calling functions until it is caught or it reaches to the main function. 1234567891011121314151617try{ // Statements that may throw exceptions}catch (Exception1&amp; ex1) { // handler for exception1;}catch (Exception2&amp; ex2) { // handler for exception2;}...catch (ExceptionN&amp; exN) { // handler for exceptionN;} If no exceptions arise during the execution of the **try**** block, the **catch** **block are skipped. If one of the statements inside the **try**** block throws an exception, C++ skips the remaining statements in the **try** **block and starts the process of finding the code to handle the exception. Each **catch**** block is examined in trun, from first to last, to see whether the type of the exception object is an instance of the exception class in the **catch** **block. If one **catch**** block is matched, the code in the **catch** **block is executed. If no handle is found, C++ exits this function, passes the exception to the function that invoked the function, and continues the same process to find a handler. If no handler is found in the chain of functions being invoked, the program prints an error message on the console and terminates. If the exception type is Exception3, it is caught by the catch block for han\u0002dling exception ex3 in function2. statement5 is skipped, and statement6 is executed. If the exception type is Exception2, function2 is aborted, the control is returned to function1, and the exception is caught by the catch block for han\u0002dling exception ex2 in function1. statement3 is skipped, and statement4 is executed. If the exception type is Exception1, function1 is aborted, the control is returned to the main function, and the exception is caught by the catch block for handling exception ex1 in the main function. statement1 is skipped, and statement2 is executed. If the exception is not caught in function2, function1, and main, the program terminates. statement1 and statement2 are not executed. Rethrowing Exceptions# After an exception is caught, it can be rethrown to the caller of the function. 123456789try{ // statements;}catch (TheException&amp; ex) { // perform operations before exits; throw;} The statement **throw**** **rethrows the exception so that other handlers get a chance to process it. 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;stdexcept&gt;using namespace std;int f1(){ try { throw runtime_error(\"Exception in f1\"); } catch(exception&amp; ex) { cout &lt;&lt; \"Exception caught in function f1\" &lt;&lt; endl; cout &lt;&lt; ex.what() &lt;&lt; endl; throw; }}int main(){ try { f1(); } catch(exception&amp; ex) { cout &lt;&lt; \"Exception caught in function main\" &lt;&lt; endl; cout &lt;&lt; ex.what() &lt;&lt; endl; } return 0;} Exception Specification# You can declare potential exceptions a function may throw in the function header. An exception specification, also known as throw list, lists exceptions that a function can throw. So far, we have seen the function defined without a throw list. In this case, the function can throw any exception. So, it is tempting to omit exception specification. However, this is not a good practice. A function should give warning of any exceptions it might throw. So that programmers can write robust program to deal with these potential exceptions in a **try-catch**** **block. 1returnType functionName(parameterList) throw (exceptionList) 12345678910111213141516Triangle(double side1, double side2, double side3) throw (NonPositiveSideException, TriangleException){ check(side1); // throws NonPositiveSideException check(side2); check(side3); if(!isValid(side1, side2, side3)) { throw TriangleException(side1, side2, side3); } this-&gt;side1 = side1; this-&gt;side2 = side2; this-&gt;side3 = side3;} If a function attempts to throw an exception. A standard C++ function **unexpected**** **is invoked, which normally terminates the program. If a function specifies **bad_exception** in its throw list, the function will throw a **bad_exception** if an unspecified exception is throw from the function. When to use Exceptions# **Use exceptions for exceptional circumstances, not for simple logic errors that can be caught easily using an if statement. ** Exception handling separates error-handling code from normal programming tasks, thus making programs easier to read and to modify. Be aware, however, that exception handling usually requires more time and resources, because it requires instantiating a new exception object, rolling back the call stack, and propa\u0002gating the exception through the chain of functions invoked to search for the handler. Use exception handling ✔️ Do not using exception handling ❌ We want the exception to be processed by its caller We can handle the exception in the function where it occurs Common exceptions that may occur in multiple classes Simple errors that may occur in individual functions Dealing with unexpected error conditions Dealing with simple, expected situations A general paradigm for exception handling is that yo u declare to throw an exception in a function as shown in (a) below, and use the function in a try-catch block as shown in (b). --------------------The End------------------","link":"/C/Cpp/Cpp98/Introduction%20to%20Programming%20with%20C++%20%E5%95%83%E4%B9%A6%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"},{"name":"Learning rate优化","slug":"Learning-rate优化","link":"/tags/Learning-rate%E4%BC%98%E5%8C%96/"},{"name":"Test time augmentation","slug":"Test-time-augmentation","link":"/tags/Test-time-augmentation/"},{"name":"Self attention","slug":"Self-attention","link":"/tags/Self-attention/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","link":"/tags/Self-Supervised-Learning/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"},{"name":"Auto-encoder","slug":"Auto-encoder","link":"/tags/Auto-encoder/"},{"name":"Explainable ML","slug":"Explainable-ML","link":"/tags/Explainable-ML/"},{"name":"Domain Adaptation","slug":"Domain-Adaptation","link":"/tags/Domain-Adaptation/"},{"name":"Adversarial Attack","slug":"Adversarial-Attack","link":"/tags/Adversarial-Attack/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Network Compression","slug":"Network-Compression","link":"/tags/Network-Compression/"},{"name":"Life Long Learning","slug":"Life-Long-Learning","link":"/tags/Life-Long-Learning/"},{"name":"Meta Learning","slug":"Meta-Learning","link":"/tags/Meta-Learning/"},{"name":"计算成像","slug":"计算成像","link":"/tags/%E8%AE%A1%E7%AE%97%E6%88%90%E5%83%8F/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"快照压缩成像","slug":"快照压缩成像","link":"/tags/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/"},{"name":"C++","slug":"C","link":"/tags/C/"}],"categories":[{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"李宏毅机器学习","slug":"Deep-Learning/李宏毅机器学习","link":"/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"手搓记录","slug":"Deep-Learning/手搓记录","link":"/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/"},{"name":"论文阅读","slug":"论文阅读","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"快照压缩成像","slug":"论文阅读/快照压缩成像","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/"},{"name":"C++","slug":"C","link":"/categories/C/"}]}
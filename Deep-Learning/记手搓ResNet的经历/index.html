<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>记手搓ResNet的经历 · 核子的Blog</title><meta name="description" content="如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用




前言
ResNet基本思想
代码
各部分详解

Residual block

resblock_basic类
resblock_bottlenect类


resnet类


References



"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/logo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/logo@2x.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>记手搓ResNet的经历</a></h3></div><div class="post-content"><p><blockquote>
<p>如果不想听我叨叨的话可以直接前往代码部分进行copy，并参照注释里的demo进行使用</p>
</blockquote>
<div class="toc">
<!-- toc -->
<ul>
<li><a href="#qian-yan">前言</a></li>
<li><a href="#resnet-ji-ben-si-xiang">ResNet基本思想</a></li>
<li><a href="#dai-ma">代码</a></li>
<li><a href="#ge-bu-fen-xiang-jie">各部分详解</a>
<ul>
<li><a href="#residual-block">Residual block</a>
<ul>
<li><a href="#resblock-basic-lei">resblock_basic类</a></li>
<li><a href="#resblock-bottlenect-lei">resblock_bottlenect类</a></li>
</ul>
</li>
<li><a href="#resnet-lei">resnet类</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->
</div>
<h1 tabindex="-1"><span id="qian-yan">前言</span></h1>
<p>在上学期的机器视觉大作业中我用到了ResNet50-Unet，寒假中做分类任务时又用到了ResNet，但是之前我用ResNet要么是pip之后直接import，要么是参照hw里面助教给的初始代码进行增删。本着搞懂ResNet这么一个经典模型的心态，我决定自己手搓一遍ResNet（好吧其实还是有参照，但是在参照的基础上加了一点东西）。</p>
<h1 tabindex="-1"><span id="resnet-ji-ben-si-xiang">ResNet基本思想</span></h1>
<p>ResNet通过引入直接连接的旁路（shortcut），减少了反向传播时梯度消失的问题，使得模型能搭的更深，更不容易过拟合。</p>
<img src="image-20230115224650967.png" alt="image-20230115224650967" style="zoom:67%;">
<p>下表是各种CNN架构在ImageNet数据集上的top-5 error rate，可以看到ResNet相比VGG等其它架构，有着更好的效果。</p>
<img src="image-20230115225116240.png" alt="image-20230115225116240" style="zoom: 67%;">
<h1 tabindex="-1"><span id="dai-ma">代码</span></h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Implementation of ResNet with pytorch</span></span><br><span class="line"><span class="string">Simple usage:</span></span><br><span class="line"><span class="string">    from resnet_pytorch import *</span></span><br><span class="line"><span class="string">    classifier = resnet()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">All usage:</span></span><br><span class="line"><span class="string">    demo:</span></span><br><span class="line"><span class="string">	Customization resnet:</span></span><br><span class="line"><span class="string">	    classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11)</span></span><br><span class="line"><span class="string">        Only specify the type of resnet:</span></span><br><span class="line"><span class="string">            classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;)</span></span><br><span class="line"><span class="string">        Input no parameters:</span></span><br><span class="line"><span class="string">            classifier = resnet(class_num=16) # return resnet50</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">References:</span></span><br><span class="line"><span class="string">[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.</span></span><br><span class="line"><span class="string">    Deep Residual Learning for Image Recognition</span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1512.03385v1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[2] https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[3] 《深度学习计算机视觉》 Mohamed Elgendy, page 191-197</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">resblock_basic</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        the block for resnet18 and resnet34</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1</span></span><br><span class="line">    <span class="comment"># in bottleneck, the expansion of filters in the last layer of the block is 4</span></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.res_function = nn.Sequential(</span><br><span class="line">            <span class="comment"># if the kernel size equals to 3, the padding should be 1</span></span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels * self.expansion)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the output size and number of channels are equal to the input,</span></span><br><span class="line">        <span class="comment"># the shortcut path do nothing</span></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the output size or number of channels is unequal to the input,</span></span><br><span class="line">        <span class="comment"># the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != out_channels * self.expansion:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels * self.expansion)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.ReLU(inplace=<span class="literal">True</span>)(self.res_function(x) + self.shortcut(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">resblock_bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        the block for resnet50, resnet101 and resnet152</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># in resblock for resnet18 and resnet34, the expansion of filters in the last layer of the block is 1</span></span><br><span class="line">    <span class="comment"># in bottleneck, the expansion of filters in the last layer of the block is 4</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.res_function = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># maxpooling is replaced by convolution layer with stride unequal to 1</span></span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels * self.expansion)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the output size or number of channels is unequal to the input,</span></span><br><span class="line">        <span class="comment"># the shortcut path should be a sequence of 1x1 convolution layer to downsample and a batchnormalization layer</span></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != out_channels * self.expansion:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels * self.expansion)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.ReLU(inplace=<span class="literal">True</span>)(self.res_function(x) + self.shortcut(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">resnet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block=resblock_bottleneck, channel=<span class="number">3</span>, filter_list=<span class="literal">None</span>, block_num_list=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 class_num=<span class="number">10</span>, net_type=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            block: type of block, default: bottleneck</span></span><br><span class="line"><span class="string">            channel: the channel of image, 1 for gray image and 3 for RGB image. default: 3</span></span><br><span class="line"><span class="string">            filter_list: the filter numbers of each blocks&#x27; first layer. default: None</span></span><br><span class="line"><span class="string">            block_num_list: repeat times for each block, the length should be equal to filter_list. default: None</span></span><br><span class="line"><span class="string">            class_num: the number of classes for classification. default: 10</span></span><br><span class="line"><span class="string">            net_type: the type of resnet, &#x27;resnet50&#x27; for example. default: None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            demo:</span></span><br><span class="line"><span class="string">                Customization resnet:</span></span><br><span class="line"><span class="string">                    classifier = resnet(resblock_basic, 3, [64, 128, 256, 512], [1, 1, 1, 1], 11)</span></span><br><span class="line"><span class="string">                Only specify the type of resnet:</span></span><br><span class="line"><span class="string">                    classifier = resnet(class_num=12, net_type=&quot;resnet101&quot;)</span></span><br><span class="line"><span class="string">                Input no parameters:</span></span><br><span class="line"><span class="string">                    classifier = resnet(class_num=16) # return resnet50</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> block_num_list <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            block_num_list = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">if</span> filter_list <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># different types of resnet in the original paper</span></span><br><span class="line">        <span class="keyword">if</span> net_type == <span class="string">&#x27;resnet18&#x27;</span>:</span><br><span class="line">            block = resblock_basic</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">            block_num_list = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">elif</span> net_type == <span class="string">&#x27;resnet34&#x27;</span>:</span><br><span class="line">            block = resblock_basic</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">            block_num_list = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> net_type == <span class="string">&#x27;resnet50&#x27;</span>:</span><br><span class="line">            block = resblock_bottleneck</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">            block_num_list = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> net_type == <span class="string">&#x27;resnet101&#x27;</span>:</span><br><span class="line">            block = resblock_bottleneck</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">            block_num_list = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> net_type == <span class="string">&#x27;resnet152&#x27;</span>:</span><br><span class="line">            block = resblock_bottleneck</span><br><span class="line">            filter_list = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">            block_num_list = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        self.resblock_in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.pre_conv_layer = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=channel, out_channels=self.resblock_in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(self.resblock_in_channel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        stride_list = [<span class="number">1</span>] + [<span class="number">2</span>] * (<span class="built_in">len</span>(filter_list) - <span class="number">1</span>)</span><br><span class="line">        self.resblocks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(filter_list)):</span><br><span class="line">            self.resblocks.append(self._make_block(block, filter_list[i], block_num_list[i], stride_list[i]))</span><br><span class="line"></span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.25</span>),</span><br><span class="line">            nn.Linear(filter_list[-<span class="number">1</span>], class_num)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_block</span>(<span class="params">self, block, <span class="built_in">filter</span>, block_num, stride</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (block_num - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.resblock_in_channel, <span class="built_in">filter</span>, stride))</span><br><span class="line">            self.resblock_in_channel = <span class="built_in">filter</span> * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.pre_conv_layer(x)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.resblocks:</span><br><span class="line">            output = block(output)</span><br><span class="line">        output = self.avg_pool(output)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        output = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h1 tabindex="-1"><span id="ge-bu-fen-xiang-jie">各部分详解</span></h1>
<h2 tabindex="-1"><span id="residual-block">Residual block</span></h2>
<p>在ResNet的原始论文中，提出了如下图两种residual block，右边的一种被称为bottleneck。前一种residual block在ResNet层数较浅时使用，如ResNet18，ResNet34；后一种residual block在ResNet层数较深时使用，如ResNet50、ResNet101、ResNet152。</p>
<img src="image-20230115230500605.png" alt="image-20230115230500605" style="zoom:67%;">
<h3 tabindex="-1"><span id="resblock-basic-lei">resblock_basic类</span></h3>
<p><strong>前向传播过程</strong>：residual block前向传播的过程，要经过两次卷积+batch normalization，其中第一次卷积、batch normaliztion后，需要经过ReLU，而第二次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。</p>
<p><strong>输出channel数</strong>：block中，两个卷积层输出的channel数是相同的。</p>
<p><strong>降采样方法</strong>：residual block里面不设置max pooling，而是通过卷积层中设置大于1的步长起到降采样的作用，一个block中只有第一层卷积层中的stride可能大于1，第二个卷积层的stride为1。</p>
<p><strong>padding</strong>：由于卷积核大小为3x3，所以两个卷积层的padding都应该为1。</p>
<p><strong>shortcut path</strong>：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。</p>
<h3 tabindex="-1"><span id="resblock-bottlenect-lei">resblock_bottlenect类</span></h3>
<p><strong>前向传播过程</strong>：residual block前向传播的过程，要经过三次卷积+batch normalization，其中前两次卷积、batch normaliztion后，需要经过ReLU，而第三次卷积+batch normalization后，会和通过shortcut path的输入一起进行ReLU。</p>
<p><strong>输出channel数</strong>：block中，前两个卷积层输出的channel数是相同的，而第三个卷积层输出的channel数是前两层的四倍。</p>
<p><strong>降采样方法</strong>：一个block中只有第二层3x3卷积层中的stride可能大于1，第一、三个1x1卷积层的stride为1。</p>
<p><strong>padding</strong>：由于第二层卷积核大小为3x3，所以第二个卷积层的padding应该为1。</p>
<p><strong>shortcut path</strong>：shortcut path可能会遇到两种情况，如果residual block的输入面积和channel数和第二个卷积层的输出相同，那么shortcut path不需要做任何操作；如果输出面积和输入面积不同，或者输出和输入channel数不同，那么需要在shortcut path中加入1x1卷积层和batch normalization进行降采样。</p>
<h2 tabindex="-1"><span id="resnet-lei">resnet类</span></h2>
<p>在原始论文中，ResNet要先经过一个7x7卷积层，然后在经过若干个residual block，最后通过FC得到输出。</p>
<p><strong>预卷积层</strong>：原始论文中，预卷积层卷积核大小为7x7，所以padding=3，该卷积层步长为2，起到降采样作用，输出channel数设置为64。</p>
<p><strong>residual block序列</strong>：中间的residual block序列可以用 <code>nn.ModuleList</code>存放，通过 <code>_make_block</code>函数循环添加。</p>
<p><strong>自适应平均池化层</strong>：将特征图自适应转化为序列。</p>
<p><strong>全连接层</strong>：设置0.25 dropout率，然后再全连接。</p>
<h1 tabindex="-1"><span id="references">References</span></h1>
<ol>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385v1">https://arxiv.org/abs/1512.03385v1</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py">https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py</a></li>
<li>《深度学习计算机视觉》 Mohamed Elgendy, page 191-197</li>
</ol>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-01-15</span><a class="tag" href="/categories/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/CNN/" title="CNN">CNN </a><i class="fa fa-tag"></i><a class="tag" href="/tags/ResNet/" title="ResNet">ResNet </a><span class="leancloud_visitors"></span><span>大约2088个字, 6分钟57秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86%0Ahttp://hezj-opt.github.io/Deep-Learning/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5-Transformer">下一篇</a></li></ul></div><script src="/js/visitors.js"></script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div></body></html>
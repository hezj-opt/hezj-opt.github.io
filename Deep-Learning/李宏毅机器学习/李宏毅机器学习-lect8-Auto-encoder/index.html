<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>李宏毅机器学习-lect8 Auto-encoder · 核子的Blog</title><meta name="description" content="Auto-encoder的基本思想

Auto-encoder的训练
为什么Auto-encoder是有效的
Auto-encoder和BERT的关系


Auto-encoder的应用

Feature Disentanglement
Discrete Latent Representation
"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/mylogo.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/mylogo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/mylogo.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div><details class="ltr" open><summary>目录</summary><div class="tocmenu"><p><ol class="toclist"><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#auto-encoder-de-ji-ben-si-xiang"><span class="toclist-number">1.</span> <span class="toclist-text">Auto-encoder的基本思想</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#auto-encoder-de-xun-lian"><span class="toclist-number">1.1.</span> <span class="toclist-text">Auto-encoder的训练</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#wei-shi-me-auto-encoder-shi-you-xiao-de"><span class="toclist-number">1.2.</span> <span class="toclist-text">为什么Auto-encoder是有效的</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#auto-encoder-he-bert-de-guan-xi"><span class="toclist-number">1.3.</span> <span class="toclist-text">Auto-encoder和BERT的关系</span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#auto-encoder-de-ying-yong"><span class="toclist-number">2.</span> <span class="toclist-text">Auto-encoder的应用</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#feature-disentanglement"><span class="toclist-number">2.1.</span> <span class="toclist-text">Feature Disentanglement</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#discrete-latent-representation"><span class="toclist-number">2.2.</span> <span class="toclist-text">Discrete Latent Representation</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#generator"><span class="toclist-number">2.3.</span> <span class="toclist-text">Generator</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#compression"><span class="toclist-number">2.4.</span> <span class="toclist-text">Compression</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#anomaly-detection"><span class="toclist-number">2.5.</span> <span class="toclist-text">Anomaly Detection</span></a></li></ol></li></ol></p></div></details></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>李宏毅机器学习-lect8 Auto-encoder</a></h3></div><div class="post-content"><p>
<h1 tabindex="-1"><span id="auto-encoder-de-ji-ben-si-xiang">Auto-encoder的基本思想</span><a href="#auto-encoder-de-ji-ben-si-xiang" class="header-anchor">#</a></h1>
<h2 tabindex="-1"><span id="auto-encoder-de-xun-lian">Auto-encoder的训练</span><a href="#auto-encoder-de-xun-lian" class="header-anchor">#</a></h2>
<p>Auto-encoder的是以无监督学习训练的，训练时会把一个Encoder和一个Decoder拼接在一起，将Encoder产生的向量输入Decoder，目标是让Decoder尽可能去复原输入。如果要使得Decoder能复原Encoder的输入，就要求Encoder能有效地将输入编码为向量，所以训练好以后，Encoder就有强大的编码功能，就可以拿去做一些下游任务（这和BERT是类似的）。</p>
<p>其实Auto-encoder和Cycle-GAN的思想也是有点类似的（其实是先有Auto-encoder），Cycle-GAN是希望一个Generator产生的图片经过另一个Generator后能够复原，训练成功后得到两个Generator。</p>
<img src="image-20230130232201700.png" alt="image-20230130232201700" style="zoom:50%;">
<h2 tabindex="-1"><span id="wei-shi-me-auto-encoder-shi-you-xiao-de">为什么Auto-encoder是有效的</span><a href="#wei-shi-me-auto-encoder-shi-you-xiao-de" class="header-anchor">#</a></h2>
<p>以图像的任务为例，把图片看成高维度的向量，Auto-encoder的Encoder就是将一个高维度的向量降维成低维度的向量。</p>
<p>由于维度降低，必然有信息丢失。Decoder能复原成功，主要是因为图片虽然是高维的向量，但是其变化是有限的，比如下图中也许3x3的图片可能实际上只有4种变化是合乎情理的，所以可以把9维向量降维为2维向量，实际任务中的图片也会符合某种分布，这会限制图片的变化种类，所以用低维度向量来表示图片依然保留了图片的特性，所以Decoder才能复原图片。</p>
<p>那么为什么Auto-encoder中的Encoder在下游任务中是有效的呢？因为训练好的Encoder能有效地将数据降维，从而把复杂的数据用简单的方式表示，所以在下游任务中只用需要一点带标注的资料对模型进行训练就可以得到一个很好的结果。这个其实和BERT很像，BERT也是讲高维的语言信息降维，然后输入给输出层，在微调的时候，由于BERT具有很好的提取特征的能力，只需要很少的训练资料就可以让模型学会下游任务。</p>
<img src="image-20230130232325074.png" alt="image-20230130232325074" style="zoom:50%;">
<h2 tabindex="-1"><span id="auto-encoder-he-bert-de-guan-xi">Auto-encoder和BERT的关系</span><a href="#auto-encoder-he-bert-de-guan-xi" class="header-anchor">#</a></h2>
<p>从训练过程来看，在实际训练Auto-encoder的时候，我们其实会把数据（比如图片）加上一点噪声，然后希望Decoder输出的数据能和加上噪声前的数据接近。那么其实Auto-encoder的训练过程就很像BERT了，因为预训练BERT时也是会给数据加上噪声，比如给句子挖空让BERT学做“填空题”。</p>
<p>从作用来看BERT和Encoder的作用类似，都是完成特征的提取；线性输出层和Decoder作用类似，都是对特征进行处理。</p>
<img src="image-20230130232610415.png" alt="image-20230130232610415" style="zoom:50%;">
<img src="image-20230130232637648.png" alt="image-20230130232637648" style="zoom:50%;">
<h1 tabindex="-1"><span id="auto-encoder-de-ying-yong">Auto-encoder的应用</span><a href="#auto-encoder-de-ying-yong" class="header-anchor">#</a></h1>
<p>Auto-encoder在训练好以后，除了可以将Encoder拿出来迁移到下游任务上，Auto-encoder还有很多其他应用。</p>
<h2 tabindex="-1"><span id="feature-disentanglement">Feature Disentanglement</span><a href="#feature-disentanglement" class="header-anchor">#</a></h2>
<p>Feature Disentanglement，字面翻译就是特征解耦，就是将Encoder输出的向量包含的纠缠的特征（我们不知道哪些维度代表哪些特征）进行分离。举例来说，Auto-encoder可以做到输入一段声音，让Encoder输出的向量前一半代表说话内容，后一半代表说话的人。这样就有可能实现语音合成之类的应用，比如输入李宏毅老师说的"How are you?“和新桓结衣说的"Hello”，输出新垣结衣说的"How are you?"。</p>
<img src="image-20230130231239465.png" alt="image-20230130231239465" style="zoom:50%;">
<img src="image-20230130231204364.png" alt="image-20230130231204364" style="zoom:50%;">
<h2 tabindex="-1"><span id="discrete-latent-representation">Discrete Latent Representation</span><a href="#discrete-latent-representation" class="header-anchor">#</a></h2>
<p>一般来说，Encoder输出的向量中的每一个数都是一般的实数，但是有时我们希望输出的向量中每一维都只有0和1，这样可能可以更好地解释Encoder输出，比如输入一张人物头像图片，第一维的1代表有戴眼镜，0代表没有戴眼镜。有时我们甚至可以让输出的向量是一个one hot向量，这样可能可以实现无监督学习训练模型做分类任务，比如手写数字识别，输出十维向量，观察向量中哪一维为1，就可以得出分类结果。</p>
<img src="image-20230130231837380.png" alt="image-20230130231837380" style="zoom:50%;">
<p>在Discrete Representation中，最知名的是VQVAE，Encoder输出一个实数向量，和codebook中的各个向量（也是通过学习得到的）计算相似度（有点像Self-attention，Encoder输出向量是query，codebook中向量是key），将相似度最大的向量拿出来，作为输入Decoder的向量。这可以使得Embedding的种类是有限的，这样可能可以做语音辨识，因为可能可以把Encoder的连续输出对应到离散的音标。</p>
<img src="image-20230130231923377.png" alt="image-20230130231923377" style="zoom:50%;">
<p>除此之外，Auto-encoder的思想在文本上也有很好的应用。我们希望Encoder（此时Encoder和Decoder都是seq2seq模型）可以将输入的文章进行特征提取，从而产生文章摘要，但是如果只是用Auto-Encoder的架构，会发现Encoder和Decoder之间会达成某种“契约”，就是Encoder生成的文字只有Decoder能看懂，而无法被人理解。所以为了产生让人能理解的摘要，需要让产生的摘要经过一个Discriminator，用来判定文章是不是人写的，那么其实仔细一想，这不是Cycle-GAN吗？（^_^）</p>
<p>所以Auto-encoder也是一种理解Cycle-GAN的方式？</p>
<img src="image-20230130232010044.png" alt="image-20230130232010044" style="zoom:50%;">
<h2 tabindex="-1"><span id="generator">Generator</span><a href="#generator" class="header-anchor">#</a></h2>
<p>我们也可以将Decoder单独拿出来，作为一个Generator进行使用，可以实现从某个分布中随机抽取一个向量，然后产生图片，这其实就是Variational auto-encoder（VAE）的想法。</p>
<img src="image-20230130231045032.png" alt="image-20230130231045032" style="zoom:50%;">
<h2 tabindex="-1"><span id="compression">Compression</span><a href="#compression" class="header-anchor">#</a></h2>
<p>利用训练好的Auto-encoder，我们可以进行图片的压缩。因为图片经过Encoder时会丢失一部分信息，所以经过Decoder后复原的图像相比原始图片会有一定失真，这是一种有损压缩的方式。</p>
<img src="image-20230130230927289.png" alt="image-20230130230927289" style="zoom:50%;">
<h2 tabindex="-1"><span id="anomaly-detection">Anomaly Detection</span><a href="#anomaly-detection" class="header-anchor">#</a></h2>
<p>Auto-encoder也可以做异常样本检测，我们把训练的数据的分布看成一个domain，训练成功的Auto-encoder是可以让这个domain上的数据在经过Encoder编码、Decoder解码后复原的。但是如果今天输入的数据不在这个domain内，比如训练的时候输入真人头像，输入二次元人物头像时就很有可能无法复原。应用Anomaly Detection，可以实现诈骗信息、异常交易记录的检测等具体的应用。</p>
<p>异常检测的问题很可能不能用训练分类器的方法去做，因为异常样本往往是很少的，会产生极端的样本不平衡。</p>
<img src="image-20230130230549885.png" alt="image-20230130230549885" style="zoom:50%;">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-01-30</span><a class="tag" href="/categories/Deep-Learning/" title="Deep Learning">Deep Learning </a><a class="tag" href="/categories/Deep-Learning/李宏毅机器学习/" title="李宏毅机器学习">李宏毅机器学习 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Auto-encoder/" title="Auto-encoder">Auto-encoder </a><span class="leancloud_visitors"></span><span>大约1844个字, 6分钟8秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8%20Auto-encoder%0Ahttp://hezj-opt.github.io/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/" title="李宏毅机器学习-lect9 Explainable ML">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/" title="李宏毅机器学习-Why Deep Learning?">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="gitalk_container" style="padding:10px"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>const gitalk = new Gitalk({
    clientID: '75fa8f8526b947111419',
    clientSecret: '28a1006aeebc5fe3d2e6d838846fa202d3362fd0',
    repo: 'comments',      // The repository of store comments,
    owner: 'hezj-opt',
    admin: ['hezj-opt'],
    id: '李宏毅机器学习-lect8 Auto-encoder',      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
})
gitalk.render('gitalk_container')
</script><div id="gitalk-container" style="padding-left: 50px;padding-right: 50px;padding-bottom: 70px"></div><script>var gitalk = new Gitalk({
clientID: "",
clientSecret: "",
repo: "comments",
owner: "hezj-opt",
admin:  [""],
id: md5(decodeURI(location.pathname)),      // Ensure uniqueness and length less than 50
language: 'zh-CN' 
})
gitalk.render('gitalk-container')</script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div><script src="/js/baidu-tongji.js"></script><!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
          环境配置
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" title="hexo配置问题"><i class="post-icon gg-file-document"></i>hexo配置问题</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">14</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
          李宏毅机器学习
        </a>
      <span class="tree-list-count">13</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/" title="李宏毅机器学习-Why Deep Learning?"><i class="post-icon gg-file-document"></i>李宏毅机器学习-Why Deep Learning?</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/" title="李宏毅机器学习-hw3-CNN-总结"><i class="post-icon gg-file-document"></i>李宏毅机器学习-hw3-CNN-总结</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/" title="李宏毅机器学习-lect4 Self attention"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect4 Self attention</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6 GAN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect6 GAN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/" title="李宏毅机器学习-lect3 CNN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect3 CNN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5 Transformer"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect5 Transformer</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/" title="李宏毅机器学习-lect7 Self-Supervised Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect7 Self-Supervised Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/" title="李宏毅机器学习-为什么训练会失败"><i class="post-icon gg-file-document"></i>李宏毅机器学习-为什么训练会失败</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/" title="李宏毅机器学习-lect8 Auto-encoder"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect8 Auto-encoder</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/" title="李宏毅机器学习-lect9 Explainable ML"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect9 Explainable ML</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/" title="李宏毅机器学习-lect11 Domain Adaptation"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect11 Domain Adaptation</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/" title="李宏毅机器学习-lect10 Adversarial Attack"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect10 Adversarial Attack</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect12-Reinforcement-Learning/" title="李宏毅机器学习-lect12 Deep Reinforcement Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect12 Deep Reinforcement Learning</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/">
          手搓记录
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历"><i class="post-icon gg-file-document"></i>记手搓ResNet的经历</a></li></ul></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body></html>
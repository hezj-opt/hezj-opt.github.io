<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>李宏毅机器学习-hw3-CNN-总结 · 核子的Blog</title><meta name="description" content="任务介绍
训练结果
训练方法

Data augmentation
模型设计
Loss function的选择与调参
Learning rate调整方案
Test time augmentation


进一步提升的可能

数据增强可能可以尝试mix up
模型设计可能可以继续尝试
Learning"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/mylogo.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/mylogo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/mylogo.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div><details class="ltr" open><summary>目录</summary><div class="tocmenu"><p><ol class="toclist"><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#ren-wu-jie-shao"><span class="toclist-number">1.</span> <span class="toclist-text">任务介绍</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#xun-lian-jie-guo"><span class="toclist-number">2.</span> <span class="toclist-text">训练结果</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#xun-lian-fang-fa"><span class="toclist-number">3.</span> <span class="toclist-text">训练方法</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#data-augmentation"><span class="toclist-number">3.1.</span> <span class="toclist-text">Data augmentation</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#mo-xing-she-ji"><span class="toclist-number">3.2.</span> <span class="toclist-text">模型设计</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#loss-function-de-xuan-ze-yu-diao-can"><span class="toclist-number">3.3.</span> <span class="toclist-text">Loss function的选择与调参</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#learning-rate-diao-zheng-fang-an"><span class="toclist-number">3.4.</span> <span class="toclist-text">Learning rate调整方案</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#test-time-augmentation"><span class="toclist-number">3.5.</span> <span class="toclist-text">Test time augmentation</span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#jin-yi-bu-ti-sheng-de-ke-neng"><span class="toclist-number">4.</span> <span class="toclist-text">进一步提升的可能</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#shu-ju-zeng-qiang-ke-neng-ke-yi-chang-shi-mix-up"><span class="toclist-number">4.1.</span> <span class="toclist-text">数据增强可能可以尝试mix up</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#mo-xing-she-ji-ke-neng-ke-yi-ji-xu-chang-shi"><span class="toclist-number">4.2.</span> <span class="toclist-text">模型设计可能可以继续尝试</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#learning-rate-diao-zheng-fang-an-you-dai-ti-sheng"><span class="toclist-number">4.3.</span> <span class="toclist-text">Learning rate调整方案有待提升</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#cross-validation-ensemble"><span class="toclist-number">4.4.</span> <span class="toclist-text">Cross Validation + Ensemble</span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#qi-ta-zhi-de-ji-lu-de-dong-xi"><span class="toclist-number">5.</span> <span class="toclist-text">其它值得记录的东西</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#autodl-tensorboard-shi-yong"><span class="toclist-number">5.1.</span> <span class="toclist-text">autodl tensorboard使用</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#layer-de-ti-du-bian-hua"><span class="toclist-number">5.2.</span> <span class="toclist-text">layer的梯度变化</span></a></li></ol></li></ol></p></div></details></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>李宏毅机器学习-hw3-CNN-总结</a></h3></div><div class="post-content"><p>
<h1 tabindex="-1"><span id="ren-wu-jie-shao">任务介绍</span><a href="#ren-wu-jie-shao" class="header-anchor">#</a></h1>
<p>这个作业要求训练一个能对11种食物进行分类的CNN。使用的数据集为food-11，数据集划分如下：</p>
<ul>
<li>训练集：9866张带有label的图像</li>
<li>验证集：3430张带有label的图像</li>
<li>测试集：3347张不带有label的图像</li>
</ul>
<p>完成训练后，将用测试集进行测试，输出含有3347张图片的预测label的csv文件，上传至kaggle后系统根据预测准确率自动评分。</p>
<p>Baseline如下：</p>
<ul>
<li>Simple : 0.50099</li>
<li>Medium : 0.73207</li>
<li>Strong : 0.81872</li>
<li>Boss : 0.88446</li>
</ul>
<h1 tabindex="-1"><span id="xun-lian-jie-guo">训练结果</span><a href="#xun-lian-jie-guo" class="header-anchor">#</a></h1>
<table border="1">
    <tr>
        <th>Times</th>
        <th>Private Score</th>
        <th>Public Score</th>
        <th>Improvement（相比上一步）</th>
    </tr>
    <tr>
        <td>1</td>
        <td>0.57063</td>
        <td>0.55278</td>
        <td>直接运行初始代码</td>
    </tr>
    <tr>
        <td>2</td>
        <td>0.71830</td>
        <td>0.75298</td>
        <td>数据增强、调整模型、调整loss function</td>
    </tr>
    <tr>
        <td>3</td>
        <td>0.75757</td>
        <td>0.78884</td>
        <td>调整数据增强、手搓ResNet、loss function调参</td>
    </tr>
    <tr>
        <td>4</td>
        <td>0.77592</td>
        <td>0.80378</td>
        <td>减少ResNet层数，调整learning rate</td>
    </tr>
    <tr>
        <td>5</td>
        <td>0.77251</td>
        <td>0.80677</td>
        <td>减少ResNet层数</td>
    </tr>
    <tr>
        <td>6</td>
        <td>0.82415</td>
        <td>0.85358</td>
        <td>增加ResNet层数</td>
    </tr>
    <tr>
        <td>7</td>
        <td>0.84208</td>
        <td>0.87450</td>
        <td>使用tta</td>
    </tr>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">Times</th>
<th style="text-align:center">private score</th>
<th style="text-align:center">public score</th>
<th style="text-align:center">Improvement（相比前一步）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.57063</td>
<td style="text-align:center">0.55278</td>
<td style="text-align:center">直接运行初始代码</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">0.71830</td>
<td style="text-align:center">0.75298</td>
<td style="text-align:center">数据增强、调整模型、调整loss function</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0.75757</td>
<td style="text-align:center">0.78884</td>
<td style="text-align:center">调整数据增强、手搓ResNet、loss function调参</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">0.77592</td>
<td style="text-align:center">0.80378</td>
<td style="text-align:center">减少ResNet层数、调整learning rate</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.77251</td>
<td style="text-align:center">0.80677</td>
<td style="text-align:center">减少ResNet层数</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">0.82415</td>
<td style="text-align:center">0.85358</td>
<td style="text-align:center">增加ResNet层数</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">0.84208</td>
<td style="text-align:center">0.87450</td>
<td style="text-align:center">使用tta</td>
</tr>
</tbody>
</table>
<h1 tabindex="-1"><span id="xun-lian-fang-fa">训练方法</span><a href="#xun-lian-fang-fa" class="header-anchor">#</a></h1>
<h2 tabindex="-1"><span id="data-augmentation">Data augmentation</span><a href="#data-augmentation" class="header-anchor">#</a></h2>
<p>在观察了部分数据之后，发现合理的图片增强方式包括但不限于</p>
<ul>
<li>缩放裁剪</li>
<li>随机翻转</li>
<li>随机旋转</li>
<li>仿射变换</li>
<li>随机灰度化</li>
</ul>
<p>最终代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train_tfm = transforms.Compose([</span><br><span class="line">    <span class="comment"># Resize the image into a fixed shape (height = width = 128)</span></span><br><span class="line">    transforms.RandomResizedCrop((<span class="number">128</span>, <span class="number">128</span>), scale=(<span class="number">0.7</span>, <span class="number">1.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),   <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),     <span class="comment"># 随机上下翻转</span></span><br><span class="line">    transforms.RandomRotation(degrees=(<span class="number">0</span>, <span class="number">180</span>)), <span class="comment"># 图像随机旋转</span></span><br><span class="line">    transforms.RandomAffine(<span class="number">30</span>),</span><br><span class="line">    transforms.RandomGrayscale(<span class="number">0.2</span>), <span class="comment"># 随机灰度化</span></span><br><span class="line">    <span class="comment"># You may add some transforms here.</span></span><br><span class="line">    <span class="comment"># ToTensor() should be the last one of the transforms.</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 tabindex="-1"><span id="mo-xing-she-ji">模型设计</span><a href="#mo-xing-she-ji" class="header-anchor">#</a></h2>
<p>模型使用的是ResNet，ResNet的搭建见：<a href="https://hezj-opt.github.io/Deep-Learning/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/">记手搓ResNet的经历 · 核子的Blog (hezj-opt.github.io)</a></p>
<p>在调整参数方面，最终调整预卷积层的通道数为32，全连接层的dropout为0.4，残差块采用两个3x3卷积层和shortcut path构成的残差块，最终使用通道数为64、128、256、512的残差块数量分别为1、2、1、1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = resnet(resblock_basic, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="number">11</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>在第4次训练中，使用过层数较少的网络（<code>model = resnet(resblock_basic, 3, [64, 128, 256], [2, 2, 1], 11).to(device)</code>），但是效果不佳，当时以为是over fitting，所以在第5次训练中把网络改得更简单了（<code>model = resnet(resblock_basic, 3, [64, 128, 256], [2, 1, 1], 11).to(device)</code>），修改后发现效果不仅没有显著提升，而且训练时长增加了非常多（第4次训练大致12小时，第5次训练花了将近16小时）。所以发现是模型欠拟合，决定增加模型复杂度，才在第6次训练后通过strong baseline。</p>
<p>鉴于几次调参后的效果，推断三次训练在下图中位置红色线框覆盖的区间内。</p>
<img src="image-20230118210452127.png" alt="image-20230118210452127" style="zoom: 67%;">
<h2 tabindex="-1"><span id="loss-function-de-xuan-ze-yu-diao-can">Loss function的选择与调参</span><a href="#loss-function-de-xuan-ze-yu-diao-can" class="header-anchor">#</a></h2>
<p>查阅资料发现，使用Focal loss会比Cross entropy有更快的收敛速度（参数γ导致），而且更适合各个标签的数据集不均的情况（参数α导致）。</p>
<p>下图显示了由于Focal loss的γ参数，Focal loss会收敛得更快</p>
<img src="image-20230118213336141.png" alt="image-20230118213336141" style="zoom: 50%;">
<p>Focal loss中的α参数可以平衡训练集中各类图片数量的差异，比如在本次任务中，我先统计训练集中了11类食物的数量，然后对样本少的赋比较高的α值，样本多的食物赋比较低的α值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">count = count / np.<span class="built_in">max</span>(count) <span class="comment"># count 为11类食物样本数的统计结果</span></span><br><span class="line">alpha = <span class="number">1</span> / count</span><br><span class="line"><span class="comment"># alpha = [1, 2.317, 0.663, 1.008, 1.172, 0.750, 2.259, 3.55, 1.163, 0.663, 1.402]</span></span><br></pre></td></tr></table></figure>
<h2 tabindex="-1"><span id="learning-rate-diao-zheng-fang-an">Learning rate调整方案</span><a href="#learning-rate-diao-zheng-fang-an" class="header-anchor">#</a></h2>
<p>优化器选择Adam，初始学习率为0.0004，设置decay=1e-5</p>
<p>在学习率调整上，通过余弦退火调整学习率，退火周期为16，即每过16轮，学习率突然增大，然后再慢慢减小。如下图除了蓝线、红线以外的曲线所示。</p>
<img src="image-20230118225346987.png" alt="image-20230118225346987" style="zoom: 60%;">
<p>这么做好处有二，一是可以加快收敛速度，二则是可以帮助“翻越” loss surface上的一些小山峰，从而更好地跳出“局部最小”（其实往往不是局部最小，但是有“跳出”的作用）找到全局最优解。</p>
<h2 tabindex="-1"><span id="test-time-augmentation">Test time augmentation</span><a href="#test-time-augmentation" class="header-anchor">#</a></h2>
<p>训练时，我们对训练数据进行了增强，但是测试时用的就是正常的图片，但是我们想利用模型对增强的图片的识别效果辅助判断。所以可以让测试图片产生若干张增强的图片，依次求未增强图片和增强图片的预测向量，然后把向量相加后再求出最大值所在位置，就是最终预测结果。模型对同一个物体的原图和多张增强图像进行预测，一定程度上可以校正只对原图进行预测时的错误。</p>
<img src="image-20230118231119754.png" alt="image-20230118231119754" style="zoom: 60%;">
<p>比如在本次hw中，我对每张测试图片产生五张增强图片，对六张图片进行预测，得到 <code>preds</code>列表，内含未增强图像的预测结果（preds[0]）和五张增强图像的预测结果。最终的预测结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = <span class="number">0.5</span>* preds[<span class="number">0</span>] + <span class="number">0.1</span> * preds[<span class="number">1</span>] + <span class="number">0.1</span> * preds[<span class="number">2</span>] + <span class="number">0.1</span> * preds[<span class="number">3</span>] + <span class="number">0.1</span> * preds[<span class="number">4</span>] + <span class="number">0.1</span> * preds[<span class="number">5</span>]</span><br><span class="line">prediction = np.argmax(preds, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 tabindex="-1"><span id="jin-yi-bu-ti-sheng-de-ke-neng">进一步提升的可能</span><a href="#jin-yi-bu-ti-sheng-de-ke-neng" class="header-anchor">#</a></h1>
<h2 tabindex="-1"><span id="shu-ju-zeng-qiang-ke-neng-ke-yi-chang-shi-mix-up">数据增强可能可以尝试mix up</span><a href="#shu-ju-zeng-qiang-ke-neng-ke-yi-chang-shi-mix-up" class="header-anchor">#</a></h2>
<p>mix up数据增强是一种进阶的数据增强，它可以使得模型在判断时不会那么绝对，可以减小过拟合。如果要实现mix up，要改写Dataset类，并且还需要写适用于mix up的loss function。</p>
<img src="image-20230118233034792.png" alt="image-20230118233034792" style="zoom:60%;">
<h2 tabindex="-1"><span id="mo-xing-she-ji-ke-neng-ke-yi-ji-xu-chang-shi">模型设计可能可以继续尝试</span><a href="#mo-xing-she-ji-ke-neng-ke-yi-ji-xu-chang-shi" class="header-anchor">#</a></h2>
<p>过了strong baseline以后我没有继续调整模型，表格中最后一次产生的结果实际上用的是第6次训练得到的模型，只是在测试时采取tta。所以可能可以继续尝试加深模型，直至出现过拟合，再适当减小模型。</p>
<h2 tabindex="-1"><span id="learning-rate-diao-zheng-fang-an-you-dai-ti-sheng">Learning rate调整方案有待提升</span><a href="#learning-rate-diao-zheng-fang-an-you-dai-ti-sheng" class="header-anchor">#</a></h2>
<p>我的实现中退火周期时固定的，但是看了原始论文后，发现设置合适的初始退火周期，然后在每个退火周期后，将退火周期增大一倍可能是个不错的策略。</p>
<h2 tabindex="-1"><span id="cross-validation-ensemble">Cross Validation + Ensemble</span><a href="#cross-validation-ensemble" class="header-anchor">#</a></h2>
<p>简单地来说，Cross Validation + Ensemble是先把训练集和验证集重新进行划分，进行k种划分，训练出k个模型，最终用k个模型对图片进行多次预测。效仿tta的方式融合产生预测结果，此时不同的模型之间可以互相弥补缺陷，所以可以得到比较好的预测结果。</p>
<p>但是这玩意真的耗时间，第6次训练花费时间为8-9h，如果训练四个模型得35h左右……</p>
<img src="image-20230118233623821.png" alt="image-20230118233623821" style="zoom:67%;">
<h1 tabindex="-1"><span id="qi-ta-zhi-de-ji-lu-de-dong-xi">其它值得记录的东西</span><a href="#qi-ta-zhi-de-ji-lu-de-dong-xi" class="header-anchor">#</a></h1>
<h2 tabindex="-1"><span id="autodl-tensorboard-shi-yong">autodl tensorboard使用</span><a href="#autodl-tensorboard-shi-yong" class="header-anchor">#</a></h2>
<p>本次作业中我学习了autodl自带的tensorboard，发现操作十分简单。</p>
<ol>
<li>
<p>开机后打开AutoPanel</p>
<img src="image-20230118235328694.png" alt="image-20230118235328694" style="zoom:50%;">
</li>
<li>
<p>用pip下载tensorboard</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install tensorboard</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>新建一个writer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">'/root/tf-logs'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>把想要看的东西放进tensorboard</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss 曲线</span></span><br><span class="line">writer.add_scalars(main_tag=<span class="string">'Loss'</span>,</span><br><span class="line">                   tag_scalar_dict={<span class="string">'train'</span>: train_loss,</span><br><span class="line">                                    <span class="string">'valid'</span>: valid_loss},</span><br><span class="line">                   global_step=epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuarcy曲线</span></span><br><span class="line">writer.add_scalars(main_tag=<span class="string">'Accuracy'</span>,</span><br><span class="line">                   tag_scalar_dict={<span class="string">'train'</span>: <span class="built_in">float</span>(train_acc),</span><br><span class="line">                                    <span class="string">'valid'</span>: <span class="built_in">float</span>(valid_acc)},</span><br><span class="line">                   global_step=epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 各层的参数的梯度绝对值曲线</span></span><br><span class="line"><span class="keyword">for</span> name, parms <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">	writer.add_scalar(<span class="string">f"Grad/<span class="subst">{name}</span>"</span>, torch.norm(parms.grad), epoch + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 tabindex="-1"><span id="layer-de-ti-du-bian-hua">layer的梯度变化</span><a href="#layer-de-ti-du-bian-hua" class="header-anchor">#</a></h2>
<p>下面两张图时预卷积层的weight的梯度模长变化和某个残差块中的一个卷积层的weight的梯度模长变化（经过平滑后的结果）。所以说可以发现哪怕训练到几乎停滞，其实并不是陷入局部最小值，甚至不在鞍点。所以之前推测学习率调整方案还可以进一步优化。</p>
<img src="Grad_fc.1.weight.png" alt="Grad_fc.1.weight">
<p><img src="Grad_resblocks.0.0.res_function.1.weight.png" alt="Grad_resblocks.0.0.res_function.1.weight"></p>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-01-18</span><a class="tag" href="/categories/Deep-Learning/" title="Deep Learning">Deep Learning </a><a class="tag" href="/categories/Deep-Learning/李宏毅机器学习/" title="李宏毅机器学习">李宏毅机器学习 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Computer-Vision/" title="Computer Vision">Computer Vision </a><i class="fa fa-tag"></i><a class="tag" href="/tags/CNN/" title="CNN">CNN </a><i class="fa fa-tag"></i><a class="tag" href="/tags/ResNet/" title="ResNet">ResNet </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Learning-rate优化/" title="Learning rate优化">Learning rate优化 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Test-time-augmentation/" title="Test time augmentation">Test time augmentation </a><span class="leancloud_visitors"></span><span>大约2118个字, 7分钟3秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93%0Ahttp://hezj-opt.github.io/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6 GAN">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="gitalk_container" style="padding:10px"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>const gitalk = new Gitalk({
    clientID: '75fa8f8526b947111419',
    clientSecret: '28a1006aeebc5fe3d2e6d838846fa202d3362fd0',
    repo: 'comments',      // The repository of store comments,
    owner: 'hezj-opt',
    admin: ['hezj-opt'],
    id: '李宏毅机器学习-hw3-CNN-总结',      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
})
gitalk.render('gitalk_container')
</script><div id="gitalk-container" style="padding-left: 50px;padding-right: 50px;padding-bottom: 70px"></div><script>var gitalk = new Gitalk({
clientID: "",
clientSecret: "",
repo: "comments",
owner: "hezj-opt",
admin:  [""],
id: md5(decodeURI(location.pathname)),      // Ensure uniqueness and length less than 50
language: 'zh-CN' 
})
gitalk.render('gitalk-container')</script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div><script src="/js/baidu-tongji.js"></script><!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
          环境配置
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" title="hexo配置问题"><i class="post-icon gg-file-document"></i>hexo配置问题</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">18</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
          李宏毅机器学习
        </a>
      <span class="tree-list-count">17</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/" title="李宏毅机器学习-Why Deep Learning?"><i class="post-icon gg-file-document"></i>李宏毅机器学习-Why Deep Learning?</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/" title="李宏毅机器学习-hw3-CNN-总结"><i class="post-icon gg-file-document"></i>李宏毅机器学习-hw3-CNN-总结</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/" title="李宏毅机器学习-lect4 Self attention"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect4 Self attention</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6 GAN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect6 GAN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/" title="李宏毅机器学习-lect3 CNN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect3 CNN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5 Transformer"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect5 Transformer</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/" title="李宏毅机器学习-lect7 Self-Supervised Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect7 Self-Supervised Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/" title="李宏毅机器学习-为什么训练会失败"><i class="post-icon gg-file-document"></i>李宏毅机器学习-为什么训练会失败</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/" title="李宏毅机器学习-lect8 Auto-encoder"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect8 Auto-encoder</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/" title="李宏毅机器学习-lect9 Explainable ML"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect9 Explainable ML</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/" title="李宏毅机器学习-lect11 Domain Adaptation"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect11 Domain Adaptation</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/" title="李宏毅机器学习-lect10 Adversarial Attack"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect10 Adversarial Attack</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect12-Reinforcement-Learning/" title="李宏毅机器学习-lect12 Deep Reinforcement Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect12 Deep Reinforcement Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect13-Network-Compression/" title="李宏毅机器学习-lect13 Network Compression"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect13 Network Compression</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect14-Life-Long-Learning/" title="李宏毅机器学习-lect14 Life Long Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect14 Life Long Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect15-Meta-Learning/" title="李宏毅机器学习-lect15 Meta Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect15 Meta Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="李宏毅机器学习-各式各样的注意力机制"><i class="post-icon gg-file-document"></i>李宏毅机器学习-各式各样的注意力机制</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/">
          手搓记录
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历"><i class="post-icon gg-file-document"></i>记手搓ResNet的经历</a></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">
          论文阅读
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E6%88%90%E5%83%8F/">
          压缩感知成像
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E6%88%90%E5%83%8F/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E6%88%90%E5%83%8F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/" title="压缩感知成像论文阅读记录"><i class="post-icon gg-file-document"></i>压缩感知成像论文阅读记录</a></li></ul></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body></html>
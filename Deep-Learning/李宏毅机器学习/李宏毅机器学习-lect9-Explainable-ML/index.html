<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>李宏毅机器学习-lect9 Explainable ML · 核子的Blog</title><meta name="description" content="为什么我们需要可解释的机器学习
可解释机器学习的目标
Local Explaination

理解对模型决策重要的部分

如何理解
限制
为什么要理解对模型输出重要的部分


理解模型对于输入的处理

将输出进行降维
观察模型参数
Probing




Global Explaination

基"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/mylogo.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/mylogo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/mylogo.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div><details class="ltr" open><summary>目录</summary><div class="tocmenu"><p><ol class="toclist"><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#wei-shi-me-wo-men-xu-yao-ke-jie-shi-de-ji-qi-xue-xi"><span class="toclist-number">1.</span> <span class="toclist-text">为什么我们需要可解释的机器学习</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#ke-jie-shi-ji-qi-xue-xi-de-mu-biao"><span class="toclist-number">2.</span> <span class="toclist-text">可解释机器学习的目标</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#local-explaination"><span class="toclist-number">3.</span> <span class="toclist-text">Local Explaination</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#li-jie-dui-mo-xing-jue-ce-chong-yao-de-bu-fen"><span class="toclist-number">3.1.</span> <span class="toclist-text">理解对模型决策重要的部分</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#ru-he-li-jie"><span class="toclist-number">3.1.1.</span> <span class="toclist-text">如何理解</span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#xian-zhi"><span class="toclist-number">3.1.2.</span> <span class="toclist-text">限制</span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#wei-shi-me-yao-li-jie-dui-mo-xing-shu-chu-chong-yao-de-bu-fen"><span class="toclist-number">3.1.3.</span> <span class="toclist-text">为什么要理解对模型输出重要的部分</span></a></li></ol></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#li-jie-mo-xing-dui-yu-shu-ru-de-chu-li"><span class="toclist-number">3.2.</span> <span class="toclist-text">理解模型对于输入的处理</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#jiang-shu-chu-jin-xing-jiang-wei"><span class="toclist-number">3.2.1.</span> <span class="toclist-text">将输出进行降维</span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#guan-cha-mo-xing-can-shu"><span class="toclist-number">3.2.2.</span> <span class="toclist-text">观察模型参数</span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link" href="#probing"><span class="toclist-number">3.2.3.</span> <span class="toclist-text">Probing</span></a></li></ol></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#global-explaination"><span class="toclist-number">4.</span> <span class="toclist-text">Global Explaination</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#ji-yu-xian-yan-zhi-shi-de-xian-zhi"><span class="toclist-number">4.1.</span> <span class="toclist-text">基于先验知识的限制</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#ji-yu-sheng-cheng-de-xian-zhi"><span class="toclist-number">4.2.</span> <span class="toclist-text">基于生成的限制</span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#lime"><span class="toclist-number">5.</span> <span class="toclist-text">LIME</span></a></li></ol></p></div></details></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>李宏毅机器学习-lect9 Explainable ML</a></h3></div><div class="post-content"><p>
<h1 tabindex="-1"><span id="wei-shi-me-wo-men-xu-yao-ke-jie-shi-de-ji-qi-xue-xi">为什么我们需要可解释的机器学习</span><a href="#wei-shi-me-wo-men-xu-yao-ke-jie-shi-de-ji-qi-xue-xi" class="header-anchor">#</a></h1>
<p>在很多场合，仅仅让AI给出答案是不够的，我们还需要让AI告诉我们为什么它认为这是答案，比如：</p>
<ul>
<li>AI执法时除了给出判决，还需要给出它的依据</li>
<li>自动驾驶时汽车突然刹车，事后可能要分析做出突然刹车的决定的理由</li>
<li>AI开药时除了给出处方，还需要给出开这种处方的理由</li>
</ul>
<h1 tabindex="-1"><span id="ke-jie-shi-ji-qi-xue-xi-de-mu-biao">可解释机器学习的目标</span><a href="#ke-jie-shi-ji-qi-xue-xi-de-mu-biao" class="header-anchor">#</a></h1>
<p>关于这个话题有很多的观点，李宏毅老师的观点是我们并不是因为神经网络是黑箱而不敢用它的决策，因为我们的大脑其实也可以看成黑箱，但我们还是遵从大脑的决策。其实我们只是需要一个用这个决策的理由罢了。所以可解释机器学习的目标，也许只是让人有一个相信AI的理由而已。</p>
<p>Harvard做过一个心理学实验，这个实验是以不同方式询问排队使用打印机的同学是否可以插队并收集反应。</p>
<ul>
<li>“对不起，我只有5面需要打印，你能让我先用打印机吗？”——60%的人同意</li>
<li>“对不起，我只有5面需要打印，你能让我先用打印机吗？<strong>因为我赶时间</strong>。”——94%的人同意</li>
<li>“对不起，我只有5面需要打印，你能让我先用打印机吗？<strong>因为我需要打印</strong>。”——93%的人同意</li>
</ul>
<p>所以哪怕第三种询问方式的理由非常无厘头，也能大大提高同意插队的概率，所以也许决策时更多只是需要一个理由。这个实验一定程度上能佐证李宏毅老师的观点。</p>
<h1 tabindex="-1"><span id="local-explaination">Local Explaination</span><a href="#local-explaination" class="header-anchor">#</a></h1>
<blockquote>
<p><strong>让我们了解一只猫在模型“眼里”是什么样的</strong></p>
</blockquote>
<h2 tabindex="-1"><span id="li-jie-dui-mo-xing-jue-ce-chong-yao-de-bu-fen">理解对模型决策重要的部分</span><a href="#li-jie-dui-mo-xing-jue-ce-chong-yao-de-bu-fen" class="header-anchor">#</a></h2>
<h3 tabindex="-1"><span id="ru-he-li-jie">如何理解</span><a href="#ru-he-li-jie" class="header-anchor">#</a></h3>
<p>比如今天我们输入一只猫，我们希望知道哪些部分对于模型判断它是只猫是重要的，那么我们可以将它的一部分遮挡，然后观察被遮挡后的图片被模型判断出是猫的概率，如果某个部分被遮挡后的图片被认为是猫的概率较低，就可以认为这部分对模型的判断而言是重要的。</p>
<img src="image-20230201182439087.png" alt="image-20230201182439087" style="zoom:50%;">
<p>比如下面这张图中，用灰色块去遮挡第一排的图片的不同位置，产生第二排的结果。第二排中蓝色部分代表这部分被遮挡后判断正确概率较低。</p>
<img src="image-20230201182550254.png" alt="image-20230201182550254" style="zoom:50%;">
<p>如果想要更精确地了解哪部分对于模型是重要的，可以在每一个像素上加上一个很小的值，然后观察模型输出概率的变化，然后通过变化率<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">Δ</mi><mi>e</mi></mrow><mrow><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\Delta e}{\Delta x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span><span class="mord mathnormal mtight">x</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></eq> 来判断某个像素对于模型判断的重要性。那么其实这个方法就是<strong>把图片看成矩阵<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></eq>，构造<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e=f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></eq>，然后进行矩阵求导</strong>，再代入图片<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></eq>的值得到导数，也就是下图中第二排图片，即Saliency Map。</p>
<img src="image-20230201182625138.png" alt="image-20230201182625138" style="zoom:50%;">
<h3 tabindex="-1"><span id="xian-zhi">限制</span><a href="#xian-zhi" class="header-anchor">#</a></h3>
<p>但是实际上，产生的Saliency Map大部分都是含有比较多的噪声的，所以<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</a>中提出可以将原图加上不同的随机噪声，然后得到多张Saliency Map后再取平均，可以起到平滑的作用。</p>
<img src="image-20230201183049934.png" alt="image-20230201183049934" style="zoom:50%;">
<p>除此之外，可能还会遇到Gradient Saturation的问题，比如判断鼻子长度和是大象的概率的相关性时，当鼻子比较短时，是大象的概率会随鼻子长度的增长而增长，但是鼻子长度比较长的时候，可能被判断为是大象的概率就不会增长了。这个例子是说明在生成Saliency Map中，可能某些重要要素会由于Gradient Saturation的问题导致没有被检测出来。</p>
<p>对于这个问题，可以通过<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.02639">https://arxiv.org/abs/1611.02639</a>提到的Integrated gradient来解决。</p>
<img src="image-20230201183031919.png" alt="image-20230201183031919" style="zoom:50%;">
<p>下图是照相机图片的Integrated gradient图和gradient图的对比，明显可以发现Integrate gradient可以更好的解释模型为什么做出物体是照相机的判断。</p>
<img src="image-20230201204545640.png" alt="image-20230201204545640" style="zoom:50%;">
<h3 tabindex="-1"><span id="wei-shi-me-yao-li-jie-dui-mo-xing-shu-chu-chong-yao-de-bu-fen">为什么要理解对模型输出重要的部分</span><a href="#wei-shi-me-yao-li-jie-dui-mo-xing-shu-chu-chong-yao-de-bu-fen" class="header-anchor">#</a></h3>
<p>因为有时候模型并不一定是看到猫眼、猫耳等特征才知道是猫的，比如下图中左下角有一串文字，模型认为这串文字才是分类的重点，这与我们的目标是不符合的。通过观察哪些部分对模型是重要的，我们可以了解模型是不是学会了我们真正想让模型学习的任务。</p>
<img src="image-20230201182751174.png" alt="image-20230201182751174" style="zoom:50%;">
<h2 tabindex="-1"><span id="li-jie-mo-xing-dui-yu-shu-ru-de-chu-li">理解模型对于输入的处理</span><a href="#li-jie-mo-xing-dui-yu-shu-ru-de-chu-li" class="header-anchor">#</a></h2>
<h3 tabindex="-1"><span id="jiang-shu-chu-jin-xing-jiang-wei">将输出进行降维</span><a href="#jiang-shu-chu-jin-xing-jiang-wei" class="header-anchor">#</a></h3>
<p>Hinton在12年时就尝试过把语音识别系统的输出进行降维，然后发现不同语者（用不同颜色的点表示）说的同样的话的分布是接近的，所以可以认为语音识别系统真正学会了无视说话的人，而将注意力集中在说话的内容上。</p>
<img src="image-20230201183703257.png" alt="image-20230201183703257" style="zoom:50%;">
<h3 tabindex="-1"><span id="guan-cha-mo-xing-can-shu">观察模型参数</span><a href="#guan-cha-mo-xing-can-shu" class="header-anchor">#</a></h3>
<p>比如在Self-attention中，我们可以观察attention score来了解模型对于某个输入的哪些部分比较在意，以及这个Self-attention层对这个输入做了什么处理。</p>
<img src="image-20230201183723665.png" alt="image-20230201183723665" style="zoom:50%;">
<h3 tabindex="-1"><span id="probing">Probing</span><a href="#probing" class="header-anchor">#</a></h3>
<p>我们可以将模型中某一层的输出进行处理，来观察这一层之前的模型对输入做了什么处理。比如可以把BERT中某一层的输出输入一个分类器里面，进行词性的分类。但是要求分类器的性能比较好。</p>
<img src="image-20230201183822403.png" alt="image-20230201183822403" style="zoom:50%;">
<p>或者在语音识别系统中，我们把前几层的输出作为TTS（text to speech）的输入，假设发现TTS的输出的语音中说话的人已经无法被分辨，那么模型的前几层的作用可能就是把输入的声音中说话的人的特征模糊了。</p>
<img src="image-20230201183906328.png" alt="image-20230201183906328" style="zoom:50%;">
<h1 tabindex="-1"><span id="global-explaination">Global Explaination</span><a href="#global-explaination" class="header-anchor">#</a></h1>
<blockquote>
<p><strong>让我们了解模型“心中”的猫是什么样的</strong></p>
</blockquote>
<p>在CNN中，可以把卷积层的输出理解为特征图，特征图的值越大，说明图像中特征越明显，那么我们可以<strong>固定卷积层的参数，用gradient ascent（和gradient decent是类似的）的方法去找出一张让卷积层输出的特征图的所有像素的值的累加最大的图片<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">X^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span></eq></strong>。观察<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">X^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span></eq>我们可以知道各个卷积层对什么特征是感兴趣的。</p>
<img src="image-20230201184243424.png" alt="image-20230201184243424" style="zoom:50%;">
<p>比如一个做手写数字识别的12层CNN中我们对每个卷积层求它的<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">X^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></eq>，得到左侧的图片，那么发现有些层侧重检测竖线，有些层侧重检测横线。</p>
<img src="image-20230201184345197.png" alt="image-20230201184345197" style="zoom:50%;">
<h2 tabindex="-1"><span id="ji-yu-xian-yan-zhi-shi-de-xian-zhi">基于先验知识的限制</span><a href="#ji-yu-xian-yan-zhi-shi-de-xian-zhi" class="header-anchor">#</a></h2>
<p>通过之前提到的gradient ascent找出<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">X^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span></eq>的方法，我们知道了每个卷积层侧重处理什么特征。更进一步，以手写数字辨识模型为例，我们可以<strong>固定整个模型的参数，通过gradient ascent来找出使得输出为某个数字概率最大的图片</strong>，这样或许我们就可以看到模型“心中”的数字长什么样。</p>
<p>但是实际上，直接这么操作产生的图片并不会有很好的结果，而是产生下方图片左侧的马赛克状的图片。但是我们如果从adversarial attack（21年貌似先讲adversarial attack，22年才是先讲explainable ML）的角度来想，一个人眼不可见的杂讯可以使得模型判别的结果直接改变，那么就能比较好地理解为什么直接用gradient ascent会产生马赛克状的图片。</p>
<p>所以我们需要给优化函数加上一个<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></eq>项，这一项代表输出的图片有多像人眼看到的数字，构造这个函数可能需要一些先验知识，比如我们认为看到的数字中白色的像素点不会很多，所以图片总灰度值可能不会特别大，那么我们可以使<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">R(X)=-\sum_{i,j}|X_{ij}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1858em;vertical-align:-0.4358em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></eq>，从而得到了人眼看起来有点像数字的模型“心中”的数字。</p>
<img src="image-20230201184408894.png" alt="image-20230201184408894" style="zoom:50%;">
<p>下图是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.06579">https://arxiv.org/abs/1506.06579</a>中得到的部分模型“心中”的各种物体的图像。</p>
<img src="image-20230201184810312.png" alt="image-20230201184810312" style="zoom:50%;">
<h2 tabindex="-1"><span id="ji-yu-sheng-cheng-de-xian-zhi">基于生成的限制</span><a href="#ji-yu-sheng-cheng-de-xian-zhi" class="header-anchor">#</a></h2>
<p>在优化函数中加入限制项或许可以起到效果，但是需要一定的先验知识，还需要复杂的调参过程。另一种比较明朗的方法是先训练一个把向量变成图片的生成器（GAN、VAE之类），然后把生成器和分类器接在一起，固定整个模型的参数，用gradient ascent的方法找出使得输出为某个类别的概率最大的向量<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></eq>，然后模型“心中”的图片就是向量<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></eq>通过生成器<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">G</span></span></span></span></eq>的输出<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></eq>。</p>
<img src="image-20230201184509053.png" alt="image-20230201184509053" style="zoom:50%;">
<p>下图是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.00005">https://arxiv.org/abs/1612.00005</a>中得到的一些模型“心中”的火山的图像</p>
<img src="image-20230201214711501.png" alt="image-20230201214711501" style="zoom:50%;">
<h1 tabindex="-1"><span id="lime">LIME</span><a href="#lime" class="header-anchor">#</a></h1>
<p>线性的模型的可解释性是很强的，因为我们可以了解到每一个元素的权重，但是它在复杂任务上表现不佳。所以我们也许可以用线性模型来模拟复杂模型中的一小部分，来解释这一小部分的作用。</p>
<img src="image-20230201051130176.png" alt="image-20230201051130176" style="zoom:50%;">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-01-31</span><a class="tag" href="/categories/Deep-Learning/" title="Deep Learning">Deep Learning </a><a class="tag" href="/categories/Deep-Learning/李宏毅机器学习/" title="李宏毅机器学习">李宏毅机器学习 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Explainable-ML/" title="Explainable ML">Explainable ML </a><span class="leancloud_visitors"></span><span>大约2708个字, 9分钟1秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9%20Explainable%20ML%0Ahttp://hezj-opt.github.io/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/" title="李宏毅机器学习-lect10 Adversarial Attack">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/" title="李宏毅机器学习-lect8 Auto-encoder">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="gitalk_container" style="padding:10px"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>const gitalk = new Gitalk({
    clientID: '75fa8f8526b947111419',
    clientSecret: '28a1006aeebc5fe3d2e6d838846fa202d3362fd0',
    repo: 'comments',      // The repository of store comments,
    owner: 'hezj-opt',
    admin: ['hezj-opt'],
    id: '李宏毅机器学习-lect9 Explainable ML',      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
})
gitalk.render('gitalk_container')
</script><div id="gitalk-container" style="padding-left: 50px;padding-right: 50px;padding-bottom: 70px"></div><script>var gitalk = new Gitalk({
clientID: "",
clientSecret: "",
repo: "comments",
owner: "hezj-opt",
admin:  [""],
id: md5(decodeURI(location.pathname)),      // Ensure uniqueness and length less than 50
language: 'zh-CN' 
})
gitalk.render('gitalk-container')</script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div><script src="/js/baidu-tongji.js"></script><!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
          环境配置
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" title="hexo配置问题"><i class="post-icon gg-file-document"></i>hexo配置问题</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">18</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
          李宏毅机器学习
        </a>
      <span class="tree-list-count">17</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/" title="李宏毅机器学习-Why Deep Learning?"><i class="post-icon gg-file-document"></i>李宏毅机器学习-Why Deep Learning?</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/" title="李宏毅机器学习-hw3-CNN-总结"><i class="post-icon gg-file-document"></i>李宏毅机器学习-hw3-CNN-总结</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/" title="李宏毅机器学习-lect4 Self attention"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect4 Self attention</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6 GAN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect6 GAN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/" title="李宏毅机器学习-lect3 CNN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect3 CNN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5 Transformer"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect5 Transformer</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/" title="李宏毅机器学习-lect7 Self-Supervised Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect7 Self-Supervised Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/" title="李宏毅机器学习-为什么训练会失败"><i class="post-icon gg-file-document"></i>李宏毅机器学习-为什么训练会失败</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/" title="李宏毅机器学习-lect8 Auto-encoder"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect8 Auto-encoder</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/" title="李宏毅机器学习-lect9 Explainable ML"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect9 Explainable ML</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/" title="李宏毅机器学习-lect11 Domain Adaptation"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect11 Domain Adaptation</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/" title="李宏毅机器学习-lect10 Adversarial Attack"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect10 Adversarial Attack</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect12-Reinforcement-Learning/" title="李宏毅机器学习-lect12 Deep Reinforcement Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect12 Deep Reinforcement Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect13-Network-Compression/" title="李宏毅机器学习-lect13 Network Compression"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect13 Network Compression</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect14-Life-Long-Learning/" title="李宏毅机器学习-lect14 Life Long Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect14 Life Long Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect15-Meta-Learning/" title="李宏毅机器学习-lect15 Meta Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect15 Meta Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="李宏毅机器学习-各式各样的注意力机制"><i class="post-icon gg-file-document"></i>李宏毅机器学习-各式各样的注意力机制</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/">
          手搓记录
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历"><i class="post-icon gg-file-document"></i>记手搓ResNet的经历</a></li></ul></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body></html>
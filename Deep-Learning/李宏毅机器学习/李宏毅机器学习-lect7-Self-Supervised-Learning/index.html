<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>李宏毅机器学习-lect7-Self-Supervised Learning · 核子的Blog</title><meta name="description" content="自监督学习
BERT

什么是BERT
BERT的训练方法

Masking Input
Next Sentence Prediction


怎么使用BERT

文章情感判断
句子词性标记
逻辑判断
摘录型问答


BERT的评估
为什么BERT有效

解释与佐证
对之前解释的质疑


Multi"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/mylogo.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/mylogo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/mylogo.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div><details class="ltr" open><summary>目录</summary><div class="tocmenu"><p><ol class="toclist"><li class="toclist-item toclist-level-1"><a class="toclist-link"><span class="toclist-number">1.</span> <span class="toclist-text">自监督学习 </span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link"><span class="toclist-number">2.</span> <span class="toclist-text">BERT </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.1.</span> <span class="toclist-text">什么是BERT </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.2.</span> <span class="toclist-text">BERT的训练方法 </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.2.1.</span> <span class="toclist-text">Masking Input </span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.2.2.</span> <span class="toclist-text">Next Sentence Prediction </span></a></li></ol></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.3.</span> <span class="toclist-text">怎么使用BERT </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.3.1.</span> <span class="toclist-text">文章情感判断 </span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.3.2.</span> <span class="toclist-text">句子词性标记 </span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.3.3.</span> <span class="toclist-text">逻辑判断 </span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.3.4.</span> <span class="toclist-text">摘录型问答 </span></a></li></ol></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.4.</span> <span class="toclist-text">BERT的评估 </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.5.</span> <span class="toclist-text">为什么BERT有效 </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.5.1.</span> <span class="toclist-text">解释与佐证 </span></a></li><li class="toclist-item toclist-level-3"><a class="toclist-link"><span class="toclist-number">2.5.2.</span> <span class="toclist-text">对之前解释的质疑 </span></a></li></ol></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">2.6.</span> <span class="toclist-text">Multi-lingual BERT </span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link"><span class="toclist-number">3.</span> <span class="toclist-text">GPT </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">3.1.</span> <span class="toclist-text">GPT的训练 </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">3.2.</span> <span class="toclist-text">GPT的使用 </span></a></li></ol></li><li class="toclist-item toclist-level-1"><a class="toclist-link"><span class="toclist-number">4.</span> <span class="toclist-text">自监督学习在语音和图像领域上的应用 </span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.1.</span> <span class="toclist-text">概述 </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.2.</span> <span class="toclist-text">Generative Approaches </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.3.</span> <span class="toclist-text">Predictive Approaches </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.4.</span> <span class="toclist-text">Contrastive Learning </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.5.</span> <span class="toclist-text">Bootstrapping Approaches </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.6.</span> <span class="toclist-text">Simply Extra Regularization </span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link"><span class="toclist-number">4.7.</span> <span class="toclist-text">总结 </span></a></li></ol></li></ol></p></div></details></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>李宏毅机器学习-lect7-Self-Supervised Learning</a></h3></div><div class="post-content"><p><!-- toc -->
<ul>
<li><a href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">自监督学习</a></li>
<li><a href="#bert">BERT</a>
<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AFbert">什么是BERT</a></li>
<li><a href="#bert%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95">BERT的训练方法</a>
<ul>
<li><a href="#masking-input">Masking Input</a></li>
<li><a href="#next-sentence-prediction">Next Sentence Prediction</a></li>
</ul>
</li>
<li><a href="#%E6%80%8E%E4%B9%88%E4%BD%BF%E7%94%A8bert">怎么使用BERT</a>
<ul>
<li><a href="#%E6%96%87%E7%AB%A0%E6%83%85%E6%84%9F%E5%88%A4%E6%96%AD">文章情感判断</a></li>
<li><a href="#%E5%8F%A5%E5%AD%90%E8%AF%8D%E6%80%A7%E6%A0%87%E8%AE%B0">句子词性标记</a></li>
<li><a href="#%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD">逻辑判断</a></li>
<li><a href="#%E6%91%98%E5%BD%95%E5%9E%8B%E9%97%AE%E7%AD%94">摘录型问答</a></li>
</ul>
</li>
<li><a href="#bert%E7%9A%84%E8%AF%84%E4%BC%B0">BERT的评估</a></li>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88bert%E6%9C%89%E6%95%88">为什么BERT有效</a>
<ul>
<li><a href="#%E8%A7%A3%E9%87%8A%E4%B8%8E%E4%BD%90%E8%AF%81">解释与佐证</a></li>
<li><a href="#%E5%AF%B9%E4%B9%8B%E5%89%8D%E8%A7%A3%E9%87%8A%E7%9A%84%E8%B4%A8%E7%96%91">对之前解释的质疑</a></li>
</ul>
</li>
<li><a href="#multi-lingual-bert">Multi-lingual BERT</a></li>
</ul>
</li>
<li><a href="#gpt">GPT</a>
<ul>
<li><a href="#gpt%E7%9A%84%E8%AE%AD%E7%BB%83">GPT的训练</a></li>
<li><a href="#gpt%E7%9A%84%E4%BD%BF%E7%94%A8">GPT的使用</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%AF%AD%E9%9F%B3%E5%92%8C%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8">自监督学习在语音和图像领域上的应用</a>
<ul>
<li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li>
<li><a href="#generative-approaches">Generative Approaches</a></li>
<li><a href="#predictive-approaches">Predictive Approaches</a></li>
<li><a href="#contrastive-learning">Contrastive Learning</a></li>
<li><a href="#bootstrapping-approaches">Bootstrapping Approaches</a></li>
<li><a href="#simply-extra-regularization">Simply Extra Regularization</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<h1 tabindex="-1"><span id="自监督学习">自监督学习 </span></h1>
<p>在监督学习中，我们用成对的训练资料进行训练（比如中译英人物中中文文章与其对应的英文翻译），而在无监督学习当中，是<strong>没有成对的训练资料</strong>的（比如不带英文翻译的中文文章）。</p>
<p><strong>自监督学习是无监督学习的一种</strong>，他将训练资料x的分为x’和x’‘，用x’进行训练，然后用x’'进行验证。</p>
<p>自监督学习现在被广泛用于大模型的预训练，比如BERT和GPT。</p>
<img src="image-20230126201112354.png" alt="image-20230126201112354" style="zoom:50%;">
<h1 tabindex="-1"><span id="bert">BERT </span></h1>
<h2 tabindex="-1"><span id="什么是bert">什么是BERT </span></h2>
<p>BERT是<s>芝麻街里的人物</s>一个巨大的预训练语言表征模型，它的全称是Bidirectional Encoder Representations from Transformers。它强调了不再像以往（Transformer）一样采用传统的单向（每一个token的注意力只能在之前的token上）语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，BERT自监督训练时每一个token的注意力可以放到全局。</p>
<img src="image-20230126202044318.png" alt="image-20230126202044318" style="zoom: 33%;">
<p>BERT在预训练后，只需要接上一个额外的输出层，给予一点带标注的资料微调，就可以在翻译、问答等下游语言任务中有优秀的表现。</p>
<p>从BERT开始，一大批巨大模型开始出现，如GPT、T5等。</p>
<img src="image-20230126203617783.png" alt="image-20230126203617783" style="zoom: 33%;">
<h2 tabindex="-1"><span id="bert的训练方法">BERT的训练方法 </span></h2>
<h3 tabindex="-1"><span id="masking-input">Masking Input </span></h3>
<p>用一句话概括Masking Input就是让BERT学会做“填空题”，这里的填空题是指在把输入的一些token换成特定的token或者随机的token，从而使完整的句子中出现空位，BERT需要学会预测这些被替换的token是什么，训练时目标是最小化输出token和正确token的交叉熵。</p>
<img src="image-20230126204753160.png" alt="image-20230126204753160" style="zoom:50%;">
<h3 tabindex="-1"><span id="next-sentence-prediction">Next Sentence Prediction </span></h3>
<p>这个方法是输入两个句子，判断sentence 1与sentence 2在一篇文章中的前后关系。</p>
<img src="image-20230126205911193.png" alt="image-20230126205911193" style="zoom:50%;">
<h2 tabindex="-1"><span id="怎么使用bert">怎么使用BERT </span></h2>
<p>经过预训练后的BERT学会了做“填空题”、“排序题”，此时的BERT像干细胞，没有具体的功能，但有分化为执行具体任务的能力。</p>
<p>要让BERT能做具体的任务，我们需要在BERT后加入一个输出层（BERT其实和Transformer的Encoder类似，所以后面需要输出层），然后再给予一点具体任务的成对资料进行训练（这个过程称为Fine-tune，即微调），就可以让BERT做具体的任务。<strong>微调前，输出层的参数是随机的，而BERT的参数是经过预训练的</strong>。</p>
<img src="image-20230126211906979.png" alt="image-20230126211906979" style="zoom:50%;">
<p>下面举四个例子具体说明如何使用BERT</p>
<h3 tabindex="-1"><span id="文章情感判断">文章情感判断 </span></h3>
<img src="image-20230126213029521.png" alt="image-20230126213029521" style="zoom: 40%;">
<h3 tabindex="-1"><span id="句子词性标记">句子词性标记 </span></h3>
<img src="image-20230126215031379.png" alt="image-20230126215031379" style="zoom:40%;">
<h3 tabindex="-1"><span id="逻辑判断">逻辑判断 </span></h3>
<img src="image-20230126215918853.png" alt="image-20230126215918853" style="zoom:40%;">
<h3 tabindex="-1"><span id="摘录型问答">摘录型问答 </span></h3>
<img src="image-20230126220108302.png" alt="image-20230126220108302" style="zoom:40%;">
<img src="image-20230126220149250.png" alt="image-20230126220149250" style="zoom:40%;">
<img src="image-20230126220259605.png" alt="image-20230126220259605" style="zoom:40%;">
<h2 tabindex="-1"><span id="bert的评估">BERT的评估 </span></h2>
<p>BERT常用GLUE评估，GLUE里面有九项语言相关任务，通过BERT在九项任务上的性能综合评估BERT。</p>
<img src="image-20230126220651490.png" alt="image-20230126220651490" style="zoom:50%;">
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.00537">https://arxiv.org/abs/1905.00537</a>中总结：随着时间推进，BERT的表现越来越好，甚至在多项任务上都能超过人的表现（水平直线）。</p>
<img src="image-20230126220834525.png" alt="image-20230126220834525" style="zoom:50%;">
<h2 tabindex="-1"><span id="为什么bert有效">为什么BERT有效 </span></h2>
<h3 tabindex="-1"><span id="解释与佐证">解释与佐证 </span></h3>
<p>最常见的一种解释是BERT通过做“填空题”或其他预训练任务，真的“学会了”语言，能根据上下文将文字映射到语义空间上，<strong>相近意思的词在语义空间上的映射会比较接近</strong>。比如同样是“苹”这个字，吃苹果中的“苹”和“果”、“草”的映射比较接近，苹果手机中的“苹”可能和“电”的映射比较接近。所以在BERT对每个字做了有效的映射后，输出层可以很容易地输出正确结果。</p>
<img src="image-20230126221418644.png" alt="image-20230126221418644" style="zoom:40%;">
<p>取BERT对不同语境下的“苹”的映射，求各个映射之间Cosine Similarity，发现相近语义的“苹”字之间Cosine Similarity比较高，不用语义的“苹”字之间Cosine Similarity比较低。这个实验在一定程度上能说明BERT为什么有效。</p>
<img src="image-20230126222805258.png" alt="image-20230126222805258" style="zoom:40%;">
<h3 tabindex="-1"><span id="对之前解释的质疑">对之前解释的质疑 </span></h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.07162">https://arxiv.org/abs/2103.07162</a>中指出，用语言资料训练的BERT用在蛋白质、基因、音乐领域的任务也会有比不用预训练的方法表现更好，所以BERT有效的原因真的是因为BERT“学会了”语言吗？BERT有效的原因目前还在探索中。</p>
<img src="image-20230126225643687.png" alt="image-20230126225643687" style="zoom:75%;">
<h2 tabindex="-1"><span id="multi-lingual-bert">Multi-lingual BERT </span></h2>
<p>之前提到的BERT都是单一语言的BERT，但是实际上，预训练BERT的时候可以用多语言的资料进行训练。比如下图指的就是让BERT做不同语言的“填空题”。</p>
<img src="image-20230126232629997.png" alt="image-20230126232629997" style="zoom:50%;">
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.09587">https://arxiv.org/abs/1909.09587</a>中提出在zero-shot（提问时没有给出任何参考案例）阅读理解任务中，多Multilingual-BERT在只经过英文问答题的微调后，做中文的阅读题时竟然还能有不低的正确率。</p>
<img src="image-20230126232953799.png" alt="image-20230126232953799" style="zoom:50%;">
<p>一种解释是对于Multilingual-BERT，其实不同的语言的相同语义的词在语义空间上的映射是很接近的，但是训练这样的Multilingual-BERT是需要大量的资料与运算资源，才能让BERT学到不同语言之间的联系。</p>
<img src="image-20230126234055887.png" alt="image-20230126234055887" style="zoom:67%;">
<p>但是，如果语言之间真的没有差距，那么比如做英文问答时不会冒出别的语言吗？而这实际上是不会的，因为其实不同语言之间还是有一定的距离的。所以李宏毅课题组进行了一项实验（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.10041">https://arxiv.org/abs/2010.10041</a>），把中文词汇映射的平均值减去英文词汇映射的平均值，得到两种语言之间的差距。然后让Multilingual-BERT读入英文句子，加上英文到中文之间的差距，竟然能输出中文的句子了，而且部分中英词汇在语义上是相同的。所以一定程度上说明了<strong>Multilingual-BERT中蕴藏了语言之间的联系</strong>。</p>
<img src="image-20230127002234482.png" alt="image-20230127002234482" style="zoom:50%;">
<h1 tabindex="-1"><span id="gpt">GPT </span></h1>
<p>GPT的训练和使用其实和BERT是很类似的。</p>
<h2 tabindex="-1"><span id="gpt的训练">GPT的训练 </span></h2>
<p>与BERT相似的是，GPT同样使用自监督学习进行预训练，但是预训练的任务是预测句子中的下一个token。</p>
<img src="image-20230127002639296.png" alt="image-20230127002639296" style="zoom:40%;">
<h2 tabindex="-1"><span id="gpt的使用">GPT的使用 </span></h2>
<p>和BERT相似的是，GPT在迁移到下游任务时也需要一点成对的训练资料，但是和BERT不同的是，GPT的模型比BERT大得多，以至于连微调GPT都是很困难的，所以GPT在微调是不对GPT的参数进行调节的，并采用few-shot learning（提供问题和多个范例，让GPT给出答案）、one-shot learning（提供问题和一个范例，让GPT给出答案）甚至zero-shot learning（只提供问题，直接要求GPT给出答案）的方法进行训练。</p>
<img src="image-20230127004819292.png" alt="image-20230127004819292" style="zoom:50%;">
<h1 tabindex="-1"><span id="自监督学习在语音和图像领域上的应用">自监督学习在语音和图像领域上的应用 </span></h1>
<h2 tabindex="-1"><span id="概述">概述 </span></h2>
<h2 tabindex="-1"><span id="generative-approaches">Generative Approaches </span></h2>
<h2 tabindex="-1"><span id="predictive-approaches">Predictive Approaches </span></h2>
<h2 tabindex="-1"><span id="contrastive-learning">Contrastive Learning </span></h2>
<h2 tabindex="-1"><span id="bootstrapping-approaches">Bootstrapping Approaches </span></h2>
<h2 tabindex="-1"><span id="simply-extra-regularization">Simply Extra Regularization </span></h2>
<h2 tabindex="-1"><span id="总结">总结 </span></h2>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-01-26</span><a class="tag" href="/categories/Deep-Learning/" title="Deep Learning">Deep Learning </a><a class="tag" href="/categories/Deep-Learning/李宏毅机器学习/" title="李宏毅机器学习">李宏毅机器学习 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Self-Supervised-Learning/" title="Self-Supervised Learning">Self-Supervised Learning </a><i class="fa fa-tag"></i><a class="tag" href="/tags/Bert/" title="Bert">Bert </a><span class="leancloud_visitors"></span><span>大约1975个字, 6分钟35秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised%20Learning%0Ahttp://hezj-opt.github.io/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6-GAN">下一篇</a></li></ul></div><script src="/js/visitors.js"></script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div><!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">8</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
          李宏毅机器学习
        </a>
      <span class="tree-list-count">7</span><ul class="tree-list-children"><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/" title="李宏毅机器学习-lect3-CNN">李宏毅机器学习-lect3-CNN</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">CNN 的想法 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">卷积层（Convolutional layers） </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">卷积核 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">步长 （Stride） </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">填充 （Padding） </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">参数共享（Parameter Sharing） </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">卷积层的运行过程 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">卷积层的两种理解方式 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">池化层（Pooling layers） </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Flatten layers </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">CNN 的缺点 </span></a></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/" title="李宏毅机器学习-hw3-CNN-总结">李宏毅机器学习-hw3-CNN-总结</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">任务介绍 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">训练结果 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">训练方法 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Data augmentation </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">模型设计 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Loss function的选择与调参 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Learning rate调整方案 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Test time augmentation </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">进一步提升的可能 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">数据增强可能可以尝试mix up </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">模型设计可能可以继续尝试 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Learning rate调整方案有待提升 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Cross Validation + Ensemble </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">其它值得记录的东西 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">autodl tensorboard使用 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">layer的梯度变化 </span></a></li></ol></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/" title="李宏毅机器学习-lect4-Self attention">李宏毅机器学习-lect4-Self attention</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Self attention 解决什么问题 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">FC(Fully Connected)有什么不足 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Self attention 的架构 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Self attention 的基本计算过程 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">求attention score </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">求解Self attention层输出的向量 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">矩阵视角下的计算过程 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Self attention的优化 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Multi-head Self attention </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Positional Encoding </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Self attention 和其他网络对比 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">CNN </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">RNN </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GNN </span></a></li></ol></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5-Transformer">李宏毅机器学习-lect5-Transformer</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Transformer是什么 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Transformer的应用 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Transformer的架构 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Encoder </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Decoder </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Autoregressive Transformer </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Masked Self-attention </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Cross attention </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Non autoregressive Transformer(NAT) </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Transformer的训练 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">loss的来源 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">训练的一些tips </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Teacher Forcing </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Copy Mechanism </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Guided Attention </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Beam Search </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Exposure bias </span></a></li></ol></li></ol></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6-GAN">李宏毅机器学习-lect6-GAN</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GAN的基本思想 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">生成器 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">“对抗”的含义 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">训练过程概述 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GAN的理论 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">目标函数 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">JS divergence的缺陷 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Wasserstein distance </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">评价生成器 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">评价指标 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">图像质量 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">图像多样性 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">评价函数 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Inception score </span></a></li><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">FID </span></a></li></ol></li></ol></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Conditional GAN（条件式生成） </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Conditional-GAN+监督学习 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Cycle-GAN </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">为什么需要Cycle-GAN </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Cycle-GAN的结构 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Cycle-GAN的其他应用 </span></a></li></ol></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/" title="李宏毅机器学习-lect7-Self-Supervised Learning">李宏毅机器学习-lect7-Self-Supervised Learning</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">自监督学习 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">BERT </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">什么是BERT </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">BERT的训练方法 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Masking Input </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Next Sentence Prediction </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">怎么使用BERT </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">文章情感判断 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">句子词性标记 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">逻辑判断 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">摘录型问答 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">BERT的评估 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">为什么BERT有效 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">解释与佐证 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">对之前解释的质疑 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Multi-lingual BERT </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GPT </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GPT的训练 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">GPT的使用 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">自监督学习在语音和图像领域上的应用 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">概述 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Generative Approaches </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Predictive Approaches </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Contrastive Learning </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Bootstrapping Approaches </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Simply Extra Regularization </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">总结 </span></a></li></ol></li></ol></li><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/" title="李宏毅机器学习-为什么训练会失败">李宏毅机器学习-为什么训练会失败</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Training loss很高 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">这时发生了什么 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">训练方法 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Batch </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Batch对训练效率影响 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-4"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Batch对训练效果影响 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Momentum </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Learning rate </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Loss function </span></a></li></ol></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Training data的loss很小 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">过拟合 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">调整模型复杂度 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">添加更多数据 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Mismatch </span></a></li></ol></li></ol></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/">
          手搓记录
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历">记手搓ResNet的经历</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">前言 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">ResNet基本思想 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">代码 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">各部分详解 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Residual block </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">resblock_basic类 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-3"><a class="tree-post-toc-link"><span class="tree-post-toc-text">resblock_bottlenect类 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">resnet类 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">References </span></a></li></ol></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
          环境配置
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><i class="toggle-toc-icon gg-file-add"></i><a class="tree-list-post-link" href="/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" title="hexo配置问题">hexo配置问题</a><ol class="tree-post-toc"><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">第一个网页 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">部署到github </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">配置主题 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">安装 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">配置 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">正文部分 </span></a><ol class="tree-post-toc-child"><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">写入正文 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">编辑文章的分类、标签 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">插入图片 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">插入公式 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">插入TOC目录 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-2"><a class="tree-post-toc-link"><span class="tree-post-toc-text">插入代码 </span></a></li></ol></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">About页 </span></a></li><li class="tree-post-toc-item tree-post-toc-level-1"><a class="tree-post-toc-link"><span class="tree-post-toc-text">Tag页 </span></a></li></ol></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body></html>
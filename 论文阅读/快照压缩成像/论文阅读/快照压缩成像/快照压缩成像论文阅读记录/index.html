<!DOCTYPE html><html lang="zh-CN" id="theme-light-mode"><head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="核子"><title>快照压缩成像论文阅读记录 · 核子的Blog</title><meta name="description" content="综合
成像系统
算法

综述
End2End
Deep unfolding
PnP
Untrained NN





开个文档push自己读文章……
综合#

Snapshot Compressive Imaging: Theory, Algorithms, and Applications


"><meta name="keywords" content="Blog,博客,Hexo"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/mylogo.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/">首页</a></li><li> <a href="/archives">归档</a></li><li> <a href="/categories">分类</a></li><li> <a href="/tags">标签</a></li><li> <a href="/about">关于</a></li><li> <a href="/links">友链</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/images/mylogo.webp"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/images/mylogo.webp" style="width:220px;" alt="favicon"><h3 title=""><a href="/">核子的Blog</a></h3><div class="description"><p>A simple blog</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/hezj-opt"><i class="fa fa-github"></i></a></li><li><a href="mailto:zijun_he@zju.edu.cn"><i class="fa fa-envelope"></i></a></li></ul></div><details class="ltr" open><summary>目录</summary><div class="tocmenu"><p><ol class="toclist"><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#zong-he"><span class="toclist-number">1.</span> <span class="toclist-text">综合</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#cheng-xiang-xi-tong"><span class="toclist-number">2.</span> <span class="toclist-text">成像系统</span></a></li><li class="toclist-item toclist-level-1"><a class="toclist-link" href="#suan-fa"><span class="toclist-number">3.</span> <span class="toclist-text">算法</span></a><ol class="toclist-child"><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#zong-shu"><span class="toclist-number">3.1.</span> <span class="toclist-text">综述</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#end2end"><span class="toclist-number">3.2.</span> <span class="toclist-text">End2End</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#deep-unfolding"><span class="toclist-number">3.3.</span> <span class="toclist-text">Deep unfolding</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#pnp"><span class="toclist-number">3.4.</span> <span class="toclist-text">PnP</span></a></li><li class="toclist-item toclist-level-2"><a class="toclist-link" href="#untrained-nn"><span class="toclist-number">3.5.</span> <span class="toclist-text">Untrained NN</span></a></li></ol></li></ol></p></div></details></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> 核子</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>快照压缩成像论文阅读记录</a></h3></div><div class="post-content"><p>
<p>开个文档push自己读文章……</p>
<h1 tabindex="-1"><span id="zong-he">综合</span><a href="#zong-he" class="header-anchor">#</a></h1>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9363502">Snapshot Compressive Imaging: Theory, Algorithms, and Applications</a></p>
</blockquote>
<ul>
<li>首次阅读时间：2023.2.9-2023.2.10</li>
<li>主要内容：系统性回顾了2021年之前的快照压缩成像的<strong>硬件系统、数学模型、理论、算法</strong></li>
<li>详细笔记：TBA</li>
</ul>
<h1 tabindex="-1"><span id="cheng-xiang-xi-tong">成像系统</span><a href="#cheng-xiang-xi-tong" class="header-anchor">#</a></h1>
<p>To Be Add</p>
<h1 tabindex="-1"><span id="suan-fa">算法</span><a href="#suan-fa" class="header-anchor">#</a></h1>
<h2 tabindex="-1"><span id="zong-shu">综述</span><a href="#zong-shu" class="header-anchor">#</a></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12317/2641748/Recent-advances-of-deep-learning-for-spectral-snapshot-compressive-imaging/10.1117/12.2641748.short?SSO=1">Recent advances of deep learning for spectral snapshot compressive imaging </a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.2.11</p>
</li>
<li>
<p>主要内容：回顾了2023年之前的光谱快照压缩成像算法（基于deep learning）</p>
</li>
<li>
<p>重点</p>
<ul>
<li>深度先验+PnP是值得关注的方向</li>
<li>Transformer在光谱快照压缩成像中开始发挥其优势</li>
</ul>
</li>
</ul>
<h2 tabindex="-1"><span id="end2end">End2End</span><a href="#end2end" class="header-anchor">#</a></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/html/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.html">Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.2.14</p>
</li>
<li>
<p>创新点：</p>
<ul>
<li><strong>首次将Transformer用于光谱SCI</strong></li>
<li><strong>光谱注意力机制</strong>（S-MSA，将各波长的二维图拉伸得到的向量看作token）</li>
<li><strong>掩膜引导机制</strong>（使得S-MSA模块关注具有光谱图像中高保真度的区域）</li>
</ul>
<img src="image-20230302012515320.png" alt="image-20230302012515320" style="zoom:50%;">
</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-19790-1_41">Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.2.27</p>
</li>
<li>
<p>创新点：</p>
<ul>
<li><strong>SASM</strong>（光谱感知筛选机制，a图）：让模型关注信息丰富的区域（轻微降低性能但大大提高计算效率）</li>
<li><strong>SAH-MSA</strong>（光谱聚合散列自注意力机制，c图）：通过hashing，把每个patch中强相关的token归为一组，在组内进行self-attention。</li>
</ul>
<img src="image-20230302012907642.png" alt="image-20230302012907642" style="zoom:50%;">
</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9741335">Recurrent Neural Networks for Snapshot Compressive Imaging</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.4.1</p>
</li>
<li>
<p>创新点：</p>
<ul>
<li>提出了通过measurement和mask生成每一帧的平均测量值<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>Y</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8201em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">ˉ</span></span></span></span></span></span></span></span></span></span></eq>和每一帧的初始值的预处理方法</li>
<li>提出了<strong>AttRes-CNN</strong>，用于重建第一帧</li>
<li>提出了<strong>双向RNN网络</strong>用于重建其余帧</li>
<li>结合MSE Loss和<strong>对抗性训练</strong></li>
</ul>
<img src="image-20230403092204002.png" alt="image-20230403092204002" style="zoom: 60%;">
</li>
</ul>
<h2 tabindex="-1"><span id="deep-unfolding">Deep unfolding</span><a href="#deep-unfolding" class="header-anchor">#</a></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.10102">Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.3.1</p>
</li>
<li>
<p>创新点：</p>
<ul>
<li>
<p><strong>第一个基于Transformer的深度展开方法-DAUHST</strong>（2022.5）</p>
</li>
<li>
<p>提出<strong>DAUF</strong>，从压缩图像和物理掩码中估计参数，然后使用这些参数来控制每次迭代，开始从退化模型指导迭代</p>
</li>
<li>
<p>提出<strong>Half-Shuffle Transformer (HST)</strong>，它可以同时捕获本地内容和远程依赖项，克服了CNN或local Transformer（捕获远程依赖有局限）以及global Transformer的缺陷（计算成本高）</p>
<img src="image-20230302013526525.png" alt="image-20230302013526525" style="zoom:50%;">
</li>
</ul>
</li>
</ul>
<h2 tabindex="-1"><span id="pnp">PnP</span><a href="#pnp" class="header-anchor">#</a></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Plug-and-Play_Algorithms_for_Large-Scale_Snapshot_Compressive_Imaging_CVPR_2020_paper.html">Plug-and-Play Algorithms for Large-Scale Snapshot Compressive Imaging</a></p>
</blockquote>
<ul>
<li>首次阅读时间：忘了……</li>
<li>创新点：将深度去噪网络FFDNet集成到PnP-GAP中，具有较低计算量，并且首次表明 PnP 可以从二维快照测量中恢复 FHD 彩色视频（3840 × 1644 × 48，PNSR 高于 30dB）。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://opg.optica.org/prj/fulltext.cfm?uri=prj-9-2-B18&amp;id=446778">Deep plug-and-play priors for spectral snapshot compressive imaging</a></p>
</blockquote>
<ul>
<li>首次阅读时间：忘了……</li>
<li>创新点：
<ul>
<li>首次提出PnP方法重建高光谱图像</li>
<li>考虑到光谱特性，重建每一帧时用与其相邻的帧进行重建，并且传入noise map调控去噪强度</li>
</ul>
</li>
</ul>
<h2 tabindex="-1"><span id="untrained-nn">Untrained NN</span><a href="#untrained-nn" class="header-anchor">#</a></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.html">Deep Image Prior</a></p>
</blockquote>
<ul>
<li>首次阅读时间：2023.2.11</li>
<li>创新点：用<strong>未经训练</strong>的网络作为先验，可用于去噪、超分辨等一系列任务</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://opg.optica.org/ol/abstract.cfm?uri=ol-46-8-1888">Snapshot temporal compressive microscopy using an iterative algorithm with untrained neural networks</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.2.12</p>
</li>
<li>
<p>创新点：<strong>GAP-TV + deep image prior</strong>混合优化，用于视频SCI。也是首次将DIP用于SCI（2021年2月前没有将DIP用于SCI）。</p>
<img src="image-20230302005859408.png" alt="image-20230302005859408" style="zoom: 50%;">
</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Meng_Self-Supervised_Neural_Networks_for_Spectral_Snapshot_Compressive_Imaging_ICCV_2021_paper.html">Self-Supervised Neural Networks for Spectral Snapshot Compressive Imaging</a></p>
</blockquote>
<ul>
<li>首次阅读时间：2023.2.12</li>
<li>创新点：提出<strong>PnP-DIP/PnP-DIP-HSI算法，使用ADMM（+HSI先验）+DIP</strong>，将DIP集成到PnP中进行混合优化，用于光谱SCI。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://opg.optica.org/ol/abstract.cfm?uri=ol-48-1-109">Coded aperture compressive temporal imaging using complementary codes and untrained neural networks for high-quality reconstruction</a></p>
</blockquote>
<ul>
<li>首次阅读时间：2023.2.13</li>
<li>创新点：<strong>互补编码+UNN解码</strong>，用于视频SCI。互补编码和UNN也许是“天生一对”，那么或许也会有有别的编码方式和别的算法是“天作之合”</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html">SwinIR: Image Restoration Using Swin Transformer</a></p>
</blockquote>
<ul>
<li>
<p>首次阅读时间：2023.2.28</p>
</li>
<li>
<p>创新点：用<strong>Swin Transformer</strong>进行图像修复、超分辨等。流程：浅层信息提取+深层信息提取+重建</p>
<img src="image-20230302011150365.png" alt="image-20230302011150365" style="zoom:50%;">
</li>
</ul>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script></p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>作者: 核子</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2023-03-01</span><a class="tag" href="/categories/论文阅读/" title="论文阅读">论文阅读 </a><a class="tag" href="/categories/论文阅读/快照压缩成像/" title="快照压缩成像">快照压缩成像 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/计算成像/" title="计算成像">计算成像 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/论文阅读/" title="论文阅读">论文阅读 </a><i class="fa fa-tag"></i><a class="tag" href="/tags/快照压缩成像/" title="快照压缩成像">快照压缩成像 </a><span class="leancloud_visitors"></span><span>大约1075个字, 3分钟35秒读完</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/intent/tweet?text=%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0%EF%BC%9A%0A%0A%E6%A0%B8%E5%AD%90%E7%9A%84Blog%20%C2%B7%20%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95%0Ahttp://hezj-opt.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/%0A"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/C/Cpp/Cpp98/Introduction%20to%20Programming%20with%20C++%20%E5%95%83%E4%B9%A6%E8%AE%B0%E5%BD%95/" title="《Introduction to Programming with C++》读书笔记">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="李宏毅机器学习-各式各样的注意力机制">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="gitalk_container" style="padding:10px"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>const gitalk = new Gitalk({
    clientID: '75fa8f8526b947111419',
    clientSecret: '28a1006aeebc5fe3d2e6d838846fa202d3362fd0',
    repo: 'comments',      // The repository of store comments,
    owner: 'hezj-opt',
    admin: ['hezj-opt'],
    id: '快照压缩成像论文阅读记录',      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
})
gitalk.render('gitalk_container')
</script><div id="gitalk-container" style="padding-left: 50px;padding-right: 50px;padding-bottom: 70px"></div><script>var gitalk = new Gitalk({
clientID: "",
clientSecret: "",
repo: "comments",
owner: "hezj-opt",
admin:  [""],
id: md5(decodeURI(location.pathname)),      // Ensure uniqueness and length less than 50
language: 'zh-CN' 
})
gitalk.render('gitalk-container')</script></div></div></div></div><script src="/js/darkLightToggle.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(无标题)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div><script src="/js/baidu-tongji.js"></script><!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
          环境配置
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/hexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" title="hexo配置问题"><i class="post-icon gg-file-document"></i>hexo配置问题</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">18</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
          李宏毅机器学习
        </a>
      <span class="tree-list-count">17</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Why-Deep-Learning/" title="李宏毅机器学习-Why Deep Learning?"><i class="post-icon gg-file-document"></i>李宏毅机器学习-Why Deep Learning?</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-hw3-CNN-%E6%80%BB%E7%BB%93/" title="李宏毅机器学习-hw3-CNN-总结"><i class="post-icon gg-file-document"></i>李宏毅机器学习-hw3-CNN-总结</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect4-Self-attention/" title="李宏毅机器学习-lect4 Self attention"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect4 Self attention</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect6-GAN/" title="李宏毅机器学习-lect6 GAN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect6 GAN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect3-CNN/" title="李宏毅机器学习-lect3 CNN"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect3 CNN</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect5-Transformer/" title="李宏毅机器学习-lect5 Transformer"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect5 Transformer</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect7-Self-Supervised-Learning/" title="李宏毅机器学习-lect7 Self-Supervised Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect7 Self-Supervised Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%A4%B1%E8%B4%A5/" title="李宏毅机器学习-为什么训练会失败"><i class="post-icon gg-file-document"></i>李宏毅机器学习-为什么训练会失败</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect8-Auto-encoder/" title="李宏毅机器学习-lect8 Auto-encoder"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect8 Auto-encoder</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect9-Explainable-ML/" title="李宏毅机器学习-lect9 Explainable ML"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect9 Explainable ML</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect11-Domain-Adaptation/" title="李宏毅机器学习-lect11 Domain Adaptation"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect11 Domain Adaptation</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect10-Adversarial-Attack/" title="李宏毅机器学习-lect10 Adversarial Attack"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect10 Adversarial Attack</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect12-Reinforcement-Learning/" title="李宏毅机器学习-lect12 Deep Reinforcement Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect12 Deep Reinforcement Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect13-Network-Compression/" title="李宏毅机器学习-lect13 Network Compression"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect13 Network Compression</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect14-Life-Long-Learning/" title="李宏毅机器学习-lect14 Life Long Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect14 Life Long Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lect15-Meta-Learning/" title="李宏毅机器学习-lect15 Meta Learning"><i class="post-icon gg-file-document"></i>李宏毅机器学习-lect15 Meta Learning</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="李宏毅机器学习-各式各样的注意力机制"><i class="post-icon gg-file-document"></i>李宏毅机器学习-各式各样的注意力机制</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/">
          手搓记录
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/Deep-Learning/%E6%89%8B%E6%90%93%E8%AE%B0%E5%BD%95/%E8%AE%B0%E6%89%8B%E6%90%93ResNet%E7%9A%84%E7%BB%8F%E5%8E%86/" title="记手搓ResNet的经历"><i class="post-icon gg-file-document"></i>记手搓ResNet的经历</a></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">
          论文阅读
        </a>
      <span class="tree-list-count">2</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/">
          快照压缩成像
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F/%E5%BF%AB%E7%85%A7%E5%8E%8B%E7%BC%A9%E6%88%90%E5%83%8F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/" title="快照压缩成像论文阅读记录"><i class="post-icon gg-file-document"></i>快照压缩成像论文阅读记录</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/Deep-Learning/">
          Deep Learning
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/Deep-Learning/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/" title="深度学习论文阅读记录"><i class="post-icon gg-file-document"></i>深度学习论文阅读记录</a></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/C/">
          C++
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/C/Cpp/Cpp98/Introduction%20to%20Programming%20with%20C++%20%E5%95%83%E4%B9%A6%E8%AE%B0%E5%BD%95/" title="《Introduction to Programming with C++》读书笔记"><i class="post-icon gg-file-document"></i>《Introduction to Programming with C++》读书笔记</a></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body></html>